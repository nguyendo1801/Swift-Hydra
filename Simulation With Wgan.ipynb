{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Beta-CVAE] Epoch 2/150, loss=1285.01\n",
      "[Beta-CVAE] Epoch 4/150, loss=1277.05\n",
      "[Beta-CVAE] Epoch 6/150, loss=1267.29\n",
      "[Beta-CVAE] Epoch 8/150, loss=1236.02\n",
      "[Beta-CVAE] Epoch 10/150, loss=1171.67\n",
      "[Beta-CVAE] Epoch 12/150, loss=1073.04\n",
      "[Beta-CVAE] Epoch 14/150, loss=1021.88\n",
      "[Beta-CVAE] Epoch 16/150, loss=1010.67\n",
      "[Beta-CVAE] Epoch 18/150, loss=995.68\n",
      "[Beta-CVAE] Epoch 20/150, loss=981.79\n",
      "[Beta-CVAE] Epoch 22/150, loss=978.88\n",
      "[Beta-CVAE] Epoch 24/150, loss=951.28\n",
      "[Beta-CVAE] Epoch 26/150, loss=953.21\n",
      "[Beta-CVAE] Epoch 28/150, loss=955.91\n",
      "[Beta-CVAE] Epoch 30/150, loss=924.41\n",
      "[Beta-CVAE] Epoch 32/150, loss=930.67\n",
      "[Beta-CVAE] Epoch 34/150, loss=918.81\n",
      "[Beta-CVAE] Epoch 36/150, loss=909.92\n",
      "[Beta-CVAE] Epoch 38/150, loss=898.35\n",
      "[Beta-CVAE] Epoch 40/150, loss=891.01\n",
      "[Beta-CVAE] Epoch 42/150, loss=880.74\n",
      "[Beta-CVAE] Epoch 44/150, loss=897.23\n",
      "[Beta-CVAE] Epoch 46/150, loss=872.27\n",
      "[Beta-CVAE] Epoch 48/150, loss=867.88\n",
      "[Beta-CVAE] Epoch 50/150, loss=866.02\n",
      "[Beta-CVAE] Epoch 52/150, loss=860.41\n",
      "[Beta-CVAE] Epoch 54/150, loss=849.65\n",
      "[Beta-CVAE] Epoch 56/150, loss=852.48\n",
      "[Beta-CVAE] Epoch 58/150, loss=847.42\n",
      "[Beta-CVAE] Epoch 60/150, loss=837.64\n",
      "[Beta-CVAE] Epoch 62/150, loss=837.15\n",
      "[Beta-CVAE] Epoch 64/150, loss=835.62\n",
      "[Beta-CVAE] Epoch 66/150, loss=833.00\n",
      "[Beta-CVAE] Epoch 68/150, loss=823.29\n",
      "[Beta-CVAE] Epoch 70/150, loss=821.97\n",
      "[Beta-CVAE] Epoch 72/150, loss=821.73\n",
      "[Beta-CVAE] Epoch 74/150, loss=822.03\n",
      "[Beta-CVAE] Epoch 76/150, loss=809.02\n",
      "[Beta-CVAE] Epoch 78/150, loss=797.76\n",
      "[Beta-CVAE] Epoch 80/150, loss=809.52\n",
      "[Beta-CVAE] Epoch 82/150, loss=793.77\n",
      "[Beta-CVAE] Epoch 84/150, loss=790.44\n",
      "[Beta-CVAE] Epoch 86/150, loss=799.56\n",
      "[Beta-CVAE] Epoch 88/150, loss=784.67\n",
      "[Beta-CVAE] Epoch 90/150, loss=793.07\n",
      "[Beta-CVAE] Epoch 92/150, loss=781.62\n",
      "[Beta-CVAE] Epoch 94/150, loss=763.39\n",
      "[Beta-CVAE] Epoch 96/150, loss=762.18\n",
      "[Beta-CVAE] Epoch 98/150, loss=765.10\n",
      "[Beta-CVAE] Epoch 100/150, loss=750.37\n",
      "[Beta-CVAE] Epoch 102/150, loss=756.40\n",
      "[Beta-CVAE] Epoch 104/150, loss=768.50\n",
      "[Beta-CVAE] Epoch 106/150, loss=749.50\n",
      "[Beta-CVAE] Epoch 108/150, loss=756.81\n",
      "[Beta-CVAE] Epoch 110/150, loss=746.12\n",
      "[Beta-CVAE] Epoch 112/150, loss=747.50\n",
      "[Beta-CVAE] Epoch 114/150, loss=755.37\n",
      "[Beta-CVAE] Epoch 116/150, loss=737.98\n",
      "[Beta-CVAE] Epoch 118/150, loss=750.95\n",
      "[Beta-CVAE] Epoch 120/150, loss=731.46\n",
      "[Beta-CVAE] Epoch 122/150, loss=736.02\n",
      "[Beta-CVAE] Epoch 124/150, loss=742.49\n",
      "[Beta-CVAE] Epoch 126/150, loss=729.37\n",
      "[Beta-CVAE] Epoch 128/150, loss=745.76\n",
      "[Beta-CVAE] Epoch 130/150, loss=741.18\n",
      "[Beta-CVAE] Epoch 132/150, loss=734.57\n",
      "[Beta-CVAE] Epoch 134/150, loss=722.95\n",
      "[Beta-CVAE] Epoch 136/150, loss=727.20\n",
      "[Beta-CVAE] Epoch 138/150, loss=737.38\n",
      "[Beta-CVAE] Epoch 140/150, loss=714.76\n",
      "[Beta-CVAE] Epoch 142/150, loss=724.36\n",
      "[Beta-CVAE] Epoch 144/150, loss=742.23\n",
      "[Beta-CVAE] Epoch 146/150, loss=711.76\n",
      "[Beta-CVAE] Epoch 148/150, loss=718.81\n",
      "[Beta-CVAE] Epoch 150/150, loss=712.63\n",
      "After oversampling with Beta-CVAE:\n",
      "Class distribution in training set: {0.0: 662, 1.0: 662}\n",
      "[Transformer] Epoch 1/50, Loss=0.4492\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 2/50, Loss=0.2815\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 3/50, Loss=0.1658\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 4/50, Loss=0.1236\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 5/50, Loss=0.0745\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 6/50, Loss=0.0793\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 7/50, Loss=0.0926\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 8/50, Loss=0.0734\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 9/50, Loss=0.0789\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 10/50, Loss=0.0693\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 11/50, Loss=0.0486\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 12/50, Loss=0.0587\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 13/50, Loss=0.0442\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 14/50, Loss=0.0469\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 15/50, Loss=0.0394\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 16/50, Loss=0.0423\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 17/50, Loss=0.0401\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 18/50, Loss=0.0421\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 19/50, Loss=0.0270\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 20/50, Loss=0.0405\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 21/50, Loss=0.0248\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 22/50, Loss=0.0179\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 23/50, Loss=0.0350\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 24/50, Loss=0.0208\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 25/50, Loss=0.0216\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 26/50, Loss=0.0628\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 27/50, Loss=0.0555\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 28/50, Loss=0.0391\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 29/50, Loss=0.0402\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 30/50, Loss=0.0544\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 31/50, Loss=0.0453\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 32/50, Loss=0.0280\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 33/50, Loss=0.0139\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 34/50, Loss=0.0171\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 35/50, Loss=0.0277\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 36/50, Loss=0.0167\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 37/50, Loss=0.0213\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 38/50, Loss=0.0480\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 39/50, Loss=0.0890\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 40/50, Loss=0.0319\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 41/50, Loss=0.0277\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 42/50, Loss=0.0305\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 43/50, Loss=0.0259\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 44/50, Loss=0.0168\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 45/50, Loss=0.0448\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 46/50, Loss=0.0343\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 47/50, Loss=0.0221\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 48/50, Loss=0.0358\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 49/50, Loss=0.0280\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 50/50, Loss=0.0219\n",
      "----------------------------------------\n",
      "Beta-CVAE model saved to: ./saved_models\\beta_cvae.pth\n",
      "TransformerDetector model saved to: ./saved_models\\transformer_detector.pth\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "# Specify dataset path and device configuration\n",
    "dataset_path = r\"ADBench_datasets/6_cardio.npz\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 4.1: Load and preprocess data\n",
    "X_all, y_all = load_adbench_data(dataset_path) # new preprocessing\n",
    "input_dim = X_all.shape[1]\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_all = torch.tensor(scaler.fit_transform(X_all)).float()\n",
    "\n",
    "# Split data into train and test sets\n",
    "D_train_np, D_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_all.numpy(), y_all.numpy(), test_size=0.6, random_state=42, stratify=y_all\n",
    ")\n",
    "D_train = torch.tensor(D_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "D_test  = torch.tensor(D_test_np,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test_np,  dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training Beta-CVAE\n",
    "train_dataset = TensorDataset(D_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Step 4.2: Initialize Beta-CVAE model\n",
    "# Increase beta (e.g., beta=4.0) to emphasize KL divergence, encouraging more diverse latent space\n",
    "beta_cvae = BetaCVAE(input_dim=input_dim, hidden_dim=512, latent_dim=64, beta=1.0).to(device)\n",
    "\n",
    "# Train Beta-CVAE model\n",
    "optimizer_cvae = Adam(beta_cvae.parameters(), lr=1e-4)\n",
    "num_epochs_cvae = 150\n",
    "for epoch in range(num_epochs_cvae):\n",
    "    loss_cvae = train_beta_cvae(beta_cvae, train_loader, optimizer_cvae, device)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"[Beta-CVAE] Epoch {epoch+1}/{num_epochs_cvae}, loss={loss_cvae:.2f}\")\n",
    "\n",
    "# Step 4.3: Generate synthetic data to maximize diversity\n",
    "# Instead of sampling z ~ Normal(0,1), use z ~ Uniform([-2,2]) to increase coverage and encourage diversity\n",
    "beta_cvae.eval()\n",
    "\n",
    "# Separate minority and majority classes\n",
    "minority_mask = (y_train == 1)\n",
    "X_minority = D_train[minority_mask]\n",
    "majority_mask = (y_train == 0)\n",
    "X_majority = D_train[majority_mask]\n",
    "\n",
    "# Calculate the number of synthetic samples to generate\n",
    "num_generate = len(X_majority) - len(X_minority)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate latent variables uniformly within [-2,2]\n",
    "    z_uniform = (torch.rand(num_generate, beta_cvae.latent_dim) * 4.0) - 2.0\n",
    "    z_uniform = z_uniform.to(device)\n",
    "\n",
    "    # Assign synthetic labels (e.g., oversample minority class with y=1)\n",
    "    y_synthetic_c = torch.full((num_generate, 1), 0.9, device=device)\n",
    "\n",
    "    # Decode synthetic data\n",
    "    X_synthetic = beta_cvae.decode(z_uniform, y_synthetic_c)\n",
    "    X_synthetic = X_synthetic.cpu()\n",
    "\n",
    "# Create labels for synthetic samples\n",
    "y_synthetic_labels = torch.ones(num_generate)\n",
    "\n",
    "# Combine synthetic data with original training data\n",
    "D_train_final = torch.cat([D_train, X_synthetic], dim=0)\n",
    "y_train_final = torch.cat([y_train, y_synthetic_labels], dim=0)\n",
    "\n",
    "# Step 4.4: Train TransformerDetector on the augmented dataset\n",
    "train_dataset_final = TensorDataset(D_train_final, y_train_final)\n",
    "test_dataset = TensorDataset(D_test, y_test)\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "print(\"After oversampling with Beta-CVAE:\")\n",
    "unique, counts = np.unique(y_train_final.numpy(), return_counts=True)\n",
    "print(\"Class distribution in training set:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initialize Transformer Detector model\n",
    "model = TransformerDetector(input_size=input_dim).to(device)\n",
    "optimizer_tf = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the Transformer Detector model\n",
    "num_epochs_tf = 50\n",
    "for epoch in range(num_epochs_tf):\n",
    "    train_loss = train_detector(model, train_loader_final, optimizer_tf, criterion, device)\n",
    "    print(f\"[Transformer] Epoch {epoch+1}/{num_epochs_tf}, Loss={train_loss:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Ensure the directory for saving models exists\n",
    "save_dir = \"./saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained Beta-CVAE model\n",
    "vae_path = os.path.join(save_dir, \"beta_cvae.pth\")\n",
    "torch.save(beta_cvae.state_dict(), vae_path)\n",
    "print(f\"Beta-CVAE model saved to: {vae_path}\")\n",
    "\n",
    "# Save the trained TransformerDetector model\n",
    "detector_path = os.path.join(save_dir, \"transformer_detector.pth\")\n",
    "torch.save(model.state_dict(), detector_path)\n",
    "print(f\"TransformerDetector model saved to: {detector_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([732, 21])\n",
      "Models loaded successfully.\n",
      "===== EPISODE 1/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.9214 Sample reward: 0.9021\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.7838 Sample reward: 0.9471\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 23.2842 Sample reward: 0.9959\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.9279 Sample reward: 0.9507\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.6290 Sample reward: 0.9422\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 2.5013 Sample reward: 0.9617\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 5.4904 Sample reward: 0.9825\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0138 Diversity reward: 0.3267 Sample reward: 0.7737\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 3.0842 Sample reward: 0.9690\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 2.6620 Sample reward: 0.9639\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 1.7654 Sample reward: 0.9464\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.9306 Sample reward: 0.9031\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 7.7043 Sample reward: 0.9874\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 1.9667 Sample reward: 0.9517\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 2.9192 Sample reward: 0.9672\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0024 Diversity reward: 0.4946 Sample reward: 0.8335\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 3.7956 Sample reward: 0.9746\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 24.6270 Sample reward: 0.9961\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.7679 Sample reward: 0.8850\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.0186 Sample reward: 0.9108\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 2.5374 Sample reward: 0.9622\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 9.3237 Sample reward: 0.9897\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 2.6728 Sample reward: 0.9641\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 4.9561 Sample reward: 0.9805\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.9219 Sample reward: 0.9023\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 3.2007 Sample reward: 0.9698\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.5803 Sample reward: 0.8535\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0022 Diversity reward: 0.5102 Sample reward: 0.8377\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 5.1189 Sample reward: 0.9810\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 2.7148 Sample reward: 0.9652\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 2.5323 Sample reward: 0.9621\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0064 Diversity reward: 0.4487 Sample reward: 0.8220\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 2.9893 Sample reward: 0.9677\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 8.3621 Sample reward: 0.9883\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0072 Diversity reward: 0.4263 Sample reward: 0.8147\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0018 Diversity reward: 0.4350 Sample reward: 0.8143\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 2.6127 Sample reward: 0.9632\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 6.8176 Sample reward: 0.9856\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 1.0628 Sample reward: 0.9143\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 1.1067 Sample reward: 0.9174\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 1.4229 Sample reward: 0.9346\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0123 Diversity reward: 0.4166 Sample reward: 0.8144\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 1.2259 Sample reward: 0.9248\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0100 Diversity reward: 1.9556 Sample reward: 0.9604\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 4.6022 Diversity reward: 2.0279 Sample reward: 3.7511\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.9576 Sample reward: 0.9060\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.7849 Sample reward: 0.9471\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.2702 Sample reward: 0.9272\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0352 Diversity reward: 0.4129 Sample reward: 0.8277\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 1.6218 Sample reward: 0.9420\n",
      "===== EPISODE 2/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 1.8661 Sample reward: 0.9498\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0860 Diversity reward: 0.3388 Sample reward: 0.8224\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.5461 Sample reward: 0.8452\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3337 Sample reward: 0.7695\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.6595 Sample reward: 0.8684\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 1.1769 Sample reward: 0.9221\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4483 Sample reward: 0.8177\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.6267 Sample reward: 0.8624\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 12.9185 Sample reward: 0.9924\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0017 Diversity reward: 4.3978 Sample reward: 0.9794\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 0.2781 Sample reward: 0.7358\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 1.2873 Sample reward: 0.9283\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 9.4232 Sample reward: 0.9898\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0011 Diversity reward: 0.9365 Sample reward: 0.9044\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.0974 Sample reward: 0.9167\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4978 Sample reward: 0.8328\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 61.3261 Sample reward: 0.9985\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 1.0050 Sample reward: 0.9095\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.5553 Sample reward: 0.8475\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0064 Diversity reward: 0.7338 Sample reward: 0.8851\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0710 Diversity reward: 0.2926 Sample reward: 0.7840\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.0919 Sample reward: 0.9162\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3560 Sample reward: 0.7807\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4399 Sample reward: 0.8148\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0384 Diversity reward: 0.2549 Sample reward: 0.7379\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8158 Sample reward: 0.8909\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.6623 Sample reward: 0.8693\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.7409 Sample reward: 0.8812\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4732 Sample reward: 0.8256\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.7414 Sample reward: 0.8812\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 5.3157 Sample reward: 0.9819\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0502 Diversity reward: 0.8535 Sample reward: 0.9351\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.6005 Sample reward: 0.8574\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.9083 Sample reward: 0.9011\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 2.0698 Sample reward: 0.9540\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.2384 Sample reward: 0.9254\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8627 Sample reward: 0.8962\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 0.2599 Sample reward: 0.7224\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0009 Diversity reward: 0.3263 Sample reward: 0.7659\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1798 Sample reward: 0.6427\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.2391 Sample reward: 0.9255\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 0.8816 Sample reward: 0.8986\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.7068 Sample reward: 0.8761\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.6846 Sample reward: 0.8726\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0009 Diversity reward: 4.7687 Sample reward: 0.9803\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4220 Sample reward: 0.8085\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.3459 Sample reward: 0.9309\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0072 Diversity reward: 0.1902 Sample reward: 0.6585\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0017 Diversity reward: 0.3123 Sample reward: 0.7584\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 120}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4669 Sample reward: 0.8236\n",
      "===== EPISODE 3/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2713 Sample reward: 0.7307\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4711 Sample reward: 0.8249\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.3162 Sample reward: 0.7597\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4058 Sample reward: 0.8023\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0016 Diversity reward: 0.5536 Sample reward: 0.8481\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.2661 Sample reward: 0.7271\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.5742 Sample reward: 0.8518\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2389 Sample reward: 0.7049\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4928 Sample reward: 0.8313\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4666 Sample reward: 0.8235\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2190 Sample reward: 0.6866\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1820 Sample reward: 0.6454\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.3066 Sample reward: 0.7541\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1575 Sample reward: 0.6117\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3277 Sample reward: 0.7663\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.5874 Sample reward: 0.8547\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.5814 Sample reward: 0.8534\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.2439 Sample reward: 0.7094\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4005 Sample reward: 0.8003\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 0.1607 Sample reward: 0.6168\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2025 Sample reward: 0.6694\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8795 Sample reward: 0.8980\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.3712 Sample reward: 0.7881\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2116 Sample reward: 0.6791\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0013 Diversity reward: 0.1535 Sample reward: 0.6060\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.2271 Sample reward: 0.6945\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1918 Sample reward: 0.6573\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0042 Diversity reward: 0.2636 Sample reward: 0.7272\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 19.1398 Sample reward: 0.9950\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 1.8567 Sample reward: 0.9491\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2790 Sample reward: 0.7362\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2642 Sample reward: 0.7255\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.4912 Sample reward: 0.8313\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0196 Diversity reward: 0.1644 Sample reward: 0.6293\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3937 Sample reward: 0.7975\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.5518 Diversity reward: 0.1520 Sample reward: 0.7678\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3184 Sample reward: 0.7611\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.1315 Diversity reward: 0.3441 Sample reward: 0.8515\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 1.9455 Sample reward: 0.9514\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.8608 Sample reward: 0.8963\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 2.5678 Sample reward: 0.9629\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.8369 Sample reward: 0.8933\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2574 Sample reward: 0.7202\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3209 Sample reward: 0.7625\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.6759 Sample reward: 0.8712\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4093 Sample reward: 0.8037\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.5132 Sample reward: 0.8370\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2453 Sample reward: 0.7104\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.2000 Sample reward: 0.6667\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 170}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.3798 Sample reward: 0.7917\n",
      "===== EPISODE 4/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.3892 Sample reward: 0.7957\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2160 Sample reward: 0.6836\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2089 Sample reward: 0.6763\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.7468 Sample reward: 0.8820\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.7388 Sample reward: 0.8809\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0014 Diversity reward: 0.1196 Sample reward: 0.5451\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0088 Diversity reward: 0.2093 Sample reward: 0.6807\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3058 Sample reward: 0.7537\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2136 Sample reward: 0.6812\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2947 Sample reward: 0.7467\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.7490 Diversity reward: 0.5805 Sample reward: 1.3441\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2292 Sample reward: 0.6962\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2138 Sample reward: 0.6814\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0011 Diversity reward: 0.1155 Sample reward: 0.5363\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.2238 Sample reward: 0.6913\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1476 Sample reward: 0.5962\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0013 Diversity reward: 0.1942 Sample reward: 0.6607\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2275 Sample reward: 0.6946\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1275 Sample reward: 0.5606\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1976 Sample reward: 0.6641\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1499 Sample reward: 0.5998\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4565 Sample reward: 0.8204\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0020 Diversity reward: 0.7988 Sample reward: 0.8904\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8098 Sample reward: 0.8902\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1751 Sample reward: 0.6365\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 3.2263 Sample reward: 0.9700\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.2440 Sample reward: 0.7094\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2350 Sample reward: 0.7016\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2164 Sample reward: 0.6839\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.3050 Sample reward: 0.7531\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0016 Diversity reward: 0.1383 Sample reward: 0.5809\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0756 Diversity reward: 0.1828 Sample reward: 0.6772\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3625 Sample reward: 0.7839\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2449 Sample reward: 0.7101\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.5811 Diversity reward: 0.1345 Sample reward: 0.7268\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2236 Sample reward: 0.6910\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.3512 Sample reward: 0.7785\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.6329 Sample reward: 0.8636\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1808 Sample reward: 0.6439\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1505 Sample reward: 0.6008\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3928 Sample reward: 0.7972\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.2867 Diversity reward: 0.2163 Sample reward: 0.8068\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0977 Sample reward: 0.4942\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1987 Sample reward: 0.6653\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0550 Diversity reward: 0.1481 Sample reward: 0.6162\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.2492 Sample reward: 0.7138\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2154 Sample reward: 0.6830\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2077 Sample reward: 0.6751\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1956 Sample reward: 0.6618\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 220}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.1391 Sample reward: 0.5819\n",
      "===== EPISODE 5/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0603 Diversity reward: 4.7939 Sample reward: 1.0374\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.2097 Sample reward: 0.6772\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1512 Sample reward: 0.6019\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1611 Sample reward: 0.6171\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1980 Sample reward: 0.6645\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0873 Sample reward: 0.4662\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0055 Diversity reward: 2.9864 Sample reward: 0.9728\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1213 Sample reward: 0.5481\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.9491 Sample reward: 0.9047\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1532 Sample reward: 0.6051\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1172 Sample reward: 0.5397\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1495 Sample reward: 0.5992\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 1.3500 Sample reward: 0.9315\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1479 Sample reward: 0.5967\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1694 Sample reward: 0.6288\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0015 Diversity reward: 0.1120 Sample reward: 0.5286\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1805 Sample reward: 0.6436\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 0.0985 Sample reward: 0.4964\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3044 Sample reward: 0.7528\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 2.3494 Sample reward: 0.9592\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2009 Sample reward: 0.6676\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.3443 Sample reward: 0.7750\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1304 Sample reward: 0.5660\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.2711 Sample reward: 0.7309\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0597 Diversity reward: 5.5775 Sample reward: 1.0400\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.6930 Sample reward: 0.8742\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.1327 Sample reward: 0.5705\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1303 Sample reward: 0.5658\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1640 Sample reward: 0.6212\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2164 Sample reward: 0.6839\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0016 Diversity reward: 0.1168 Sample reward: 0.5392\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1568 Sample reward: 0.6108\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.2454 Diversity reward: 0.1822 Sample reward: 0.7397\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2342 Sample reward: 0.7008\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 1.3558 Sample reward: 0.9318\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1980 Sample reward: 0.6645\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0301 Diversity reward: 8.5655 Sample reward: 1.0179\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1651 Sample reward: 0.6228\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1468 Sample reward: 0.5949\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2267 Sample reward: 0.6940\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1491 Sample reward: 0.5986\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.4311 Sample reward: 0.8120\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0012 Diversity reward: 0.1063 Sample reward: 0.5155\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1574 Sample reward: 0.6116\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.1760 Sample reward: 0.6378\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1559 Sample reward: 0.6092\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3818 Sample reward: 0.7925\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1760 Sample reward: 0.6378\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1336 Sample reward: 0.5719\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 270}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1238 Sample reward: 0.5532\n",
      "===== EPISODE 6/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.4581 Sample reward: 0.8209\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1585 Sample reward: 0.6132\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1035 Sample reward: 0.5085\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0866 Sample reward: 0.4641\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.3862 Diversity reward: 0.2225 Sample reward: 0.8541\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1148 Sample reward: 0.5344\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1601 Sample reward: 0.6156\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1140 Sample reward: 0.5328\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0289 Diversity reward: 0.1015 Sample reward: 0.5110\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1947 Sample reward: 0.6608\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2430 Sample reward: 0.7085\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.5866 Sample reward: 0.8546\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1668 Sample reward: 0.6252\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.5453 Sample reward: 0.8451\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1159 Sample reward: 0.5368\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1348 Sample reward: 0.5742\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1079 Sample reward: 0.5189\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0995 Sample reward: 0.4987\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 2.8596 Diversity reward: 0.0919 Sample reward: 0.6953\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0927 Sample reward: 0.4809\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1929 Sample reward: 0.6586\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1650 Sample reward: 0.6227\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.4635 Diversity reward: 0.0968 Sample reward: 0.5826\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0011 Diversity reward: 0.1121 Sample reward: 0.5289\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0785 Sample reward: 0.4399\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0933 Sample reward: 0.4827\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0385 Diversity reward: 0.0964 Sample reward: 0.5000\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1754 Sample reward: 0.6369\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1116 Sample reward: 0.5274\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0858 Sample reward: 0.4619\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0837 Sample reward: 0.4558\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.5470 Sample reward: 0.8455\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0012 Diversity reward: 0.0922 Sample reward: 0.4800\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1307 Sample reward: 0.5666\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.4569 Sample reward: 0.8206\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1780 Sample reward: 0.6403\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2287 Sample reward: 0.6958\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1888 Sample reward: 0.6538\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1980 Sample reward: 0.6644\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0011 Diversity reward: 0.0850 Sample reward: 0.4597\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1550 Sample reward: 0.6078\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0945 Sample reward: 0.4858\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0070 Diversity reward: 0.2004 Sample reward: 0.6702\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0937 Sample reward: 0.4838\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1418 Sample reward: 0.5864\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1620 Sample reward: 0.6183\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4079 Sample reward: 0.8032\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1234 Sample reward: 0.5524\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1515 Sample reward: 0.6025\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 320}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1255 Sample reward: 0.5566\n",
      "===== EPISODE 7/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.6232 Sample reward: 0.8620\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.4824 Sample reward: 0.8283\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0708 Sample reward: 0.4145\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1166 Sample reward: 0.5384\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 2.8969 Sample reward: 0.9668\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1436 Sample reward: 0.5894\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1322 Sample reward: 0.5694\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1787 Sample reward: 0.6413\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4329 Sample reward: 0.8124\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1850 Sample reward: 0.6491\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1073 Sample reward: 0.5176\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.3213 Sample reward: 0.7627\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1872 Sample reward: 0.6518\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0958 Sample reward: 0.4894\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0691 Sample reward: 0.4086\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.9158 Diversity reward: 0.0942 Sample reward: 0.6317\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0616 Sample reward: 0.3813\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1608 Sample reward: 0.6165\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0718 Sample reward: 0.4181\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1180 Sample reward: 0.5413\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1170 Sample reward: 0.5393\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1198 Sample reward: 0.5451\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0905 Sample reward: 0.4750\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.1355 Diversity reward: 0.1956 Sample reward: 0.7184\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0729 Sample reward: 0.4218\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.3707 Diversity reward: 0.1324 Sample reward: 0.6735\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0742 Sample reward: 0.4260\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0957 Diversity reward: 4.7550 Sample reward: 1.0710\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.6133 Diversity reward: 0.1381 Sample reward: 0.7441\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 5.5593 Sample reward: 0.9825\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1953 Sample reward: 0.6614\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.7879 Diversity reward: 0.1028 Sample reward: 0.6526\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0033 Diversity reward: 0.0598 Sample reward: 0.3745\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1225 Sample reward: 0.5506\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 0.0962 Sample reward: 0.4904\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0311 Diversity reward: 0.0979 Sample reward: 0.5022\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1556 Sample reward: 0.6088\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0725 Sample reward: 0.4204\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 0.0987 Sample reward: 0.4969\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1165 Sample reward: 0.5381\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0744 Sample reward: 0.4267\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1207 Sample reward: 0.5469\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0603 Sample reward: 0.3763\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0701 Sample reward: 0.4120\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.2099 Sample reward: 0.6775\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0305 Diversity reward: 0.2739 Sample reward: 0.7488\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0901 Sample reward: 0.4739\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0869 Diversity reward: 0.0648 Sample reward: 0.4061\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.1544 Sample reward: 0.6071\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 370}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0704 Sample reward: 0.4133\n",
      "===== EPISODE 8/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0784 Sample reward: 0.4396\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0016 Diversity reward: 0.1326 Sample reward: 0.5706\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 2.0366 Sample reward: 0.9534\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0848 Sample reward: 0.4589\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0762 Sample reward: 0.4326\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0733 Sample reward: 0.4228\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0875 Sample reward: 0.4666\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.2227 Diversity reward: 0.0619 Sample reward: 0.4108\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0754 Sample reward: 0.4300\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 3.1739 Diversity reward: 2.2324 Sample reward: 2.7789\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0935 Sample reward: 0.4831\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.2195 Diversity reward: 0.1808 Sample reward: 0.7283\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.4600 Diversity reward: 0.0808 Sample reward: 0.5202\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1500 Sample reward: 0.6001\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0618 Sample reward: 0.3818\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0730 Sample reward: 0.4219\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0514 Sample reward: 0.3396\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0742 Sample reward: 0.4258\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0774 Sample reward: 0.4364\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.3567 Diversity reward: 0.1210 Sample reward: 0.6396\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1193 Sample reward: 0.5441\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0565 Sample reward: 0.3612\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0010 Diversity reward: 0.1875 Sample reward: 0.6526\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1014 Sample reward: 0.5035\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0027 Diversity reward: 0.0551 Sample reward: 0.3556\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.1333 Sample reward: 0.5714\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0015 Diversity reward: 0.0739 Sample reward: 0.4252\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.1337 Sample reward: 0.5724\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0069 Diversity reward: 0.1212 Sample reward: 0.5501\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1123 Sample reward: 0.5289\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0548 Sample reward: 0.3542\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0818 Sample reward: 0.4499\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 1.0899 Sample reward: 0.9163\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0897 Sample reward: 0.4730\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0015 Diversity reward: 0.0908 Sample reward: 0.4762\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.1327 Sample reward: 0.5704\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.2823 Diversity reward: 0.0803 Sample reward: 0.4940\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.2355 Diversity reward: 2.3469 Sample reward: 1.1738\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0864 Sample reward: 0.4635\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0718 Sample reward: 0.4179\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1537 Sample reward: 0.6058\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0584 Sample reward: 0.3688\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0837 Sample reward: 0.4558\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.1032 Sample reward: 0.5079\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.9860 Sample reward: 0.9522\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 2.7598 Diversity reward: 0.0613 Sample reward: 0.5018\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0832 Sample reward: 0.4541\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0717 Sample reward: 0.4178\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0666 Sample reward: 0.3998\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 420}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0744 Sample reward: 0.4268\n",
      "===== EPISODE 9/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1473 Sample reward: 0.5957\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0814 Sample reward: 0.4488\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1027 Sample reward: 0.5067\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1776 Sample reward: 0.6398\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0708 Sample reward: 0.4144\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0468 Sample reward: 0.3188\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0737 Sample reward: 0.4243\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0005 Diversity reward: 0.1725 Sample reward: 0.6332\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0550 Sample reward: 0.3547\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0026 Diversity reward: 0.0569 Sample reward: 0.3629\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0958 Sample reward: 0.4894\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0613 Sample reward: 0.3800\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0468 Sample reward: 0.3190\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0630 Sample reward: 0.3865\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0538 Sample reward: 0.3499\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0744 Sample reward: 0.4267\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0533 Sample reward: 0.3478\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0012 Diversity reward: 0.0933 Sample reward: 0.4830\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0803 Sample reward: 0.4453\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.0641 Sample reward: 0.3908\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0485 Sample reward: 0.3266\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 1.1530 Sample reward: 0.9205\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0011 Diversity reward: 0.0672 Sample reward: 0.4021\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0086 Diversity reward: 0.2761 Sample reward: 0.7388\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.1073 Sample reward: 0.5176\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0971 Sample reward: 0.4927\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1033 Sample reward: 0.5082\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0018 Diversity reward: 0.1238 Sample reward: 0.5537\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0492 Sample reward: 0.3298\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0995 Sample reward: 0.4990\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.2279 Sample reward: 0.6951\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.0716 Sample reward: 0.4173\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3024 Sample reward: 0.7515\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0778 Sample reward: 0.4376\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0740 Sample reward: 0.4251\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0674 Sample reward: 0.4026\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0982 Sample reward: 0.4955\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0609 Sample reward: 0.3784\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0755 Sample reward: 0.4304\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0635 Sample reward: 0.3885\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1480 Sample reward: 0.5969\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 8.1679 Sample reward: 0.9880\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0497 Sample reward: 0.3319\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.0463 Sample reward: 0.3166\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 1.0389 Sample reward: 0.9123\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0028 Diversity reward: 0.1800 Sample reward: 0.6440\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1106 Sample reward: 0.5253\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0096 Diversity reward: 0.1105 Sample reward: 0.5276\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.8287 Diversity reward: 1.0980 Sample reward: 1.5676\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 470}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0009 Diversity reward: 0.0672 Sample reward: 0.4021\n",
      "===== EPISODE 10/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.9835 Diversity reward: 0.0481 Sample reward: 0.3869\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0728 Sample reward: 0.4214\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0464 Sample reward: 0.3170\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0477 Sample reward: 0.3229\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1507 Sample reward: 0.6011\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8634 Sample reward: 0.8963\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3095 Sample reward: 0.7558\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.7401 Diversity reward: 0.0638 Sample reward: 0.4670\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0636 Sample reward: 0.3887\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0887 Sample reward: 0.4701\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0750 Sample reward: 0.4287\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0687 Sample reward: 0.4072\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0537 Diversity reward: 0.0594 Sample reward: 0.3800\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0696 Sample reward: 0.4103\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0456 Sample reward: 0.3133\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.1754 Sample reward: 0.6370\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0577 Sample reward: 0.3661\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0780 Sample reward: 0.4384\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.6089 Sample reward: 0.8590\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0009 Diversity reward: 0.0521 Sample reward: 0.3426\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 8.8678 Sample reward: 0.9890\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.1466 Sample reward: 0.5947\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0402 Diversity reward: 0.0845 Sample reward: 0.4662\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0490 Sample reward: 0.3288\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.8219 Sample reward: 0.8916\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.2043 Diversity reward: 0.1075 Sample reward: 0.5681\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0794 Diversity reward: 0.1213 Sample reward: 0.5711\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0776 Sample reward: 0.4371\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0437 Sample reward: 0.3041\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0552 Sample reward: 0.3558\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0124 Diversity reward: 0.0788 Sample reward: 0.4431\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0569 Sample reward: 0.3625\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0449 Sample reward: 0.3101\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0693 Sample reward: 0.4092\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0453 Sample reward: 0.3118\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0954 Sample reward: 0.4882\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0728 Sample reward: 0.4215\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1491 Sample reward: 0.5986\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0019 Diversity reward: 0.0454 Sample reward: 0.3123\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1017 Sample reward: 0.5042\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0605 Sample reward: 0.3772\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0013 Diversity reward: 0.0716 Sample reward: 0.4175\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0652 Sample reward: 0.3948\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0069 Diversity reward: 0.1205 Sample reward: 0.5486\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.1060 Sample reward: 0.5147\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0638 Diversity reward: 0.1343 Sample reward: 0.5936\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.2428 Sample reward: 0.7083\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0277 Diversity reward: 0.0438 Sample reward: 0.3070\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0622 Sample reward: 0.3834\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 520}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0539 Sample reward: 0.3503\n",
      "===== EPISODE 11/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0556 Sample reward: 0.3574\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0416 Sample reward: 0.2939\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 13.1438 Diversity reward: 0.9905 Sample reward: 5.6486\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0615 Sample reward: 0.3808\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0392 Diversity reward: 0.0462 Sample reward: 0.3197\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0688 Sample reward: 0.4076\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0383 Diversity reward: 0.0546 Sample reward: 0.3578\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 2.3223 Sample reward: 0.9588\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0519 Sample reward: 0.3419\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0734 Sample reward: 0.4234\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0411 Sample reward: 0.2913\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.0412 Sample reward: 0.2918\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0381 Sample reward: 0.2761\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.6565 Diversity reward: 0.0651 Sample reward: 0.4675\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0485 Sample reward: 0.3267\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0012 Diversity reward: 0.0367 Sample reward: 0.2688\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0432 Sample reward: 0.3017\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0524 Sample reward: 0.3437\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 0.0474 Sample reward: 0.3217\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0880 Sample reward: 0.4680\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0606 Sample reward: 0.3774\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0540 Sample reward: 0.3507\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0379 Sample reward: 0.2750\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0635 Sample reward: 0.3884\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0857 Sample reward: 0.4614\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.0132 Diversity reward: 0.1582 Sample reward: 0.6176\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.3983 Sample reward: 0.7994\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0617 Sample reward: 0.3815\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0459 Sample reward: 0.3144\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0776 Sample reward: 0.4370\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0719 Sample reward: 0.4184\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0433 Sample reward: 0.3022\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0541 Sample reward: 0.3511\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0447 Diversity reward: 2.1849 Sample reward: 0.9970\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0618 Sample reward: 0.3819\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0392 Sample reward: 0.2816\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0458 Sample reward: 0.3142\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0421 Sample reward: 0.2962\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0807 Sample reward: 0.4466\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0363 Sample reward: 0.2662\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0836 Sample reward: 0.4555\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0559 Sample reward: 0.3586\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0598 Sample reward: 0.3741\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0526 Sample reward: 0.3448\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.0421 Sample reward: 0.2962\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0537 Sample reward: 0.3492\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0389 Sample reward: 0.2803\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 692.9430 Diversity reward: 2.9931 Sample reward: 28.6996\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0342 Sample reward: 0.2547\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 570}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0452 Sample reward: 0.3114\n",
      "===== EPISODE 12/100 =====\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 0 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0662 Sample reward: 0.3985\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 1 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0450 Sample reward: 0.3104\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 2 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0377 Sample reward: 0.2740\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 3 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0469 Sample reward: 0.3193\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 4 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0333 Sample reward: 0.2501\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 5 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0389 Sample reward: 0.2800\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 6 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0821 Sample reward: 0.4510\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 7 =====\n",
      "Deceiving Detector Reward: 1.0026 Diversity reward: 0.0562 Sample reward: 0.3602\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 8 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0566 Sample reward: 0.3614\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 9 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0513 Sample reward: 0.3390\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 10 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0368 Sample reward: 0.2689\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 11 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0327 Sample reward: 0.2466\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 12 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1686 Sample reward: 0.6277\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 13 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0330 Sample reward: 0.2483\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 14 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0937 Sample reward: 0.4838\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 15 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0508 Sample reward: 0.3370\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 16 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0696 Sample reward: 0.4104\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 17 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0394 Sample reward: 0.2825\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 18 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0474 Sample reward: 0.3216\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 19 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0326 Sample reward: 0.2461\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 20 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0368 Sample reward: 0.2689\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 21 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0319 Sample reward: 0.2420\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 22 =====\n",
      "Deceiving Detector Reward: 1.0004 Diversity reward: 0.0368 Sample reward: 0.2689\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 23 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0844 Sample reward: 0.4578\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 24 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0387 Sample reward: 0.2789\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 25 =====\n",
      "Deceiving Detector Reward: 1.5389 Diversity reward: 0.0542 Sample reward: 0.4006\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 26 =====\n",
      "Deceiving Detector Reward: 1.0033 Diversity reward: 0.0576 Sample reward: 0.3659\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 27 =====\n",
      "Deceiving Detector Reward: 1.0002 Diversity reward: 0.0402 Sample reward: 0.2868\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 28 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0300 Sample reward: 0.2305\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 29 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0395 Sample reward: 0.2831\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 30 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0777 Sample reward: 0.4373\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 31 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0368 Sample reward: 0.2692\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 32 =====\n",
      "Deceiving Detector Reward: 1.0006 Diversity reward: 0.0411 Sample reward: 0.2915\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 33 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0321 Sample reward: 0.2430\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 34 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0558 Sample reward: 0.3582\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 35 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0538 Sample reward: 0.3498\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 36 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1109 Sample reward: 0.5260\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 37 =====\n",
      "Deceiving Detector Reward: 1.0008 Diversity reward: 0.0516 Sample reward: 0.3405\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 38 =====\n",
      "Deceiving Detector Reward: 1.0007 Diversity reward: 0.0664 Sample reward: 0.3990\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 39 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0850 Sample reward: 0.4596\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 40 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0655 Sample reward: 0.3957\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 41 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0553 Sample reward: 0.3561\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 42 =====\n",
      "Deceiving Detector Reward: 1.0000 Diversity reward: 0.0319 Sample reward: 0.2416\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 43 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.1926 Sample reward: 0.6582\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 44 =====\n",
      "Deceiving Detector Reward: 1.0019 Diversity reward: 0.0374 Sample reward: 0.2721\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 45 =====\n",
      "Deceiving Detector Reward: 1.0003 Diversity reward: 0.0360 Sample reward: 0.2646\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 46 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0546 Sample reward: 0.3531\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 47 =====\n",
      "Deceiving Detector Reward: 1.0009 Diversity reward: 0.2019 Sample reward: 0.6691\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 48 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.4064 Sample reward: 0.8026\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 620}\n",
      "===== Generated Data 49 =====\n",
      "Deceiving Detector Reward: 1.0001 Diversity reward: 0.0488 Sample reward: 0.3282\n",
      "Break at Episode 13: Class 1 (670) exceeds Class 0 (662).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sau khi oversampling bằng VAE:\n",
      "Phân phối lớp trong tập train: {0.0: 662, 1.0: 70}\n",
      "[Transformer] Epoch 1/200, Loss=0.3319\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.96      0.99      0.97       993\n",
      "     Class 1       0.88      0.60      0.72       106\n",
      "\n",
      "    accuracy                           0.95      1099\n",
      "   macro avg       0.92      0.80      0.84      1099\n",
      "weighted avg       0.95      0.95      0.95      1099\n",
      "\n",
      "AUC-ROC: 0.9661\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 2/200, Loss=0.1221\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.96      0.98       993\n",
      "     Class 1       0.73      0.91      0.81       106\n",
      "\n",
      "    accuracy                           0.96      1099\n",
      "   macro avg       0.86      0.93      0.89      1099\n",
      "weighted avg       0.96      0.96      0.96      1099\n",
      "\n",
      "AUC-ROC: 0.9897\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 3/200, Loss=0.0790\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.96      0.98       993\n",
      "     Class 1       0.71      0.95      0.81       106\n",
      "\n",
      "    accuracy                           0.96      1099\n",
      "   macro avg       0.85      0.96      0.89      1099\n",
      "weighted avg       0.97      0.96      0.96      1099\n",
      "\n",
      "AUC-ROC: 0.9913\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 4/200, Loss=0.0702\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.99       993\n",
      "     Class 1       0.85      0.91      0.88       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.92      0.94      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9943\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 5/200, Loss=0.0528\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 6/200, Loss=0.0422\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.98      0.88      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.94      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 7/200, Loss=0.0489\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.98      0.78      0.87       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.98      0.89      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 8/200, Loss=0.0513\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.88      0.99      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.94      0.99      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9982\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 9/200, Loss=0.0353\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.89      0.95      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.97      0.96      1099\n",
      "weighted avg       0.99      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9980\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 10/200, Loss=0.0321\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.90      0.98      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9987\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 11/200, Loss=0.0440\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.92      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 12/200, Loss=0.0255\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9980\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 13/200, Loss=0.0168\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 14/200, Loss=0.0144\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 15/200, Loss=0.0246\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.88      0.98      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.94      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 16/200, Loss=0.0197\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.90      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 17/200, Loss=0.0189\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.99       993\n",
      "     Class 1       0.85      0.92      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.92      0.95      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9952\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 18/200, Loss=0.0355\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.97      0.84      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.98      0.92      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9953\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 19/200, Loss=0.0454\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9959\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 20/200, Loss=0.0245\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 21/200, Loss=0.0192\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.88      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.97      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9947\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 22/200, Loss=0.0196\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9990\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 23/200, Loss=0.0157\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.89      0.98      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.94      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9990\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 24/200, Loss=0.0187\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.97      0.79      0.87       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.97      0.89      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9948\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 25/200, Loss=0.0370\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.90      0.96      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 26/200, Loss=0.0155\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.90      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9961\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 27/200, Loss=0.0109\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9961\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 28/200, Loss=0.0152\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 29/200, Loss=0.0121\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 30/200, Loss=0.0145\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 31/200, Loss=0.0116\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 32/200, Loss=0.0125\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.93      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 33/200, Loss=0.0067\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 34/200, Loss=0.0060\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.94      0.96      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.98      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 35/200, Loss=0.0056\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 36/200, Loss=0.0045\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 37/200, Loss=0.0039\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9976\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 38/200, Loss=0.0095\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.92      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 39/200, Loss=0.0188\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9985\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 40/200, Loss=0.0399\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.83      0.97      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.91      0.98      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9957\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 41/200, Loss=0.0401\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.89      0.93      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9940\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 42/200, Loss=0.0258\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.99       993\n",
      "     Class 1       0.86      0.93      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.96      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 43/200, Loss=0.0282\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.91      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 44/200, Loss=0.0312\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.88      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.97      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9953\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 45/200, Loss=0.0274\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.87      0.97      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.98      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 46/200, Loss=0.0174\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.90      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9961\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 47/200, Loss=0.0140\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.87      0.97      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.98      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9985\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 48/200, Loss=0.0146\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.92      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 49/200, Loss=0.0122\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.90      0.95      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 50/200, Loss=0.0081\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 51/200, Loss=0.0114\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.95      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 52/200, Loss=0.0059\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.93      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 53/200, Loss=0.0069\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.93      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 54/200, Loss=0.0036\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 55/200, Loss=0.0126\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.90      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9880\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 56/200, Loss=0.0389\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.82      0.96      0.88       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.91      0.97      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9945\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 57/200, Loss=0.0186\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9947\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 58/200, Loss=0.0132\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 59/200, Loss=0.0077\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.93      0.96      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.98      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 60/200, Loss=0.0066\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 61/200, Loss=0.0108\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9949\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 62/200, Loss=0.0450\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.97      0.82      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.97      0.91      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9804\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 63/200, Loss=0.0536\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.99       993\n",
      "     Class 1       0.84      0.92      0.88       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.91      0.95      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9691\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 64/200, Loss=0.0286\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.90      0.90      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.94      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9862\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 65/200, Loss=0.0293\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.93      0.96      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.98      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9982\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 66/200, Loss=0.0167\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9980\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 67/200, Loss=0.0064\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9965\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 68/200, Loss=0.0083\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9956\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 69/200, Loss=0.0069\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.90      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9954\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 70/200, Loss=0.0277\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9950\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 71/200, Loss=0.0060\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9948\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 72/200, Loss=0.0041\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9942\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 73/200, Loss=0.0072\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9951\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 74/200, Loss=0.0028\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.91      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9951\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 75/200, Loss=0.0026\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 76/200, Loss=0.0021\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 77/200, Loss=0.0012\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 78/200, Loss=0.0045\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 79/200, Loss=0.0027\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 80/200, Loss=0.0012\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 81/200, Loss=0.0009\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.92      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9963\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 82/200, Loss=0.0016\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 83/200, Loss=0.0023\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.97      0.84      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.98      0.92      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9939\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 84/200, Loss=0.0572\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      1.00      0.99       993\n",
      "     Class 1       0.98      0.82      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.98      0.91      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9830\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 85/200, Loss=0.0710\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.97      0.98       993\n",
      "     Class 1       0.75      0.95      0.84       106\n",
      "\n",
      "    accuracy                           0.96      1099\n",
      "   macro avg       0.87      0.96      0.91      1099\n",
      "weighted avg       0.97      0.96      0.97      1099\n",
      "\n",
      "AUC-ROC: 0.9919\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 86/200, Loss=0.0475\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.99      0.99       993\n",
      "     Class 1       0.91      0.85      0.88       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.92      0.93      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9954\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 87/200, Loss=0.0188\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.89      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9957\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 88/200, Loss=0.0107\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 89/200, Loss=0.0160\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.93      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9962\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 90/200, Loss=0.0127\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.91      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 91/200, Loss=0.0092\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.90      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 92/200, Loss=0.0059\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.90      0.93      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 93/200, Loss=0.0072\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9965\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 94/200, Loss=0.0053\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 95/200, Loss=0.0044\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 96/200, Loss=0.0131\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.90      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9951\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 97/200, Loss=0.0278\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9947\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 98/200, Loss=0.0204\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.89      0.99      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.94      0.99      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9984\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 99/200, Loss=0.0273\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.99      0.99       993\n",
      "     Class 1       0.92      0.86      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.93      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9921\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 100/200, Loss=0.0201\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.90      0.97      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 101/200, Loss=0.0167\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.91      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9962\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 102/200, Loss=0.0094\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 103/200, Loss=0.0217\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 104/200, Loss=0.0462\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.98       993\n",
      "     Class 1       0.85      0.88      0.86       106\n",
      "\n",
      "    accuracy                           0.97      1099\n",
      "   macro avg       0.92      0.93      0.92      1099\n",
      "weighted avg       0.97      0.97      0.97      1099\n",
      "\n",
      "AUC-ROC: 0.9829\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 105/200, Loss=0.0293\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.88      0.92      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9958\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 106/200, Loss=0.0198\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 107/200, Loss=0.0121\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.94      0.96      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.98      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 108/200, Loss=0.0132\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.93      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 109/200, Loss=0.0083\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.93      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9978\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 110/200, Loss=0.0054\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.93      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 111/200, Loss=0.0047\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.91      0.96      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.98      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 112/200, Loss=0.0036\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9978\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 113/200, Loss=0.0022\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 114/200, Loss=0.0020\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 115/200, Loss=0.0011\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.94      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.95      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9976\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 116/200, Loss=0.0176\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.89      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.94      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9930\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 117/200, Loss=0.0197\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.86      0.98      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.98      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9983\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 118/200, Loss=0.0214\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.91      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9978\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 119/200, Loss=0.0057\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.89      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9943\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 120/200, Loss=0.0035\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 121/200, Loss=0.0061\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.88      0.93      0.90       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 122/200, Loss=0.0067\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.89      0.93      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.96      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 123/200, Loss=0.0020\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.96      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 124/200, Loss=0.0050\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.90      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 125/200, Loss=0.0092\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 126/200, Loss=0.0017\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.92      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 127/200, Loss=0.0009\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.91      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.95      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 128/200, Loss=0.0030\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9920\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 129/200, Loss=0.0009\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.98      0.99       993\n",
      "     Class 1       0.87      0.92      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.95      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9758\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 130/200, Loss=0.0049\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.92      0.93       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9929\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 131/200, Loss=0.0173\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.91      0.91      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9905\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 132/200, Loss=0.0288\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.99      0.99       993\n",
      "     Class 1       0.93      0.85      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.92      0.94      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9936\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 133/200, Loss=0.0298\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.90      0.94      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.95      0.97      0.96      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9978\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 134/200, Loss=0.0125\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9983\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 135/200, Loss=0.0108\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.98      0.92      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.99      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 136/200, Loss=0.0076\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9984\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 137/200, Loss=0.0045\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9987\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 138/200, Loss=0.0053\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.97      0.92      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 139/200, Loss=0.0196\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9984\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 140/200, Loss=0.0252\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9989\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 141/200, Loss=0.0060\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      1.00      0.99       993\n",
      "     Class 1       0.96      0.93      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.98      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9984\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 142/200, Loss=0.0044\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 143/200, Loss=0.0013\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9980\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 144/200, Loss=0.0006\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9976\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 145/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 146/200, Loss=0.0003\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9975\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 147/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 148/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 149/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9974\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 150/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 151/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 152/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 153/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 154/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 155/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 156/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9972\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 157/200, Loss=0.0003\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 158/200, Loss=0.0005\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 159/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 160/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.92      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 161/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 162/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 163/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 164/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 165/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 166/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 167/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 168/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 169/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 170/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 171/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 172/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 173/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 174/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9970\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 175/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 176/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 177/200, Loss=0.0001\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 178/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 179/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 180/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9968\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 181/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 182/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9969\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 183/200, Loss=0.0002\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.95      0.94      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9967\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 184/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9963\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 185/200, Loss=0.0000\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9964\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 186/200, Loss=0.0414\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      1.00      0.98       993\n",
      "     Class 1       0.96      0.73      0.83       106\n",
      "\n",
      "    accuracy                           0.97      1099\n",
      "   macro avg       0.97      0.86      0.91      1099\n",
      "weighted avg       0.97      0.97      0.97      1099\n",
      "\n",
      "AUC-ROC: 0.9852\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 187/200, Loss=0.1314\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.86      0.96      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.97      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9961\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 188/200, Loss=0.0584\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.91      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.96      0.95      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9966\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 189/200, Loss=0.0305\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.91      0.98      0.95       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.99      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 190/200, Loss=0.0361\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.96      0.98       993\n",
      "     Class 1       0.72      0.98      0.83       106\n",
      "\n",
      "    accuracy                           0.96      1099\n",
      "   macro avg       0.86      0.97      0.91      1099\n",
      "weighted avg       0.97      0.96      0.96      1099\n",
      "\n",
      "AUC-ROC: 0.9956\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 191/200, Loss=0.0608\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.88      0.95      0.91       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.94      0.97      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9944\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 192/200, Loss=0.0605\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.87      0.98      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.98      0.96      1099\n",
      "weighted avg       0.99      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9983\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 193/200, Loss=0.0335\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9973\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 194/200, Loss=0.0181\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.92      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.96      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9980\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 195/200, Loss=0.0201\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.94      0.93      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.97      0.96      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9981\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 196/200, Loss=0.0136\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.99      0.99       993\n",
      "     Class 1       0.92      0.97      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.98      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9977\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 197/200, Loss=0.0173\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.94      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9971\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 198/200, Loss=0.0132\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9978\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 199/200, Loss=0.0127\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99       993\n",
      "     Class 1       0.93      0.95      0.94       106\n",
      "\n",
      "    accuracy                           0.99      1099\n",
      "   macro avg       0.96      0.97      0.97      1099\n",
      "weighted avg       0.99      0.99      0.99      1099\n",
      "\n",
      "AUC-ROC: 0.9979\n",
      "----------------------------------------\n",
      "[Transformer] Epoch 200/200, Loss=0.0213\n",
      "Test set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.98      0.99       993\n",
      "     Class 1       0.87      0.97      0.92       106\n",
      "\n",
      "    accuracy                           0.98      1099\n",
      "   macro avg       0.93      0.98      0.95      1099\n",
      "weighted avg       0.98      0.98      0.98      1099\n",
      "\n",
      "AUC-ROC: 0.9976\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import *\n",
    "from model import *\n",
    "import umap\n",
    "\n",
    "# Ví dụ đường dẫn\n",
    "dataset_path = r\"ADBench_datasets/6_cardio.npz\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Lưu Beta-CVAE\n",
    "# Đảm bảo thư mục lưu trữ tồn tại\n",
    "save_dir = \"./saved_models\"\n",
    "vae_path = os.path.join(save_dir, \"beta_cvae.pth\")\n",
    "detector_path = os.path.join(save_dir, \"transformer_detector.pth\")\n",
    "\n",
    "# 4.1: Load Data\n",
    "X_all, y_all = load_adbench_data(dataset_path) # this loads data from the specified path woith preprocessing\n",
    "input_dim = X_all.shape[1]\n",
    "scaler = StandardScaler()\n",
    "X_all = torch.tensor(scaler.fit_transform(X_all)).float()\n",
    "\n",
    "# Chia train/test\n",
    "D_train_np, D_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_all.numpy(), y_all.numpy(), test_size=0.6, random_state=42, stratify=y_all\n",
    ")\n",
    "D_train = torch.tensor(D_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "D_test  = torch.tensor(D_test_np,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test_np,  dtype=torch.float32)\n",
    "print(D_train.shape)\n",
    "# DataLoader cho Beta-CVAE\n",
    "train_dataset = TensorDataset(D_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Khởi tạo mô hình cùng cấu hình ban đầu\n",
    "loaded_beta_cvae = BetaCVAE(input_dim=input_dim, hidden_dim=512, latent_dim=64, beta=1.0).to(device)\n",
    "loaded_detector_model = TransformerDetector(input_size=input_dim).to(device)\n",
    "\n",
    "# Load trạng thái mô hình đã lưu\n",
    "# Chỉ load trọng số\n",
    "loaded_beta_cvae.load_state_dict(torch.load(vae_path, weights_only=True))\n",
    "loaded_detector_model.load_state_dict(torch.load(detector_path, weights_only=True))\n",
    "\n",
    "# Đặt mô hình về chế độ eval (nếu chỉ sử dụng inference)\n",
    "loaded_beta_cvae.eval()\n",
    "loaded_detector_model.eval()\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "\n",
    "num_episodes = 100\n",
    "num_gen_data = 50\n",
    "batch_size = 128\n",
    "new_detector = TransformerDetector(input_size=input_dim).to(device)\n",
    "optimizer_cvae = Adam(loaded_beta_cvae.parameters(), lr=1e-4)\n",
    "optimizer_detector = Adam(new_detector.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "# File log\n",
    "log_dir = \"./logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "beta_cvae_log = os.path.join(log_dir, \"beta_cvae.log\")\n",
    "detector_log = os.path.join(log_dir, \"detector.log\")\n",
    "adversarial_log = os.path.join(log_dir, \"adversarial_samples.log\")\n",
    "synthetic_data = []\n",
    "start_idx = 0  # Vị trí bắt đầu ban đầu\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    # Lấy mask cho class 1\n",
    "    class1_mask = (y_train == 1)  # Boolean tensor: True nếu y_train[i] == 1\n",
    "    class0_mask = (y_train == 0)  # Boolean tensor: True nếu y_train[i] == 0\n",
    "\n",
    "    # Đếm số lượng phần tử của mỗi class\n",
    "    num_class1 = class1_mask.sum().item()\n",
    "    num_class0 = class0_mask.sum().item()\n",
    "\n",
    "    # Kiểm tra nếu số lượng class 1 vượt quá class 0, thoát vòng lặp\n",
    "    if num_class1 > num_class0:\n",
    "        print(f\"Break at Episode {ep + 1}: Class 1 ({num_class1}) exceeds Class 0 ({num_class0}).\")\n",
    "        break\n",
    "    # Lọc các mẫu thuộc class 1 từ D_train\n",
    "    # Lọc các mẫu thuộc class 1 từ D_train\n",
    "    D_train_grow_tensor = D_train[class1_mask]  # Tensor chứa tất cả các mẫu class 1\n",
    "    # Chuyển mỗi hàng thành một tensor và lưu vào list\n",
    "    D_train_grow = [row for row in D_train_grow_tensor]\n",
    "    print(f\"===== EPISODE {ep + 1}/{num_episodes} =====\")\n",
    "\n",
    "    # Train Beta-CVAE\n",
    "    train_dataset = TensorDataset(D_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    num_epochs_cvae = 10\n",
    "    for epoch in range(num_epochs_cvae):\n",
    "        loss_cvae = train_beta_cvae(loaded_beta_cvae, train_loader, optimizer_cvae, device)\n",
    "        log_to_file(beta_cvae_log, f\"Epoch {epoch + 1}/{num_epochs_cvae}, Loss: {loss_cvae:.4f}\")\n",
    "\n",
    "    detector_epochs = 5\n",
    "    train_dataset_detector = TensorDataset(D_train, y_train)\n",
    "    train_loader_detector = DataLoader(train_dataset_detector, batch_size=batch_size, shuffle=True)\n",
    "    for det_epoch in range(detector_epochs):\n",
    "        detector_loss = train_detector(new_detector, train_loader_detector, optimizer_detector, criterion, device)\n",
    "        # Giả sử y_train có dạng 0/1\n",
    "    # print(f\"===== Evaluate in Testing set =====\")\n",
    "    # evaluate_with_classification_report_and_auc(model, test_loader, device, threshold=0.5)\n",
    "    # Generate Adversarial Samples\n",
    "    idx_class1 = (y_train == 1).nonzero(as_tuple=True)[0]\n",
    "    new_samples, new_labels = [], []\n",
    "    # Lấy các index từ idx_class1, xử lý wrap-around nếu cần\n",
    "    end_idx = start_idx + num_gen_data\n",
    "    indices = idx_class1[start_idx:end_idx]\n",
    "\n",
    "    # Nếu vượt quá độ dài của idx_class1, quay lại từ đầu\n",
    "    if end_idx > len(idx_class1):\n",
    "        indices += idx_class1[:end_idx - len(idx_class1)]\n",
    "\n",
    "    # Cập nhật vị trí bắt đầu cho vòng lặp tiếp theo\n",
    "    start_idx = end_idx % len(idx_class1)\n",
    "    for syn_data in range(num_gen_data):\n",
    "        unique, counts = np.unique(y_train.numpy(), return_counts=True)\n",
    "        print(\"Phân phối lớp trong tập train:\", dict(zip(unique, counts)))\n",
    "        print(\"===== Generated Data {} =====\".format(syn_data))\n",
    "        random_idx = random.choice(idx_class1)\n",
    "        x_orig = D_train[random_idx]\n",
    "        x_adv = One_Step_To_Feasible_Action(\n",
    "            beta_cvae=loaded_beta_cvae,\n",
    "            detector=loaded_detector_model,\n",
    "            x_orig=x_orig,\n",
    "            device=device,\n",
    "            previously_generated=D_train_grow,\n",
    "            alpha=1.0,\n",
    "            lambda_div=0.1,\n",
    "            lr=0.01,\n",
    "            steps=20,\n",
    "            log_file=adversarial_log,\n",
    "        )\n",
    "        new_samples.append(x_adv.unsqueeze(0))\n",
    "        new_labels.append(torch.tensor([1]))\n",
    "        synthetic_data.append(x_adv)\n",
    "    new_samples = torch.cat(new_samples, dim=0)\n",
    "    new_labels = torch.cat(new_labels, dim=0)\n",
    "    D_train = torch.cat([D_train, new_samples], dim=0)\n",
    "    y_train = torch.cat([y_train, new_labels], dim=0)\n",
    "\n",
    "#___________________________________________________________\n",
    "# 4.4: Visualize Generated Data\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('default')  # Đảm bảo sử dụng style mặc định\n",
    "\n",
    "D_train = torch.tensor(D_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "D_test  = torch.tensor(D_test_np,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test_np,  dtype=torch.float32)\n",
    "\n",
    "X_synthetic = np.asarray(synthetic_data)\n",
    "X_synthetic = torch.tensor(X_synthetic,  dtype=torch.float32)\n",
    "\n",
    "# 1) Gộp dữ liệu\n",
    "X_plot = torch.cat([D_train, D_test, X_synthetic], dim=0).numpy()\n",
    "\n",
    "# Tạo nhãn:\n",
    "# - Phần đầu: y_train (0 hoặc 1)\n",
    "# - Tiếp theo: y_test (0 hoặc 1)\n",
    "# - Cuối cùng: synthetic (2)\n",
    "N_train = len(D_train)\n",
    "N_test = len(D_test)\n",
    "N_synthetic = len(X_synthetic)\n",
    "y_plot = np.concatenate([\n",
    "    y_train.numpy(),            # Nhãn train\n",
    "    y_test.numpy(),             # Nhãn test\n",
    "    np.full((N_synthetic,), 2)  # Nhãn synthetic\n",
    "], axis=0)\n",
    "\n",
    "# 2) Chuẩn hoá dữ liệu (nếu cần)\n",
    "scaler = StandardScaler()\n",
    "X_plot_scaled = scaler.fit_transform(X_plot)\n",
    "\n",
    "# 3) Tính UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, n_jobs=-1)\n",
    "\n",
    "X_embedded = reducer.fit_transform(X_plot_scaled)\n",
    "# X_embedded.shape = (N_train + N_test + N_synthetic, 2)\n",
    "\n",
    "# 4) Vẽ\n",
    "plt.figure(figsize=(10, 8), facecolor='white')  # Đặt nền trắng\n",
    "\n",
    "# Train Class 0 -> đỏ\n",
    "idx0_train = (y_plot[:N_train] == 0)\n",
    "plt.scatter(X_embedded[:N_train][idx0_train, 0], X_embedded[:N_train][idx0_train, 1],\n",
    "            c='darkred', alpha=0.6, label='Class 0 (Train)')\n",
    "\n",
    "# Train Class 1 -> xanh dương\n",
    "idx1_train = (y_plot[:N_train] == 1)\n",
    "plt.scatter(X_embedded[:N_train][idx1_train, 0], X_embedded[:N_train][idx1_train, 1],\n",
    "            c='darkblue', alpha=0.6, label='Class 1 (Train)')\n",
    "\n",
    "# Test Class 0 -> cam\n",
    "idx0_test = (y_plot[N_train:N_train + N_test] == 0)\n",
    "plt.scatter(X_embedded[N_train:N_train + N_test][idx0_test, 0],\n",
    "            X_embedded[N_train:N_train + N_test][idx0_test, 1],\n",
    "            c='orange', alpha=0.6, label='Class 0 (Test)')\n",
    "\n",
    "# Test Class 1 -> xanh nhạt\n",
    "idx1_test = (y_plot[N_train:N_train + N_test] == 1)\n",
    "plt.scatter(X_embedded[N_train:N_train + N_test][idx1_test, 0],\n",
    "            X_embedded[N_train:N_train + N_test][idx1_test, 1],\n",
    "            c='skyblue', alpha=0.6, label='Class 1 (Test)')\n",
    "\n",
    "# Synthetic Data -> xanh lá\n",
    "idx_syn = (y_plot[N_train + N_test:] == 2)\n",
    "plt.scatter(X_embedded[N_train + N_test:][idx_syn, 0],\n",
    "            X_embedded[N_train + N_test:][idx_syn, 1],\n",
    "            c='green', alpha=0.6, label='Synthetic')\n",
    "\n",
    "plt.title(\"UMAP Visualization: Train, Test, and Synthetic Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#______________________________________________________\n",
    "  # 4.4: Train mô hình TransformerDetector\n",
    "train_dataset_final = TensorDataset(D_train, y_train)\n",
    "test_dataset = TensorDataset(D_test, y_test)\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "print(\"Sau khi oversampling bằng VAE:\")\n",
    "unique, counts = np.unique(y_train.numpy(), return_counts=True)\n",
    "print(\"Phân phối lớp trong tập train:\", dict(zip(unique, counts)))\n",
    "\n",
    "# model = MixtureOfExperts(input_size=input_dim, num_experts= 10)\n",
    "model = TransformerDetector(input_size=input_dim).to(device)\n",
    "optimizer_tf = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs_tf = 200\n",
    "score_total=0\n",
    "for epoch in range(num_epochs_tf):\n",
    "    train_loss = train_detector(model, train_loader_final, optimizer_tf, criterion, device)\n",
    "    print(f\"[Transformer] Epoch {epoch + 1}/{num_epochs_tf}, Loss={train_loss:.4f}\")\n",
    "\n",
    "    # Đánh giá\n",
    "    print(\"Test set evaluation:\")\n",
    "    r, score = evaluate_with_classification_report_and_auc(model, test_loader, device, threshold=0.4)\n",
    "    print(\"-\" * 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21])\n",
      "tensor([0.0017], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x_orig = X_all[100].to(device).unsqueeze(0)\n",
    "print(x_orig.shape)  # Reshape to batch format (1, d)\n",
    "y_class1 = torch.full((1, 1), 0, device=device)  # Target class label (e.g., 0.8)\n",
    "\n",
    "    # Encode input data into latent space\n",
    "with torch.no_grad():\n",
    "        mean, logvar = beta_cvae.encode(x_orig, y_class1)\n",
    "        z = beta_cvae.reparameterize(mean, logvar).detach().clone()\n",
    "\n",
    "\n",
    "\n",
    "        # Decode latent variable back to data space\n",
    "x_synthetic = beta_cvae.decode(z, y_class1)\n",
    "\n",
    "print(new_detector(x_synthetic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Epoch [1/500] | D_loss: 11.7636 | G_loss: -0.0203\n",
      "Epoch [2/500] | D_loss: 11.3852 | G_loss: -0.0178\n",
      "Epoch [3/500] | D_loss: 11.1204 | G_loss: -0.0155\n",
      "Epoch [4/500] | D_loss: 11.0674 | G_loss: -0.0108\n",
      "Epoch [5/500] | D_loss: 10.4688 | G_loss: -0.0162\n",
      "Epoch [6/500] | D_loss: 10.0931 | G_loss: -0.0094\n",
      "Epoch [7/500] | D_loss: 9.6623 | G_loss: -0.0108\n",
      "Epoch [8/500] | D_loss: 9.2715 | G_loss: -0.0119\n",
      "Epoch [9/500] | D_loss: 8.8547 | G_loss: -0.0102\n",
      "Epoch [10/500] | D_loss: 8.3865 | G_loss: -0.0106\n",
      "Epoch [11/500] | D_loss: 8.2052 | G_loss: -0.0120\n",
      "Epoch [12/500] | D_loss: 7.8424 | G_loss: -0.0122\n",
      "Epoch [13/500] | D_loss: 7.5488 | G_loss: -0.0157\n",
      "Epoch [14/500] | D_loss: 7.2708 | G_loss: -0.0204\n",
      "Epoch [15/500] | D_loss: 6.9207 | G_loss: -0.0144\n",
      "Epoch [16/500] | D_loss: 6.1258 | G_loss: -0.0228\n",
      "Epoch [17/500] | D_loss: 6.0539 | G_loss: -0.0196\n",
      "Epoch [18/500] | D_loss: 5.6838 | G_loss: -0.0264\n",
      "Epoch [19/500] | D_loss: 5.3352 | G_loss: -0.0179\n",
      "Epoch [20/500] | D_loss: 5.0556 | G_loss: -0.0264\n",
      "Epoch [21/500] | D_loss: 5.0322 | G_loss: -0.0348\n",
      "Epoch [22/500] | D_loss: 4.3722 | G_loss: -0.0396\n",
      "Epoch [23/500] | D_loss: 4.3357 | G_loss: -0.0437\n",
      "Epoch [24/500] | D_loss: 3.9120 | G_loss: -0.0686\n",
      "Epoch [25/500] | D_loss: 3.6583 | G_loss: -0.0619\n",
      "Epoch [26/500] | D_loss: 3.3186 | G_loss: -0.0597\n",
      "Epoch [27/500] | D_loss: 3.4190 | G_loss: -0.0611\n",
      "Epoch [28/500] | D_loss: 3.0122 | G_loss: -0.0855\n",
      "Epoch [29/500] | D_loss: 2.6293 | G_loss: -0.1041\n",
      "Epoch [30/500] | D_loss: 2.3653 | G_loss: -0.1099\n",
      "Epoch [31/500] | D_loss: 2.4459 | G_loss: -0.1051\n",
      "Epoch [32/500] | D_loss: 2.2233 | G_loss: -0.1138\n",
      "Epoch [33/500] | D_loss: 1.8461 | G_loss: -0.1370\n",
      "Epoch [34/500] | D_loss: 1.8323 | G_loss: -0.1259\n",
      "Epoch [35/500] | D_loss: 1.7890 | G_loss: -0.1590\n",
      "Epoch [36/500] | D_loss: 1.4236 | G_loss: -0.1356\n",
      "Epoch [37/500] | D_loss: 1.4018 | G_loss: -0.1514\n",
      "Epoch [38/500] | D_loss: 1.0943 | G_loss: -0.1922\n",
      "Epoch [39/500] | D_loss: 0.9953 | G_loss: -0.1885\n",
      "Epoch [40/500] | D_loss: 0.9306 | G_loss: -0.2551\n",
      "Epoch [41/500] | D_loss: 0.8803 | G_loss: -0.2166\n",
      "Epoch [42/500] | D_loss: 0.6988 | G_loss: -0.2246\n",
      "Epoch [43/500] | D_loss: 0.9202 | G_loss: -0.2723\n",
      "Epoch [44/500] | D_loss: 0.7258 | G_loss: -0.2566\n",
      "Epoch [45/500] | D_loss: 0.5701 | G_loss: -0.2838\n",
      "Epoch [46/500] | D_loss: 0.8568 | G_loss: -0.2534\n",
      "Epoch [47/500] | D_loss: 0.8688 | G_loss: -0.3769\n",
      "Epoch [48/500] | D_loss: 0.2545 | G_loss: -0.3243\n",
      "Epoch [49/500] | D_loss: 0.5433 | G_loss: -0.3278\n",
      "Epoch [50/500] | D_loss: 0.3147 | G_loss: -0.3454\n",
      "Epoch [51/500] | D_loss: 0.2300 | G_loss: -0.3441\n",
      "Epoch [52/500] | D_loss: 0.3993 | G_loss: -0.4204\n",
      "Epoch [53/500] | D_loss: 0.3082 | G_loss: -0.4543\n",
      "Epoch [54/500] | D_loss: 0.5862 | G_loss: -0.3846\n",
      "Epoch [55/500] | D_loss: 0.0394 | G_loss: -0.4209\n",
      "Epoch [56/500] | D_loss: 0.4036 | G_loss: -0.4807\n",
      "Epoch [57/500] | D_loss: 0.4020 | G_loss: -0.4535\n",
      "Epoch [58/500] | D_loss: 0.0452 | G_loss: -0.4433\n",
      "Epoch [59/500] | D_loss: 0.3498 | G_loss: -0.4045\n",
      "Epoch [60/500] | D_loss: 0.5929 | G_loss: -0.4916\n",
      "Epoch [61/500] | D_loss: 0.0196 | G_loss: -0.5152\n",
      "Epoch [62/500] | D_loss: 0.0042 | G_loss: -0.5051\n",
      "Epoch [63/500] | D_loss: -0.3204 | G_loss: -0.5079\n",
      "Epoch [64/500] | D_loss: -0.0034 | G_loss: -0.4561\n",
      "Epoch [65/500] | D_loss: 0.1212 | G_loss: -0.6233\n",
      "Epoch [66/500] | D_loss: -0.0223 | G_loss: -0.4180\n",
      "Epoch [67/500] | D_loss: 0.0714 | G_loss: -0.5084\n",
      "Epoch [68/500] | D_loss: 0.3741 | G_loss: -0.4922\n",
      "Epoch [69/500] | D_loss: -0.0082 | G_loss: -0.5422\n",
      "Epoch [70/500] | D_loss: -0.3651 | G_loss: -0.6389\n",
      "Epoch [71/500] | D_loss: -0.2901 | G_loss: -0.4722\n",
      "Epoch [72/500] | D_loss: -0.4150 | G_loss: -0.5656\n",
      "Epoch [73/500] | D_loss: -0.6964 | G_loss: -0.4740\n",
      "Epoch [74/500] | D_loss: 0.1270 | G_loss: -0.6468\n",
      "Epoch [75/500] | D_loss: -0.4099 | G_loss: -0.6196\n",
      "Epoch [76/500] | D_loss: -0.4149 | G_loss: -0.4727\n",
      "Epoch [77/500] | D_loss: -0.5383 | G_loss: -0.5566\n",
      "Epoch [78/500] | D_loss: -0.3335 | G_loss: -0.5515\n",
      "Epoch [79/500] | D_loss: -0.6514 | G_loss: -0.7385\n",
      "Epoch [80/500] | D_loss: 0.1302 | G_loss: -0.7213\n",
      "Epoch [81/500] | D_loss: -0.3509 | G_loss: -0.5841\n",
      "Epoch [82/500] | D_loss: -0.3927 | G_loss: -0.5424\n",
      "Epoch [83/500] | D_loss: -0.6500 | G_loss: -0.6781\n",
      "Epoch [84/500] | D_loss: -0.2349 | G_loss: -0.4539\n",
      "Epoch [85/500] | D_loss: -0.5799 | G_loss: -0.7671\n",
      "Epoch [86/500] | D_loss: -0.4301 | G_loss: -0.6696\n",
      "Epoch [87/500] | D_loss: -0.1967 | G_loss: -0.6058\n",
      "Epoch [88/500] | D_loss: -0.8320 | G_loss: -0.6225\n",
      "Epoch [89/500] | D_loss: -0.3378 | G_loss: -0.7419\n",
      "Epoch [90/500] | D_loss: -0.4198 | G_loss: -0.6993\n",
      "Epoch [91/500] | D_loss: -0.7494 | G_loss: -0.5284\n",
      "Epoch [92/500] | D_loss: -0.3222 | G_loss: -0.7613\n",
      "Epoch [93/500] | D_loss: -0.3704 | G_loss: -0.6644\n",
      "Epoch [94/500] | D_loss: -0.3580 | G_loss: -0.6207\n",
      "Epoch [95/500] | D_loss: -0.3997 | G_loss: -0.7087\n",
      "Epoch [96/500] | D_loss: -1.0267 | G_loss: -0.7328\n",
      "Epoch [97/500] | D_loss: -0.3965 | G_loss: -0.6480\n",
      "Epoch [98/500] | D_loss: -0.7643 | G_loss: -0.5172\n",
      "Epoch [99/500] | D_loss: -0.6540 | G_loss: -0.7825\n",
      "Epoch [100/500] | D_loss: -0.9718 | G_loss: -0.6871\n",
      "Epoch [101/500] | D_loss: -0.3775 | G_loss: -0.5973\n",
      "Epoch [102/500] | D_loss: -1.0685 | G_loss: -0.7945\n",
      "Epoch [103/500] | D_loss: -0.9065 | G_loss: -0.7884\n",
      "Epoch [104/500] | D_loss: -0.8090 | G_loss: -0.5740\n",
      "Epoch [105/500] | D_loss: -0.9317 | G_loss: -0.8950\n",
      "Epoch [106/500] | D_loss: -1.2654 | G_loss: -0.7099\n",
      "Epoch [107/500] | D_loss: -0.9055 | G_loss: -0.9039\n",
      "Epoch [108/500] | D_loss: -0.6214 | G_loss: -0.5625\n",
      "Epoch [109/500] | D_loss: -1.1356 | G_loss: -0.7612\n",
      "Epoch [110/500] | D_loss: -0.9019 | G_loss: -0.6691\n",
      "Epoch [111/500] | D_loss: -1.0801 | G_loss: -0.9358\n",
      "Epoch [112/500] | D_loss: -1.1751 | G_loss: -0.8556\n",
      "Epoch [113/500] | D_loss: -1.1235 | G_loss: -0.8196\n",
      "Epoch [114/500] | D_loss: -1.2134 | G_loss: -0.8592\n",
      "Epoch [115/500] | D_loss: -0.7777 | G_loss: -0.8817\n",
      "Epoch [116/500] | D_loss: -1.5645 | G_loss: -0.9949\n",
      "Epoch [117/500] | D_loss: -1.7171 | G_loss: -0.7450\n",
      "Epoch [118/500] | D_loss: -1.5264 | G_loss: -0.4394\n",
      "Epoch [119/500] | D_loss: -2.3697 | G_loss: -0.9466\n",
      "Epoch [120/500] | D_loss: -1.8895 | G_loss: -0.7460\n",
      "Epoch [121/500] | D_loss: -1.1673 | G_loss: -0.7194\n",
      "Epoch [122/500] | D_loss: -1.7820 | G_loss: -0.7436\n",
      "Epoch [123/500] | D_loss: -2.0865 | G_loss: -0.6775\n",
      "Epoch [124/500] | D_loss: -1.7621 | G_loss: -0.8142\n",
      "Epoch [125/500] | D_loss: -1.7190 | G_loss: -0.4578\n",
      "Epoch [126/500] | D_loss: -2.0128 | G_loss: -0.7323\n",
      "Epoch [127/500] | D_loss: -1.4934 | G_loss: -0.7797\n",
      "Epoch [128/500] | D_loss: -2.0382 | G_loss: -0.5528\n",
      "Epoch [129/500] | D_loss: -1.7586 | G_loss: -0.9499\n",
      "Epoch [130/500] | D_loss: -1.4285 | G_loss: -0.6782\n",
      "Epoch [131/500] | D_loss: -1.8897 | G_loss: -0.6238\n",
      "Epoch [132/500] | D_loss: -2.2792 | G_loss: -0.8455\n",
      "Epoch [133/500] | D_loss: -2.1996 | G_loss: -0.5146\n",
      "Epoch [134/500] | D_loss: -1.6064 | G_loss: -0.8812\n",
      "Epoch [135/500] | D_loss: -1.7699 | G_loss: -0.9504\n",
      "Epoch [136/500] | D_loss: -2.1837 | G_loss: -0.6656\n",
      "Epoch [137/500] | D_loss: -2.2104 | G_loss: -0.7260\n",
      "Epoch [138/500] | D_loss: -2.2820 | G_loss: -0.4853\n",
      "Epoch [139/500] | D_loss: -2.2597 | G_loss: -0.7750\n",
      "Epoch [140/500] | D_loss: -2.6908 | G_loss: -0.6539\n",
      "Epoch [141/500] | D_loss: -2.0455 | G_loss: -0.1693\n",
      "Epoch [142/500] | D_loss: -2.2386 | G_loss: -0.5452\n",
      "Epoch [143/500] | D_loss: -2.4390 | G_loss: -0.7406\n",
      "Epoch [144/500] | D_loss: -2.5043 | G_loss: -0.5797\n",
      "Epoch [145/500] | D_loss: -2.3879 | G_loss: -1.0984\n",
      "Epoch [146/500] | D_loss: -2.4091 | G_loss: -0.7420\n",
      "Epoch [147/500] | D_loss: -2.9392 | G_loss: -0.5434\n",
      "Epoch [148/500] | D_loss: -2.6517 | G_loss: -0.7419\n",
      "Epoch [149/500] | D_loss: -3.6454 | G_loss: -0.4164\n",
      "Epoch [150/500] | D_loss: -2.3810 | G_loss: -0.4320\n",
      "Epoch [151/500] | D_loss: -2.5520 | G_loss: -0.5471\n",
      "Epoch [152/500] | D_loss: -2.8806 | G_loss: -0.6303\n",
      "Epoch [153/500] | D_loss: -2.9943 | G_loss: -0.5715\n",
      "Epoch [154/500] | D_loss: -3.0227 | G_loss: -0.4388\n",
      "Epoch [155/500] | D_loss: -3.4709 | G_loss: -0.6067\n",
      "Epoch [156/500] | D_loss: -3.1539 | G_loss: -0.3605\n",
      "Epoch [157/500] | D_loss: -3.6021 | G_loss: -0.1139\n",
      "Epoch [158/500] | D_loss: -3.3197 | G_loss: -0.1912\n",
      "Epoch [159/500] | D_loss: -3.4012 | G_loss: -0.4847\n",
      "Epoch [160/500] | D_loss: -3.7834 | G_loss: -0.4691\n",
      "Epoch [161/500] | D_loss: -4.2045 | G_loss: -0.3268\n",
      "Epoch [162/500] | D_loss: -3.0566 | G_loss: -0.6319\n",
      "Epoch [163/500] | D_loss: -3.7180 | G_loss: 0.0124\n",
      "Epoch [164/500] | D_loss: -3.7625 | G_loss: -0.6076\n",
      "Epoch [165/500] | D_loss: -4.1671 | G_loss: -0.5822\n",
      "Epoch [166/500] | D_loss: -3.7503 | G_loss: -0.2288\n",
      "Epoch [167/500] | D_loss: -4.4175 | G_loss: -1.0523\n",
      "Epoch [168/500] | D_loss: -4.4090 | G_loss: -0.1888\n",
      "Epoch [169/500] | D_loss: -4.1984 | G_loss: -0.1697\n",
      "Epoch [170/500] | D_loss: -4.6988 | G_loss: -0.3638\n",
      "Epoch [171/500] | D_loss: -3.4575 | G_loss: -0.4396\n",
      "Epoch [172/500] | D_loss: -4.4129 | G_loss: 0.4344\n",
      "Epoch [173/500] | D_loss: -2.6062 | G_loss: -0.3193\n",
      "Epoch [174/500] | D_loss: -4.4563 | G_loss: -0.4459\n",
      "Epoch [175/500] | D_loss: -4.7583 | G_loss: 0.0253\n",
      "Epoch [176/500] | D_loss: -4.0846 | G_loss: 0.0445\n",
      "Epoch [177/500] | D_loss: -4.5192 | G_loss: -0.0134\n",
      "Epoch [178/500] | D_loss: -4.1661 | G_loss: 0.2625\n",
      "Epoch [179/500] | D_loss: -5.5883 | G_loss: 0.1375\n",
      "Epoch [180/500] | D_loss: -4.8761 | G_loss: -0.0196\n",
      "Epoch [181/500] | D_loss: -5.4426 | G_loss: 0.0930\n",
      "Epoch [182/500] | D_loss: -5.5477 | G_loss: 0.1767\n",
      "Epoch [183/500] | D_loss: -4.4561 | G_loss: -0.1552\n",
      "Epoch [184/500] | D_loss: -4.1652 | G_loss: -0.0502\n",
      "Epoch [185/500] | D_loss: -4.7012 | G_loss: 0.2474\n",
      "Epoch [186/500] | D_loss: -3.5831 | G_loss: 0.4641\n",
      "Epoch [187/500] | D_loss: -4.9674 | G_loss: -0.1939\n",
      "Epoch [188/500] | D_loss: -4.9703 | G_loss: 0.3143\n",
      "Epoch [189/500] | D_loss: -5.7603 | G_loss: 0.2156\n",
      "Epoch [190/500] | D_loss: -3.7910 | G_loss: 1.1413\n",
      "Epoch [191/500] | D_loss: -4.9842 | G_loss: 0.0727\n",
      "Epoch [192/500] | D_loss: -4.7343 | G_loss: 0.7017\n",
      "Epoch [193/500] | D_loss: -5.5433 | G_loss: 0.2101\n",
      "Epoch [194/500] | D_loss: -5.0403 | G_loss: 0.2430\n",
      "Epoch [195/500] | D_loss: -6.3596 | G_loss: -0.3939\n",
      "Epoch [196/500] | D_loss: -4.9566 | G_loss: 0.4820\n",
      "Epoch [197/500] | D_loss: -6.6519 | G_loss: 0.0514\n",
      "Epoch [198/500] | D_loss: -6.4848 | G_loss: 0.8437\n",
      "Epoch [199/500] | D_loss: -6.8833 | G_loss: 0.4266\n",
      "Epoch [200/500] | D_loss: -5.6170 | G_loss: 0.4226\n",
      "Epoch [201/500] | D_loss: -4.2764 | G_loss: -0.0017\n",
      "Epoch [202/500] | D_loss: -4.9390 | G_loss: 0.2306\n",
      "Epoch [203/500] | D_loss: -5.8051 | G_loss: 0.9826\n",
      "Epoch [204/500] | D_loss: -3.4162 | G_loss: -0.1987\n",
      "Epoch [205/500] | D_loss: -6.9698 | G_loss: 0.7711\n",
      "Epoch [206/500] | D_loss: -5.3478 | G_loss: -0.1818\n",
      "Epoch [207/500] | D_loss: -5.8176 | G_loss: 0.4561\n",
      "Epoch [208/500] | D_loss: -4.1422 | G_loss: 0.4843\n",
      "Epoch [209/500] | D_loss: -6.3543 | G_loss: 0.1145\n",
      "Epoch [210/500] | D_loss: -7.9081 | G_loss: 0.5006\n",
      "Epoch [211/500] | D_loss: -4.6635 | G_loss: 0.6319\n",
      "Epoch [212/500] | D_loss: -6.5140 | G_loss: 0.9421\n",
      "Epoch [213/500] | D_loss: -5.7730 | G_loss: 0.4170\n",
      "Epoch [214/500] | D_loss: -6.3883 | G_loss: 0.8129\n",
      "Epoch [215/500] | D_loss: -5.1317 | G_loss: 0.4512\n",
      "Epoch [216/500] | D_loss: -8.6395 | G_loss: 1.5653\n",
      "Epoch [217/500] | D_loss: -6.4624 | G_loss: 2.0233\n",
      "Epoch [218/500] | D_loss: -6.3279 | G_loss: 1.1835\n",
      "Epoch [219/500] | D_loss: -5.9885 | G_loss: 0.2144\n",
      "Epoch [220/500] | D_loss: -6.4017 | G_loss: -0.0465\n",
      "Epoch [221/500] | D_loss: -6.3120 | G_loss: 0.3844\n",
      "Epoch [222/500] | D_loss: -6.2875 | G_loss: 0.8423\n",
      "Epoch [223/500] | D_loss: -5.9802 | G_loss: 1.1759\n",
      "Epoch [224/500] | D_loss: -5.6499 | G_loss: 0.8887\n",
      "Epoch [225/500] | D_loss: -8.4475 | G_loss: -0.3069\n",
      "Epoch [226/500] | D_loss: -5.4999 | G_loss: -0.1314\n",
      "Epoch [227/500] | D_loss: -7.2466 | G_loss: 0.4817\n",
      "Epoch [228/500] | D_loss: -7.1117 | G_loss: 0.4979\n",
      "Epoch [229/500] | D_loss: -8.0698 | G_loss: 0.8271\n",
      "Epoch [230/500] | D_loss: -4.6888 | G_loss: 0.6807\n",
      "Epoch [231/500] | D_loss: -6.6440 | G_loss: 0.5371\n",
      "Epoch [232/500] | D_loss: -5.1464 | G_loss: 0.5474\n",
      "Epoch [233/500] | D_loss: -7.4217 | G_loss: 0.8895\n",
      "Epoch [234/500] | D_loss: -7.1943 | G_loss: 1.7370\n",
      "Epoch [235/500] | D_loss: -6.4304 | G_loss: 0.4282\n",
      "Epoch [236/500] | D_loss: -7.4658 | G_loss: 1.7686\n",
      "Epoch [237/500] | D_loss: -7.6503 | G_loss: 0.9619\n",
      "Epoch [238/500] | D_loss: -7.8572 | G_loss: 1.1379\n",
      "Epoch [239/500] | D_loss: -9.0276 | G_loss: 2.0504\n",
      "Epoch [240/500] | D_loss: -8.1425 | G_loss: 0.8554\n",
      "Epoch [241/500] | D_loss: -8.3273 | G_loss: 1.0266\n",
      "Epoch [242/500] | D_loss: -5.4144 | G_loss: 1.0550\n",
      "Epoch [243/500] | D_loss: -9.9586 | G_loss: 1.4483\n",
      "Epoch [244/500] | D_loss: -8.2579 | G_loss: -0.2137\n",
      "Epoch [245/500] | D_loss: -9.3503 | G_loss: 1.2742\n",
      "Epoch [246/500] | D_loss: -7.4186 | G_loss: 0.9262\n",
      "Epoch [247/500] | D_loss: -8.8011 | G_loss: 0.4119\n",
      "Epoch [248/500] | D_loss: -6.2954 | G_loss: 0.5873\n",
      "Epoch [249/500] | D_loss: -6.3913 | G_loss: 1.3691\n",
      "Epoch [250/500] | D_loss: -4.9598 | G_loss: 0.4195\n",
      "Epoch [251/500] | D_loss: -9.1298 | G_loss: 1.2001\n",
      "Epoch [252/500] | D_loss: -6.5386 | G_loss: 0.0344\n",
      "Epoch [253/500] | D_loss: -6.9793 | G_loss: 2.2070\n",
      "Epoch [254/500] | D_loss: -7.4152 | G_loss: 0.0777\n",
      "Epoch [255/500] | D_loss: -6.8648 | G_loss: 2.4841\n",
      "Epoch [256/500] | D_loss: -7.7932 | G_loss: 0.4703\n",
      "Epoch [257/500] | D_loss: -7.2196 | G_loss: 1.3190\n",
      "Epoch [258/500] | D_loss: -9.2369 | G_loss: 0.5114\n",
      "Epoch [259/500] | D_loss: -6.1944 | G_loss: 0.4994\n",
      "Epoch [260/500] | D_loss: -7.8393 | G_loss: 1.3314\n",
      "Epoch [261/500] | D_loss: -6.6992 | G_loss: 1.3540\n",
      "Epoch [262/500] | D_loss: -9.5032 | G_loss: 0.5448\n",
      "Epoch [263/500] | D_loss: -7.8574 | G_loss: 1.6259\n",
      "Epoch [264/500] | D_loss: -9.6320 | G_loss: 2.7180\n",
      "Epoch [265/500] | D_loss: -8.6456 | G_loss: 1.4227\n",
      "Epoch [266/500] | D_loss: -8.5561 | G_loss: 1.4259\n",
      "Epoch [267/500] | D_loss: -9.2922 | G_loss: 1.9248\n",
      "Epoch [268/500] | D_loss: -8.7868 | G_loss: -1.3898\n",
      "Epoch [269/500] | D_loss: -7.2295 | G_loss: 2.4032\n",
      "Epoch [270/500] | D_loss: -9.6427 | G_loss: 2.6439\n",
      "Epoch [271/500] | D_loss: -9.1097 | G_loss: 1.0678\n",
      "Epoch [272/500] | D_loss: -7.7946 | G_loss: 3.1816\n",
      "Epoch [273/500] | D_loss: -8.0589 | G_loss: 1.1363\n",
      "Epoch [274/500] | D_loss: -8.5290 | G_loss: 2.2808\n",
      "Epoch [275/500] | D_loss: -12.4000 | G_loss: 0.6740\n",
      "Epoch [276/500] | D_loss: -10.5552 | G_loss: 2.1169\n",
      "Epoch [277/500] | D_loss: -7.7519 | G_loss: 2.1731\n",
      "Epoch [278/500] | D_loss: -10.1090 | G_loss: 0.9390\n",
      "Epoch [279/500] | D_loss: -7.2151 | G_loss: 1.9231\n",
      "Epoch [280/500] | D_loss: -9.0752 | G_loss: 1.9596\n",
      "Epoch [281/500] | D_loss: -8.6054 | G_loss: 1.7052\n",
      "Epoch [282/500] | D_loss: -12.0393 | G_loss: -0.5073\n",
      "Epoch [283/500] | D_loss: -9.7309 | G_loss: 1.0024\n",
      "Epoch [284/500] | D_loss: -8.6343 | G_loss: 3.5753\n",
      "Epoch [285/500] | D_loss: -8.5702 | G_loss: 2.5655\n",
      "Epoch [286/500] | D_loss: -10.2520 | G_loss: 3.6110\n",
      "Epoch [287/500] | D_loss: -11.8399 | G_loss: 2.0825\n",
      "Epoch [288/500] | D_loss: -10.3786 | G_loss: 0.7919\n",
      "Epoch [289/500] | D_loss: -8.7564 | G_loss: 0.2870\n",
      "Epoch [290/500] | D_loss: -12.3369 | G_loss: 1.3367\n",
      "Epoch [291/500] | D_loss: -9.4860 | G_loss: -0.2105\n",
      "Epoch [292/500] | D_loss: -8.1908 | G_loss: 1.1266\n",
      "Epoch [293/500] | D_loss: -7.9565 | G_loss: 3.0499\n",
      "Epoch [294/500] | D_loss: -11.1214 | G_loss: 0.0387\n",
      "Epoch [295/500] | D_loss: -9.6424 | G_loss: 2.8140\n",
      "Epoch [296/500] | D_loss: -13.0328 | G_loss: 1.4606\n",
      "Epoch [297/500] | D_loss: -10.1753 | G_loss: 1.1825\n",
      "Epoch [298/500] | D_loss: -10.8040 | G_loss: 2.0543\n",
      "Epoch [299/500] | D_loss: -8.4605 | G_loss: 0.0816\n",
      "Epoch [300/500] | D_loss: -15.0385 | G_loss: 2.3851\n",
      "Epoch [301/500] | D_loss: -9.4783 | G_loss: 1.2074\n",
      "Epoch [302/500] | D_loss: -10.6563 | G_loss: 0.0905\n",
      "Epoch [303/500] | D_loss: -10.7580 | G_loss: 3.3396\n",
      "Epoch [304/500] | D_loss: -10.4295 | G_loss: 2.4729\n",
      "Epoch [305/500] | D_loss: -6.7424 | G_loss: 0.9799\n",
      "Epoch [306/500] | D_loss: -12.1986 | G_loss: 3.1159\n",
      "Epoch [307/500] | D_loss: -14.1531 | G_loss: 2.5205\n",
      "Epoch [308/500] | D_loss: -8.4740 | G_loss: 1.3585\n",
      "Epoch [309/500] | D_loss: -11.3703 | G_loss: 2.2647\n",
      "Epoch [310/500] | D_loss: -12.8890 | G_loss: 2.8809\n",
      "Epoch [311/500] | D_loss: -7.7433 | G_loss: 2.2617\n",
      "Epoch [312/500] | D_loss: -14.1505 | G_loss: -0.1774\n",
      "Epoch [313/500] | D_loss: -13.1714 | G_loss: 0.4367\n",
      "Epoch [314/500] | D_loss: -10.6651 | G_loss: 3.3311\n",
      "Epoch [315/500] | D_loss: -11.1219 | G_loss: 1.4397\n",
      "Epoch [316/500] | D_loss: -8.6458 | G_loss: 2.7492\n",
      "Epoch [317/500] | D_loss: -8.8223 | G_loss: -0.8567\n",
      "Epoch [318/500] | D_loss: -14.7203 | G_loss: 0.8215\n",
      "Epoch [319/500] | D_loss: -10.4371 | G_loss: 1.1790\n",
      "Epoch [320/500] | D_loss: -10.7487 | G_loss: 1.8574\n",
      "Epoch [321/500] | D_loss: -11.3869 | G_loss: 1.8585\n",
      "Epoch [322/500] | D_loss: -14.0858 | G_loss: 2.9113\n",
      "Epoch [323/500] | D_loss: -12.6949 | G_loss: 2.5472\n",
      "Epoch [324/500] | D_loss: -8.6602 | G_loss: 1.1901\n",
      "Epoch [325/500] | D_loss: -11.1981 | G_loss: 6.4067\n",
      "Epoch [326/500] | D_loss: -12.8387 | G_loss: 4.7428\n",
      "Epoch [327/500] | D_loss: -7.8299 | G_loss: 2.6484\n",
      "Epoch [328/500] | D_loss: -10.7823 | G_loss: 2.7080\n",
      "Epoch [329/500] | D_loss: -11.2226 | G_loss: 2.0093\n",
      "Epoch [330/500] | D_loss: -11.2742 | G_loss: 1.9768\n",
      "Epoch [331/500] | D_loss: -13.0423 | G_loss: 2.3975\n",
      "Epoch [332/500] | D_loss: -9.9197 | G_loss: 1.2939\n",
      "Epoch [333/500] | D_loss: -11.3812 | G_loss: 3.1644\n",
      "Epoch [334/500] | D_loss: -8.9327 | G_loss: 2.0972\n",
      "Epoch [335/500] | D_loss: -10.6116 | G_loss: 4.6685\n",
      "Epoch [336/500] | D_loss: -10.2794 | G_loss: 3.5702\n",
      "Epoch [337/500] | D_loss: -9.7560 | G_loss: 1.3405\n",
      "Epoch [338/500] | D_loss: -14.7489 | G_loss: 4.8028\n",
      "Epoch [339/500] | D_loss: -16.8358 | G_loss: 1.4199\n",
      "Epoch [340/500] | D_loss: -14.2166 | G_loss: 1.4237\n",
      "Epoch [341/500] | D_loss: -14.6914 | G_loss: 3.3969\n",
      "Epoch [342/500] | D_loss: -10.4714 | G_loss: 3.0302\n",
      "Epoch [343/500] | D_loss: -8.4730 | G_loss: 1.4535\n",
      "Epoch [344/500] | D_loss: -11.4851 | G_loss: 4.2538\n",
      "Epoch [345/500] | D_loss: -13.5969 | G_loss: 2.6954\n",
      "Epoch [346/500] | D_loss: -11.2042 | G_loss: 4.2949\n",
      "Epoch [347/500] | D_loss: -12.6491 | G_loss: 1.9252\n",
      "Epoch [348/500] | D_loss: -12.4039 | G_loss: 1.5403\n",
      "Epoch [349/500] | D_loss: -15.6407 | G_loss: 4.8327\n",
      "Epoch [350/500] | D_loss: -11.6782 | G_loss: 2.3733\n",
      "Epoch [351/500] | D_loss: -14.6308 | G_loss: 3.6749\n",
      "Epoch [352/500] | D_loss: -15.8705 | G_loss: 2.8622\n",
      "Epoch [353/500] | D_loss: -14.5715 | G_loss: 2.4505\n",
      "Epoch [354/500] | D_loss: -18.9256 | G_loss: 4.5798\n",
      "Epoch [355/500] | D_loss: -13.8918 | G_loss: 2.9317\n",
      "Epoch [356/500] | D_loss: -14.3510 | G_loss: 2.5054\n",
      "Epoch [357/500] | D_loss: -15.0209 | G_loss: 1.2390\n",
      "Epoch [358/500] | D_loss: -12.3460 | G_loss: 3.0237\n",
      "Epoch [359/500] | D_loss: -16.9866 | G_loss: 1.7152\n",
      "Epoch [360/500] | D_loss: -11.9496 | G_loss: 3.4639\n",
      "Epoch [361/500] | D_loss: -14.2265 | G_loss: 2.1800\n",
      "Epoch [362/500] | D_loss: -13.5348 | G_loss: 5.3601\n",
      "Epoch [363/500] | D_loss: -14.4995 | G_loss: 5.3696\n",
      "Epoch [364/500] | D_loss: -1.7270 | G_loss: 4.0770\n",
      "Epoch [365/500] | D_loss: -14.0957 | G_loss: 3.1566\n",
      "Epoch [366/500] | D_loss: -16.8580 | G_loss: 1.4110\n",
      "Epoch [367/500] | D_loss: -13.2551 | G_loss: 4.6208\n",
      "Epoch [368/500] | D_loss: -15.1740 | G_loss: 1.8833\n",
      "Epoch [369/500] | D_loss: -15.8805 | G_loss: -0.4498\n",
      "Epoch [370/500] | D_loss: -19.7230 | G_loss: 5.6497\n",
      "Epoch [371/500] | D_loss: -13.7991 | G_loss: 0.9893\n",
      "Epoch [372/500] | D_loss: -11.3685 | G_loss: 6.2587\n",
      "Epoch [373/500] | D_loss: -14.2619 | G_loss: 1.0234\n",
      "Epoch [374/500] | D_loss: -17.4281 | G_loss: 3.9235\n",
      "Epoch [375/500] | D_loss: -16.3425 | G_loss: 1.0374\n",
      "Epoch [376/500] | D_loss: -14.3676 | G_loss: 2.5184\n",
      "Epoch [377/500] | D_loss: -10.2373 | G_loss: 5.9763\n",
      "Epoch [378/500] | D_loss: -20.0120 | G_loss: 3.0372\n",
      "Epoch [379/500] | D_loss: -10.7193 | G_loss: 3.5720\n",
      "Epoch [380/500] | D_loss: -16.1289 | G_loss: 1.6125\n",
      "Epoch [381/500] | D_loss: -14.4179 | G_loss: 6.1573\n",
      "Epoch [382/500] | D_loss: -15.1882 | G_loss: 6.1746\n",
      "Epoch [383/500] | D_loss: -19.6235 | G_loss: 2.1764\n",
      "Epoch [384/500] | D_loss: -16.4768 | G_loss: 3.7270\n",
      "Epoch [385/500] | D_loss: -18.8760 | G_loss: 8.3929\n",
      "Epoch [386/500] | D_loss: -11.0751 | G_loss: 8.4821\n",
      "Epoch [387/500] | D_loss: -20.4510 | G_loss: 4.8684\n",
      "Epoch [388/500] | D_loss: -15.7066 | G_loss: 4.3688\n",
      "Epoch [389/500] | D_loss: -15.1084 | G_loss: 5.4705\n",
      "Epoch [390/500] | D_loss: -18.3030 | G_loss: 4.9872\n",
      "Epoch [391/500] | D_loss: -15.4130 | G_loss: 3.3923\n",
      "Epoch [392/500] | D_loss: -8.5226 | G_loss: 1.2642\n",
      "Epoch [393/500] | D_loss: -10.0992 | G_loss: 4.5471\n",
      "Epoch [394/500] | D_loss: -12.3941 | G_loss: 5.1275\n",
      "Epoch [395/500] | D_loss: -14.2404 | G_loss: 6.8195\n",
      "Epoch [396/500] | D_loss: -16.2884 | G_loss: 3.0140\n",
      "Epoch [397/500] | D_loss: -23.5454 | G_loss: 4.7119\n",
      "Epoch [398/500] | D_loss: -17.3051 | G_loss: 5.2897\n",
      "Epoch [399/500] | D_loss: -12.3368 | G_loss: 5.3138\n",
      "Epoch [400/500] | D_loss: -8.4311 | G_loss: 5.3739\n",
      "Epoch [401/500] | D_loss: -18.9547 | G_loss: 4.2588\n",
      "Epoch [402/500] | D_loss: -23.4186 | G_loss: 4.3266\n",
      "Epoch [403/500] | D_loss: -15.2096 | G_loss: 6.6520\n",
      "Epoch [404/500] | D_loss: -11.2224 | G_loss: 1.4754\n",
      "Epoch [405/500] | D_loss: -15.6258 | G_loss: 6.1698\n",
      "Epoch [406/500] | D_loss: -16.5177 | G_loss: 5.0049\n",
      "Epoch [407/500] | D_loss: -14.4183 | G_loss: 5.0740\n",
      "Epoch [408/500] | D_loss: -22.6680 | G_loss: 4.5030\n",
      "Epoch [409/500] | D_loss: -26.4463 | G_loss: 0.9424\n",
      "Epoch [410/500] | D_loss: -17.4530 | G_loss: 10.0409\n",
      "Epoch [411/500] | D_loss: -9.4453 | G_loss: 7.0924\n",
      "Epoch [412/500] | D_loss: -9.8322 | G_loss: 4.6893\n",
      "Epoch [413/500] | D_loss: -13.2796 | G_loss: 5.9595\n",
      "Epoch [414/500] | D_loss: -16.2874 | G_loss: 7.8371\n",
      "Epoch [415/500] | D_loss: -14.9595 | G_loss: 3.5286\n",
      "Epoch [416/500] | D_loss: -20.9284 | G_loss: 7.9542\n",
      "Epoch [417/500] | D_loss: -16.7141 | G_loss: 9.2811\n",
      "Epoch [418/500] | D_loss: -23.7291 | G_loss: 7.4209\n",
      "Epoch [419/500] | D_loss: -14.6282 | G_loss: 8.7526\n",
      "Epoch [420/500] | D_loss: -18.7755 | G_loss: 8.1825\n",
      "Epoch [421/500] | D_loss: -19.5335 | G_loss: 3.6991\n",
      "Epoch [422/500] | D_loss: -21.0343 | G_loss: 5.6943\n",
      "Epoch [423/500] | D_loss: -21.9031 | G_loss: 5.7311\n",
      "Epoch [424/500] | D_loss: -20.0119 | G_loss: 6.4506\n",
      "Epoch [425/500] | D_loss: -24.5196 | G_loss: 2.5224\n",
      "Epoch [426/500] | D_loss: -29.2001 | G_loss: 9.8672\n",
      "Epoch [427/500] | D_loss: -23.3036 | G_loss: 3.9219\n",
      "Epoch [428/500] | D_loss: -23.2245 | G_loss: 5.9663\n",
      "Epoch [429/500] | D_loss: -11.3406 | G_loss: 3.9726\n",
      "Epoch [430/500] | D_loss: -15.6071 | G_loss: 6.0578\n",
      "Epoch [431/500] | D_loss: -12.8486 | G_loss: 0.6209\n",
      "Epoch [432/500] | D_loss: -21.3231 | G_loss: 5.4437\n",
      "Epoch [433/500] | D_loss: -22.9458 | G_loss: 6.8754\n",
      "Epoch [434/500] | D_loss: -19.6222 | G_loss: 2.7482\n",
      "Epoch [435/500] | D_loss: -21.4338 | G_loss: 2.7751\n",
      "Epoch [436/500] | D_loss: -23.3589 | G_loss: 7.0599\n",
      "Epoch [437/500] | D_loss: -21.1864 | G_loss: 7.8176\n",
      "Epoch [438/500] | D_loss: -24.8767 | G_loss: 2.1419\n",
      "Epoch [439/500] | D_loss: -15.0926 | G_loss: 10.0813\n",
      "Epoch [440/500] | D_loss: -30.3918 | G_loss: 10.1633\n",
      "Epoch [441/500] | D_loss: -20.5575 | G_loss: 10.9450\n",
      "Epoch [442/500] | D_loss: -23.0129 | G_loss: 4.4401\n",
      "Epoch [443/500] | D_loss: -19.1546 | G_loss: 8.1511\n",
      "Epoch [444/500] | D_loss: -20.1121 | G_loss: 6.7353\n",
      "Epoch [445/500] | D_loss: -14.9885 | G_loss: 6.7814\n",
      "Epoch [446/500] | D_loss: -17.8165 | G_loss: 12.0716\n",
      "Epoch [447/500] | D_loss: -13.0461 | G_loss: 7.6390\n",
      "Epoch [448/500] | D_loss: -31.0413 | G_loss: 6.1685\n",
      "Epoch [449/500] | D_loss: -26.1404 | G_loss: 6.2286\n",
      "Epoch [450/500] | D_loss: -9.9359 | G_loss: 0.1339\n",
      "Epoch [451/500] | D_loss: -22.9357 | G_loss: 7.8658\n",
      "Epoch [452/500] | D_loss: -22.7597 | G_loss: 1.7139\n",
      "Epoch [453/500] | D_loss: -15.9068 | G_loss: -0.5859\n",
      "Epoch [454/500] | D_loss: -17.6361 | G_loss: 4.9237\n",
      "Epoch [455/500] | D_loss: -24.0488 | G_loss: 4.9732\n",
      "Epoch [456/500] | D_loss: -20.2221 | G_loss: 0.2286\n",
      "Epoch [457/500] | D_loss: -29.1525 | G_loss: 2.6205\n",
      "Epoch [458/500] | D_loss: -11.4992 | G_loss: 14.7313\n",
      "Epoch [459/500] | D_loss: -19.5266 | G_loss: 9.1580\n",
      "Epoch [460/500] | D_loss: -18.6947 | G_loss: -2.1594\n",
      "Epoch [461/500] | D_loss: -19.6988 | G_loss: 4.3636\n",
      "Epoch [462/500] | D_loss: -22.2546 | G_loss: 8.5061\n",
      "Epoch [463/500] | D_loss: -21.4816 | G_loss: 6.9371\n",
      "Epoch [464/500] | D_loss: -26.1539 | G_loss: 5.3194\n",
      "Epoch [465/500] | D_loss: -20.7653 | G_loss: 9.5438\n",
      "Epoch [466/500] | D_loss: -10.2935 | G_loss: 13.8259\n",
      "Epoch [467/500] | D_loss: -19.2133 | G_loss: 7.9675\n",
      "Epoch [468/500] | D_loss: -25.1755 | G_loss: 8.8753\n",
      "Epoch [469/500] | D_loss: -23.1946 | G_loss: 5.5338\n",
      "Epoch [470/500] | D_loss: -19.7862 | G_loss: 3.8758\n",
      "Epoch [471/500] | D_loss: -19.5603 | G_loss: 8.2142\n",
      "Epoch [472/500] | D_loss: -23.6703 | G_loss: 3.9384\n",
      "Epoch [473/500] | D_loss: -18.6631 | G_loss: 8.3633\n",
      "Epoch [474/500] | D_loss: -21.9722 | G_loss: 7.5306\n",
      "Epoch [475/500] | D_loss: -28.0511 | G_loss: 7.6079\n",
      "Epoch [476/500] | D_loss: -24.9901 | G_loss: 11.1967\n",
      "Epoch [477/500] | D_loss: -21.0916 | G_loss: 10.3846\n",
      "Epoch [478/500] | D_loss: -31.6941 | G_loss: 7.7553\n",
      "Epoch [479/500] | D_loss: -12.2983 | G_loss: 6.0087\n",
      "Epoch [480/500] | D_loss: -32.9539 | G_loss: 8.7781\n",
      "Epoch [481/500] | D_loss: -26.5736 | G_loss: -0.3055\n",
      "Epoch [482/500] | D_loss: -29.1296 | G_loss: 8.9027\n",
      "Epoch [483/500] | D_loss: -28.9483 | G_loss: 11.7582\n",
      "Epoch [484/500] | D_loss: -35.2747 | G_loss: 10.8891\n",
      "Epoch [485/500] | D_loss: -21.6397 | G_loss: 6.2964\n",
      "Epoch [486/500] | D_loss: -37.2599 | G_loss: 8.2127\n",
      "Epoch [487/500] | D_loss: -22.0499 | G_loss: 4.4970\n",
      "Epoch [488/500] | D_loss: -15.1457 | G_loss: 13.0857\n",
      "Epoch [489/500] | D_loss: -31.9240 | G_loss: 7.4339\n",
      "Epoch [490/500] | D_loss: -12.7729 | G_loss: 7.4801\n",
      "Epoch [491/500] | D_loss: -23.7101 | G_loss: 7.5572\n",
      "Epoch [492/500] | D_loss: -29.7264 | G_loss: 10.5079\n",
      "Epoch [493/500] | D_loss: -23.7246 | G_loss: 8.6377\n",
      "Epoch [494/500] | D_loss: -26.9619 | G_loss: 18.5307\n",
      "Epoch [495/500] | D_loss: -25.7164 | G_loss: 7.7568\n",
      "Epoch [496/500] | D_loss: -33.4773 | G_loss: 8.8162\n",
      "Epoch [497/500] | D_loss: -22.3295 | G_loss: 0.9028\n",
      "Epoch [498/500] | D_loss: -32.5689 | G_loss: 11.9520\n",
      "Epoch [499/500] | D_loss: -15.6946 | G_loss: 18.0934\n",
      "Epoch [500/500] | D_loss: -26.5489 | G_loss: 13.1064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad as autograd_grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "latent_dim = 64         # Dimension of the noise vector.\n",
    "embed_dim = 2          # Dimension for label embeddings.\n",
    "# input_dim should be set to the number of features (n) in your vector data.\n",
    "input_dim = 21 \n",
    "print(input_dim)          # <-- Replace with your actual dimension n.\n",
    "batch_size = 64\n",
    "n_epochs = 500\n",
    "lr = 0.0002\n",
    "lambda_gp = 10          # Weight for gradient penalty\n",
    "n_critic = 5           # Number of discriminator updates per generator update\n",
    "\n",
    "# -------------------------------\n",
    "# Generator\n",
    "# -------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, embed_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Embedding for the binary labels (0 or 1)\n",
    "        self.label_emb = nn.Embedding(2, embed_dim)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + embed_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, output_dim)\n",
    "            # Optionally, you could add an activation (e.g., Tanh) here depending on your data.\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "        # Get label embeddings and concatenate with noise\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        gen_input = torch.cat((noise, label_embedding), dim=1)\n",
    "        output = self.model(gen_input)\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# Discriminator (Critic)\n",
    "# -------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Embedding for the binary labels (0 or 1)\n",
    "        self.label_emb = nn.Embedding(2, embed_dim)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + embed_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        d_input = torch.cat((x, label_embedding), dim=1)\n",
    "        validity = self.model(d_input)\n",
    "        return validity\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize models and optimizers\n",
    "# -------------------------------\n",
    "generator = Generator(latent_dim, embed_dim, input_dim).to(device)\n",
    "discriminator = Discriminator(input_dim, embed_dim).to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare your dataset\n",
    "# -------------------------------\n",
    "# Assume D_train and y_train are available as NumPy arrays or Torch tensors.\n",
    "# For example, if they are NumPy arrays, convert them to torch tensors:\n",
    "# (Replace these with your actual data.)\n",
    "# Create tensors\n",
    "D_train_tensor = torch.tensor(D_train_np, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(D_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Gradient Penalty Function\n",
    "# -------------------------------\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, labels):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # For vector data, alpha is of shape (batch_size, 1) and we expand it to match the input shape.\n",
    "    alpha = torch.rand(real_samples.size(0), 1, device=device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Pass the interpolated samples through the discriminator.\n",
    "    d_interpolates = D(interpolates, labels)\n",
    "    \n",
    "    # For computing gradients, we create a tensor of ones of the same shape as d_interpolates.\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device, requires_grad=False)\n",
    "    \n",
    "    gradients = autograd_grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_samples, real_labels) in enumerate(dataloader):\n",
    "        current_batch = real_samples.size(0)\n",
    "        real_samples = real_samples.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Sample noise and random labels for the generator.\n",
    "        z = torch.randn(current_batch, latent_dim, device=device)\n",
    "        # For the fake samples, sample labels uniformly from {0, 1}\n",
    "        fake_labels = torch.randint(0, 2, (current_batch,), device=device)\n",
    "        \n",
    "        # Generate fake samples.\n",
    "        fake_samples = generator(z, fake_labels)\n",
    "        \n",
    "        # Get discriminator outputs for real and fake samples.\n",
    "        real_validity = discriminator(real_samples, real_labels)\n",
    "        fake_validity = discriminator(fake_samples.detach(), fake_labels)\n",
    "        \n",
    "        # Compute the gradient penalty.\n",
    "        gp = compute_gradient_penalty(discriminator, real_samples.data, fake_samples.data, real_labels)\n",
    "        \n",
    "        # Wasserstein loss for discriminator.\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gp\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(current_batch, latent_dim, device=device)\n",
    "            fake_labels = torch.randint(0, 2, (current_batch,), device=device)\n",
    "            fake_samples = generator(z, fake_labels)\n",
    "            # Generator aims to have the discriminator give high scores to fake samples.\n",
    "            g_loss = -torch.mean(discriminator(fake_samples, fake_labels))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] | D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0086], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(1, latent_dim, device=device)\n",
    "# For the fake samples, sample labels uniformly from {0, 1}\n",
    "fake_labels = torch.zeros((1,), dtype=torch.long, device=device)\n",
    "\n",
    "new_detector(generator(z,fake_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([732, 21]) (1000,)\n",
      "Epoch [1/300] | D_loss: 7.0457 | G_loss: -0.1355\n",
      "Epoch [2/300] | D_loss: 6.0095 | G_loss: -0.3835\n",
      "Epoch [3/300] | D_loss: 5.1647 | G_loss: -0.8326\n",
      "Epoch [4/300] | D_loss: 4.8564 | G_loss: -1.5105\n",
      "Epoch [5/300] | D_loss: 4.8202 | G_loss: -2.6929\n",
      "Epoch [6/300] | D_loss: 5.4686 | G_loss: -3.8096\n",
      "Epoch [7/300] | D_loss: 5.9882 | G_loss: -5.0349\n",
      "Epoch [8/300] | D_loss: 6.6818 | G_loss: -5.4053\n",
      "Epoch [9/300] | D_loss: 7.0270 | G_loss: -5.2908\n",
      "Epoch [10/300] | D_loss: 7.0639 | G_loss: -5.2165\n",
      "Epoch [11/300] | D_loss: 6.7514 | G_loss: -4.7888\n",
      "Epoch [12/300] | D_loss: 6.1702 | G_loss: -4.0605\n",
      "Epoch [13/300] | D_loss: 4.9124 | G_loss: -3.2747\n",
      "Epoch [14/300] | D_loss: 4.1346 | G_loss: -2.6587\n",
      "Epoch [15/300] | D_loss: 2.7891 | G_loss: -1.9588\n",
      "Epoch [16/300] | D_loss: 1.6875 | G_loss: -1.2043\n",
      "Epoch [17/300] | D_loss: 0.3551 | G_loss: -0.1782\n",
      "Epoch [18/300] | D_loss: -1.0537 | G_loss: 0.8337\n",
      "Epoch [19/300] | D_loss: -1.7217 | G_loss: 1.5982\n",
      "Epoch [20/300] | D_loss: -2.5698 | G_loss: 2.1748\n",
      "Epoch [21/300] | D_loss: -3.2010 | G_loss: 2.6571\n",
      "Epoch [22/300] | D_loss: -3.5783 | G_loss: 3.1311\n",
      "Epoch [23/300] | D_loss: -4.3949 | G_loss: 3.4415\n",
      "Epoch [24/300] | D_loss: -4.4641 | G_loss: 3.8056\n",
      "Epoch [25/300] | D_loss: -4.4456 | G_loss: 3.8179\n",
      "Epoch [26/300] | D_loss: -4.0684 | G_loss: 3.6840\n",
      "Epoch [27/300] | D_loss: -3.7419 | G_loss: 3.2184\n",
      "Epoch [28/300] | D_loss: -3.0641 | G_loss: 2.8421\n",
      "Epoch [29/300] | D_loss: -2.8645 | G_loss: 2.2722\n",
      "Epoch [30/300] | D_loss: -2.7783 | G_loss: 1.8063\n",
      "Epoch [31/300] | D_loss: -2.0823 | G_loss: 1.4668\n",
      "Epoch [32/300] | D_loss: -2.5252 | G_loss: 1.1439\n",
      "Epoch [33/300] | D_loss: -1.6677 | G_loss: 0.8242\n",
      "Epoch [34/300] | D_loss: -1.2008 | G_loss: 0.3672\n",
      "Epoch [35/300] | D_loss: -1.1585 | G_loss: -0.0375\n",
      "Epoch [36/300] | D_loss: -0.3009 | G_loss: -0.5238\n",
      "Epoch [37/300] | D_loss: -0.3596 | G_loss: -0.9624\n",
      "Epoch [38/300] | D_loss: -0.1611 | G_loss: -1.3915\n",
      "Epoch [39/300] | D_loss: 0.1419 | G_loss: -1.6463\n",
      "Epoch [40/300] | D_loss: 0.3501 | G_loss: -1.8110\n",
      "Epoch [41/300] | D_loss: 0.5297 | G_loss: -1.9628\n",
      "Epoch [42/300] | D_loss: 0.4979 | G_loss: -2.0673\n",
      "Epoch [43/300] | D_loss: 0.9233 | G_loss: -2.0617\n",
      "Epoch [44/300] | D_loss: 0.3915 | G_loss: -1.9203\n",
      "Epoch [45/300] | D_loss: 0.4371 | G_loss: -2.0416\n",
      "Epoch [46/300] | D_loss: 0.2117 | G_loss: -1.9079\n",
      "Epoch [47/300] | D_loss: 0.2952 | G_loss: -1.7974\n",
      "Epoch [48/300] | D_loss: 0.3581 | G_loss: -1.6225\n",
      "Epoch [49/300] | D_loss: 0.0463 | G_loss: -1.5606\n",
      "Epoch [50/300] | D_loss: 0.4558 | G_loss: -1.6904\n",
      "Epoch [51/300] | D_loss: 0.0688 | G_loss: -1.6048\n",
      "Epoch [52/300] | D_loss: -0.1561 | G_loss: -1.8566\n",
      "Epoch [53/300] | D_loss: 0.1629 | G_loss: -1.7819\n",
      "Epoch [54/300] | D_loss: 0.3715 | G_loss: -1.7163\n",
      "Epoch [55/300] | D_loss: 0.2063 | G_loss: -1.5642\n",
      "Epoch [56/300] | D_loss: -0.5891 | G_loss: -1.1259\n",
      "Epoch [57/300] | D_loss: -0.5526 | G_loss: -0.8638\n",
      "Epoch [58/300] | D_loss: -0.3043 | G_loss: -0.4672\n",
      "Epoch [59/300] | D_loss: -1.5231 | G_loss: -0.2005\n",
      "Epoch [60/300] | D_loss: -1.0604 | G_loss: 0.1996\n",
      "Epoch [61/300] | D_loss: -2.0837 | G_loss: 0.5890\n",
      "Epoch [62/300] | D_loss: -2.1883 | G_loss: 0.7452\n",
      "Epoch [63/300] | D_loss: -2.2952 | G_loss: 0.8850\n",
      "Epoch [64/300] | D_loss: -2.5901 | G_loss: 1.0742\n",
      "Epoch [65/300] | D_loss: -2.5401 | G_loss: 1.2715\n",
      "Epoch [66/300] | D_loss: -2.2775 | G_loss: 1.1523\n",
      "Epoch [67/300] | D_loss: -1.8852 | G_loss: 0.7528\n",
      "Epoch [68/300] | D_loss: -1.9582 | G_loss: 0.5233\n",
      "Epoch [69/300] | D_loss: -1.9161 | G_loss: 0.2730\n",
      "Epoch [70/300] | D_loss: -1.2795 | G_loss: -0.1171\n",
      "Epoch [71/300] | D_loss: -1.3357 | G_loss: -0.2837\n",
      "Epoch [72/300] | D_loss: -1.6223 | G_loss: -0.3628\n",
      "Epoch [73/300] | D_loss: -1.4028 | G_loss: -0.3945\n",
      "Epoch [74/300] | D_loss: -1.6327 | G_loss: -0.4600\n",
      "Epoch [75/300] | D_loss: -0.9637 | G_loss: -0.6900\n",
      "Epoch [76/300] | D_loss: -1.1241 | G_loss: -1.0256\n",
      "Epoch [77/300] | D_loss: -0.7574 | G_loss: -1.4054\n",
      "Epoch [78/300] | D_loss: -0.0970 | G_loss: -1.8749\n",
      "Epoch [79/300] | D_loss: 0.5083 | G_loss: -2.1898\n",
      "Epoch [80/300] | D_loss: 0.7178 | G_loss: -2.9355\n",
      "Epoch [81/300] | D_loss: 1.2758 | G_loss: -3.6471\n",
      "Epoch [82/300] | D_loss: 2.0601 | G_loss: -4.1079\n",
      "Epoch [83/300] | D_loss: 2.1342 | G_loss: -4.8881\n",
      "Epoch [84/300] | D_loss: 3.5922 | G_loss: -5.0638\n",
      "Epoch [85/300] | D_loss: 3.0754 | G_loss: -5.1604\n",
      "Epoch [86/300] | D_loss: 3.0244 | G_loss: -5.1102\n",
      "Epoch [87/300] | D_loss: 3.0369 | G_loss: -5.0185\n",
      "Epoch [88/300] | D_loss: 3.1028 | G_loss: -4.5017\n",
      "Epoch [89/300] | D_loss: 1.6152 | G_loss: -3.7971\n",
      "Epoch [90/300] | D_loss: 0.1250 | G_loss: -2.8263\n",
      "Epoch [91/300] | D_loss: -0.6556 | G_loss: -1.7539\n",
      "Epoch [92/300] | D_loss: -0.5260 | G_loss: -0.9566\n",
      "Epoch [93/300] | D_loss: -1.2855 | G_loss: -0.5709\n",
      "Epoch [94/300] | D_loss: -0.7901 | G_loss: -0.0138\n",
      "Epoch [95/300] | D_loss: -2.1458 | G_loss: 0.2045\n",
      "Epoch [96/300] | D_loss: -2.3917 | G_loss: 0.4266\n",
      "Epoch [97/300] | D_loss: -2.4082 | G_loss: 0.8229\n",
      "Epoch [98/300] | D_loss: -1.4651 | G_loss: 0.9638\n",
      "Epoch [99/300] | D_loss: -1.0860 | G_loss: 0.7264\n",
      "Epoch [100/300] | D_loss: -0.4267 | G_loss: 0.3608\n",
      "Epoch [101/300] | D_loss: -0.4983 | G_loss: 0.3353\n",
      "Epoch [102/300] | D_loss: -1.2524 | G_loss: 0.6322\n",
      "Epoch [103/300] | D_loss: -1.3982 | G_loss: 0.8546\n",
      "Epoch [104/300] | D_loss: -1.7496 | G_loss: 1.2100\n",
      "Epoch [105/300] | D_loss: -2.0290 | G_loss: 1.3226\n",
      "Epoch [106/300] | D_loss: -2.3181 | G_loss: 1.6562\n",
      "Epoch [107/300] | D_loss: -3.2494 | G_loss: 2.0855\n",
      "Epoch [108/300] | D_loss: -3.0685 | G_loss: 2.2531\n",
      "Epoch [109/300] | D_loss: -3.5439 | G_loss: 2.5963\n",
      "Epoch [110/300] | D_loss: -2.6934 | G_loss: 2.1248\n",
      "Epoch [111/300] | D_loss: -1.9707 | G_loss: 1.4171\n",
      "Epoch [112/300] | D_loss: -1.7273 | G_loss: 1.0544\n",
      "Epoch [113/300] | D_loss: -1.6257 | G_loss: 0.5850\n",
      "Epoch [114/300] | D_loss: -0.9769 | G_loss: 0.1416\n",
      "Epoch [115/300] | D_loss: -0.5880 | G_loss: -0.3607\n",
      "Epoch [116/300] | D_loss: 0.0676 | G_loss: -0.8768\n",
      "Epoch [117/300] | D_loss: 0.3393 | G_loss: -1.3270\n",
      "Epoch [118/300] | D_loss: 0.9730 | G_loss: -1.7672\n",
      "Epoch [119/300] | D_loss: 1.2208 | G_loss: -1.9221\n",
      "Epoch [120/300] | D_loss: 1.6418 | G_loss: -2.3596\n",
      "Epoch [121/300] | D_loss: 2.0124 | G_loss: -2.7054\n",
      "Epoch [122/300] | D_loss: 2.5659 | G_loss: -3.3447\n",
      "Epoch [123/300] | D_loss: 3.5195 | G_loss: -3.6602\n",
      "Epoch [124/300] | D_loss: 3.4249 | G_loss: -3.9550\n",
      "Epoch [125/300] | D_loss: 3.9923 | G_loss: -4.4653\n",
      "Epoch [126/300] | D_loss: 4.6623 | G_loss: -4.3796\n",
      "Epoch [127/300] | D_loss: 4.1514 | G_loss: -4.4601\n",
      "Epoch [128/300] | D_loss: 3.8207 | G_loss: -4.5273\n",
      "Epoch [129/300] | D_loss: 3.8804 | G_loss: -4.4372\n",
      "Epoch [130/300] | D_loss: 4.1545 | G_loss: -4.0485\n",
      "Epoch [131/300] | D_loss: 3.6994 | G_loss: -3.6765\n",
      "Epoch [132/300] | D_loss: 3.3804 | G_loss: -3.2779\n",
      "Epoch [133/300] | D_loss: 2.8792 | G_loss: -2.9917\n",
      "Epoch [134/300] | D_loss: 2.5231 | G_loss: -3.2359\n",
      "Epoch [135/300] | D_loss: 2.4603 | G_loss: -2.9858\n",
      "Epoch [136/300] | D_loss: 2.7550 | G_loss: -3.5061\n",
      "Epoch [137/300] | D_loss: 2.9521 | G_loss: -3.9217\n",
      "Epoch [138/300] | D_loss: 3.9850 | G_loss: -4.4689\n",
      "Epoch [139/300] | D_loss: 4.8773 | G_loss: -4.9469\n",
      "Epoch [140/300] | D_loss: 4.6772 | G_loss: -5.3750\n",
      "Epoch [141/300] | D_loss: 5.4535 | G_loss: -4.8214\n",
      "Epoch [142/300] | D_loss: 4.1351 | G_loss: -4.4136\n",
      "Epoch [143/300] | D_loss: 3.5922 | G_loss: -3.3595\n",
      "Epoch [144/300] | D_loss: 2.7696 | G_loss: -2.6751\n",
      "Epoch [145/300] | D_loss: 1.7403 | G_loss: -1.8728\n",
      "Epoch [146/300] | D_loss: 0.5865 | G_loss: -0.4549\n",
      "Epoch [147/300] | D_loss: -0.8134 | G_loss: 1.1296\n",
      "Epoch [148/300] | D_loss: -2.9932 | G_loss: 2.7344\n",
      "Epoch [149/300] | D_loss: -4.0477 | G_loss: 4.2249\n",
      "Epoch [150/300] | D_loss: -5.2663 | G_loss: 5.3918\n",
      "Epoch [151/300] | D_loss: -5.6827 | G_loss: 5.7009\n",
      "Epoch [152/300] | D_loss: -5.7752 | G_loss: 6.2443\n",
      "Epoch [153/300] | D_loss: -5.3241 | G_loss: 5.4644\n",
      "Epoch [154/300] | D_loss: -5.2427 | G_loss: 5.5517\n",
      "Epoch [155/300] | D_loss: -4.6551 | G_loss: 5.5243\n",
      "Epoch [156/300] | D_loss: -3.3833 | G_loss: 4.0021\n",
      "Epoch [157/300] | D_loss: -2.6280 | G_loss: 3.3694\n",
      "Epoch [158/300] | D_loss: -2.3939 | G_loss: 2.7333\n",
      "Epoch [159/300] | D_loss: -1.4277 | G_loss: 2.0283\n",
      "Epoch [160/300] | D_loss: -0.4642 | G_loss: 1.5329\n",
      "Epoch [161/300] | D_loss: 0.0359 | G_loss: 0.8026\n",
      "Epoch [162/300] | D_loss: 1.0084 | G_loss: 0.1298\n",
      "Epoch [163/300] | D_loss: 1.5583 | G_loss: -0.5649\n",
      "Epoch [164/300] | D_loss: 2.6377 | G_loss: -1.3936\n",
      "Epoch [165/300] | D_loss: 3.6900 | G_loss: -2.0894\n",
      "Epoch [166/300] | D_loss: 4.2847 | G_loss: -2.6039\n",
      "Epoch [167/300] | D_loss: 5.1867 | G_loss: -2.9552\n",
      "Epoch [168/300] | D_loss: 5.4623 | G_loss: -2.8586\n",
      "Epoch [169/300] | D_loss: 5.2781 | G_loss: -2.4680\n",
      "Epoch [170/300] | D_loss: 5.1587 | G_loss: -2.2584\n",
      "Epoch [171/300] | D_loss: 4.9134 | G_loss: -1.8571\n",
      "Epoch [172/300] | D_loss: 4.1909 | G_loss: -1.5789\n",
      "Epoch [173/300] | D_loss: 4.0187 | G_loss: -1.3377\n",
      "Epoch [174/300] | D_loss: 4.0663 | G_loss: -1.0185\n",
      "Epoch [175/300] | D_loss: 3.7868 | G_loss: -1.1944\n",
      "Epoch [176/300] | D_loss: 3.9302 | G_loss: -1.5667\n",
      "Epoch [177/300] | D_loss: 4.0179 | G_loss: -2.0849\n",
      "Epoch [178/300] | D_loss: 4.9434 | G_loss: -2.8092\n",
      "Epoch [179/300] | D_loss: 5.2575 | G_loss: -3.3040\n",
      "Epoch [180/300] | D_loss: 6.4336 | G_loss: -4.1811\n",
      "Epoch [181/300] | D_loss: 7.3187 | G_loss: -4.9329\n",
      "Epoch [182/300] | D_loss: 8.0103 | G_loss: -5.2593\n",
      "Epoch [183/300] | D_loss: 9.2156 | G_loss: -5.3049\n",
      "Epoch [184/300] | D_loss: 9.0295 | G_loss: -5.6638\n",
      "Epoch [185/300] | D_loss: 8.5526 | G_loss: -4.3527\n",
      "Epoch [186/300] | D_loss: 7.4007 | G_loss: -2.9279\n",
      "Epoch [187/300] | D_loss: 5.6881 | G_loss: -1.1691\n",
      "Epoch [188/300] | D_loss: 3.6198 | G_loss: 0.4490\n",
      "Epoch [189/300] | D_loss: 1.0004 | G_loss: 2.3745\n",
      "Epoch [190/300] | D_loss: -0.4728 | G_loss: 4.5247\n",
      "Epoch [191/300] | D_loss: -1.8836 | G_loss: 4.7448\n",
      "Epoch [192/300] | D_loss: -2.5009 | G_loss: 5.2440\n",
      "Epoch [193/300] | D_loss: -2.7206 | G_loss: 4.9410\n",
      "Epoch [194/300] | D_loss: -1.9821 | G_loss: 3.9561\n",
      "Epoch [195/300] | D_loss: -0.6947 | G_loss: 2.9377\n",
      "Epoch [196/300] | D_loss: 0.2396 | G_loss: 2.3080\n",
      "Epoch [197/300] | D_loss: 1.3060 | G_loss: 1.4697\n",
      "Epoch [198/300] | D_loss: 1.8801 | G_loss: 0.3626\n",
      "Epoch [199/300] | D_loss: 3.1209 | G_loss: -0.4911\n",
      "Epoch [200/300] | D_loss: 4.0147 | G_loss: -1.1845\n",
      "Epoch [201/300] | D_loss: 3.9486 | G_loss: -1.7810\n",
      "Epoch [202/300] | D_loss: 4.5162 | G_loss: -2.6311\n",
      "Epoch [203/300] | D_loss: 5.9343 | G_loss: -3.6803\n",
      "Epoch [204/300] | D_loss: 6.3779 | G_loss: -4.5431\n",
      "Epoch [205/300] | D_loss: 7.9631 | G_loss: -4.9731\n",
      "Epoch [206/300] | D_loss: 8.1413 | G_loss: -5.2472\n",
      "Epoch [207/300] | D_loss: 8.7055 | G_loss: -5.0022\n",
      "Epoch [208/300] | D_loss: 8.4650 | G_loss: -4.7455\n",
      "Epoch [209/300] | D_loss: 9.3631 | G_loss: -5.1017\n",
      "Epoch [210/300] | D_loss: 8.7171 | G_loss: -5.1682\n",
      "Epoch [211/300] | D_loss: 8.5434 | G_loss: -4.8830\n",
      "Epoch [212/300] | D_loss: 8.1355 | G_loss: -3.4284\n",
      "Epoch [213/300] | D_loss: 6.6843 | G_loss: -2.4404\n",
      "Epoch [214/300] | D_loss: 6.1145 | G_loss: -0.7186\n",
      "Epoch [215/300] | D_loss: 3.6943 | G_loss: 1.6582\n",
      "Epoch [216/300] | D_loss: 1.4141 | G_loss: 3.2348\n",
      "Epoch [217/300] | D_loss: -0.7533 | G_loss: 4.7515\n",
      "Epoch [218/300] | D_loss: -1.2985 | G_loss: 5.4008\n",
      "Epoch [219/300] | D_loss: -2.3482 | G_loss: 5.1393\n",
      "Epoch [220/300] | D_loss: -1.5972 | G_loss: 4.9793\n",
      "Epoch [221/300] | D_loss: -1.3144 | G_loss: 3.3902\n",
      "Epoch [222/300] | D_loss: -0.0708 | G_loss: 1.9987\n",
      "Epoch [223/300] | D_loss: 1.1964 | G_loss: 0.4995\n",
      "Epoch [224/300] | D_loss: 2.7686 | G_loss: -0.6601\n",
      "Epoch [225/300] | D_loss: 3.9782 | G_loss: -1.5486\n",
      "Epoch [226/300] | D_loss: 5.1064 | G_loss: -2.0759\n",
      "Epoch [227/300] | D_loss: 5.2202 | G_loss: -3.1484\n",
      "Epoch [228/300] | D_loss: 6.6247 | G_loss: -3.6649\n",
      "Epoch [229/300] | D_loss: 7.4241 | G_loss: -3.8251\n",
      "Epoch [230/300] | D_loss: 7.7276 | G_loss: -4.0245\n",
      "Epoch [231/300] | D_loss: 8.6938 | G_loss: -3.8647\n",
      "Epoch [232/300] | D_loss: 8.0250 | G_loss: -3.3843\n",
      "Epoch [233/300] | D_loss: 7.5400 | G_loss: -2.9063\n",
      "Epoch [234/300] | D_loss: 7.2397 | G_loss: -2.5402\n",
      "Epoch [235/300] | D_loss: 6.2738 | G_loss: -2.2149\n",
      "Epoch [236/300] | D_loss: 6.4784 | G_loss: -2.1729\n",
      "Epoch [237/300] | D_loss: 5.6826 | G_loss: -1.5130\n",
      "Epoch [238/300] | D_loss: 5.4312 | G_loss: -1.2868\n",
      "Epoch [239/300] | D_loss: 4.8648 | G_loss: -0.7226\n",
      "Epoch [240/300] | D_loss: 4.2832 | G_loss: -0.1097\n",
      "Epoch [241/300] | D_loss: 3.3630 | G_loss: 0.7804\n",
      "Epoch [242/300] | D_loss: 2.9551 | G_loss: 1.1485\n",
      "Epoch [243/300] | D_loss: 2.3090 | G_loss: 1.4108\n",
      "Epoch [244/300] | D_loss: 2.3541 | G_loss: 1.1101\n",
      "Epoch [245/300] | D_loss: 2.6702 | G_loss: 0.0383\n",
      "Epoch [246/300] | D_loss: 3.4294 | G_loss: -0.8613\n",
      "Epoch [247/300] | D_loss: 4.3203 | G_loss: -1.9398\n",
      "Epoch [248/300] | D_loss: 5.2441 | G_loss: -2.6425\n",
      "Epoch [249/300] | D_loss: 6.7307 | G_loss: -3.1243\n",
      "Epoch [250/300] | D_loss: 6.7148 | G_loss: -3.5557\n",
      "Epoch [251/300] | D_loss: 6.8675 | G_loss: -3.4502\n",
      "Epoch [252/300] | D_loss: 6.5187 | G_loss: -2.5260\n",
      "Epoch [253/300] | D_loss: 5.9675 | G_loss: -1.9721\n",
      "Epoch [254/300] | D_loss: 5.2878 | G_loss: -0.8034\n",
      "Epoch [255/300] | D_loss: 4.0584 | G_loss: 0.4873\n",
      "Epoch [256/300] | D_loss: 3.1291 | G_loss: 0.8298\n",
      "Epoch [257/300] | D_loss: 3.4231 | G_loss: 0.9191\n",
      "Epoch [258/300] | D_loss: 2.9844 | G_loss: 0.6643\n",
      "Epoch [259/300] | D_loss: 2.9020 | G_loss: 0.0061\n",
      "Epoch [260/300] | D_loss: 3.7661 | G_loss: -1.1172\n",
      "Epoch [261/300] | D_loss: 4.5089 | G_loss: -2.6583\n",
      "Epoch [262/300] | D_loss: 6.1335 | G_loss: -4.2001\n",
      "Epoch [263/300] | D_loss: 7.4718 | G_loss: -4.9901\n",
      "Epoch [264/300] | D_loss: 8.4343 | G_loss: -5.2564\n",
      "Epoch [265/300] | D_loss: 8.2394 | G_loss: -4.5225\n",
      "Epoch [266/300] | D_loss: 8.2755 | G_loss: -3.6430\n",
      "Epoch [267/300] | D_loss: 6.5205 | G_loss: -1.9730\n",
      "Epoch [268/300] | D_loss: 5.4121 | G_loss: -1.2353\n",
      "Epoch [269/300] | D_loss: 4.3212 | G_loss: -0.2607\n",
      "Epoch [270/300] | D_loss: 3.6153 | G_loss: 0.3937\n",
      "Epoch [271/300] | D_loss: 3.0838 | G_loss: 0.6680\n",
      "Epoch [272/300] | D_loss: 3.1702 | G_loss: 0.5169\n",
      "Epoch [273/300] | D_loss: 3.5747 | G_loss: 0.1029\n",
      "Epoch [274/300] | D_loss: 3.8428 | G_loss: -0.8773\n",
      "Epoch [275/300] | D_loss: 4.7210 | G_loss: -1.5778\n",
      "Epoch [276/300] | D_loss: 5.4735 | G_loss: -2.4032\n",
      "Epoch [277/300] | D_loss: 5.7228 | G_loss: -2.7167\n",
      "Epoch [278/300] | D_loss: 6.0414 | G_loss: -2.6747\n",
      "Epoch [279/300] | D_loss: 5.9265 | G_loss: -3.3568\n",
      "Epoch [280/300] | D_loss: 6.1765 | G_loss: -3.3067\n",
      "Epoch [281/300] | D_loss: 6.5299 | G_loss: -3.4268\n",
      "Epoch [282/300] | D_loss: 6.5118 | G_loss: -2.9476\n",
      "Epoch [283/300] | D_loss: 6.4058 | G_loss: -2.4519\n",
      "Epoch [284/300] | D_loss: 6.5156 | G_loss: -2.3186\n",
      "Epoch [285/300] | D_loss: 6.2189 | G_loss: -1.7966\n",
      "Epoch [286/300] | D_loss: 6.0052 | G_loss: -1.3375\n",
      "Epoch [287/300] | D_loss: 5.4793 | G_loss: -0.5869\n",
      "Epoch [288/300] | D_loss: 4.7923 | G_loss: -0.7461\n",
      "Epoch [289/300] | D_loss: 5.0460 | G_loss: -1.2240\n",
      "Epoch [290/300] | D_loss: 5.2654 | G_loss: -1.1534\n",
      "Epoch [291/300] | D_loss: 5.6071 | G_loss: -0.9386\n",
      "Epoch [292/300] | D_loss: 5.2024 | G_loss: -1.2391\n",
      "Epoch [293/300] | D_loss: 5.3836 | G_loss: -1.3004\n",
      "Epoch [294/300] | D_loss: 5.5265 | G_loss: -0.6221\n",
      "Epoch [295/300] | D_loss: 4.2157 | G_loss: 0.7759\n",
      "Epoch [296/300] | D_loss: 2.7801 | G_loss: 1.4684\n",
      "Epoch [297/300] | D_loss: 2.1597 | G_loss: 1.3993\n",
      "Epoch [298/300] | D_loss: 2.3145 | G_loss: 1.1444\n",
      "Epoch [299/300] | D_loss: 2.8505 | G_loss: 0.3046\n",
      "Epoch [300/300] | D_loss: 3.5104 | G_loss: -0.9068\n",
      "Generated Vectors for Label 1:\n",
      " [[ -5.70731     -4.933208    -3.5503807    1.4143747    8.63389\n",
      "   13.998836    -4.6029687   -6.8593597    9.998643   -11.197343\n",
      "    0.91147286   0.12009958 -18.687517     0.34161362   1.3037485\n",
      "    6.9942055   -4.0116363   -0.77623147  -1.3537834    5.8621564\n",
      "   -7.011124  ]\n",
      " [ -5.43004     -5.255047    -3.586456     1.237208     8.803254\n",
      "   15.227625    -5.0730133   -6.9427505   11.123229   -10.819752\n",
      "    1.9913      -0.69468546 -20.979357     0.4580837    1.5280651\n",
      "    7.5111513   -3.7429147   -0.6898422   -0.7995875    6.723021\n",
      "   -6.929508  ]\n",
      " [ -6.6010065   -4.735193    -4.2224       2.1629734    9.596931\n",
      "   16.301357    -5.596006    -7.260559    10.982658   -12.551259\n",
      "    0.7306205   -0.8373771  -20.859188     0.39411038   1.9464021\n",
      "    7.7018027   -4.7696686   -0.65142095  -1.7113658    6.4443073\n",
      "   -7.5680604 ]\n",
      " [ -5.1108727   -5.7055163   -3.2533295    1.5084251    8.729003\n",
      "   14.58064     -4.430653    -6.34802     11.130845   -10.906966\n",
      "    1.3993535    0.05165843 -19.691366     0.61482793   1.9466784\n",
      "    6.714463    -3.9728632   -0.29981917  -1.0260637    5.7667484\n",
      "   -6.249481  ]\n",
      " [ -5.8794365   -4.6557713   -3.4604475    1.3607144    8.512386\n",
      "   13.567936    -4.7979994   -6.0333705   10.5966015   -9.982465\n",
      "    1.4942722   -0.6793373  -18.367456     0.36544275   1.6161346\n",
      "    6.2340083   -4.086411    -0.8723715   -1.5010962    5.7707667\n",
      "   -6.173048  ]\n",
      " [ -5.4407043   -4.93212     -3.1156564    1.7586658    7.5813594\n",
      "   12.7552185   -4.071204    -5.6745343   10.100335    -9.829692\n",
      "    1.0347176    0.02186304 -17.355066     0.5171333    1.4616747\n",
      "    5.877854    -4.1785307   -0.389772    -1.2363225    5.4129014\n",
      "   -5.570178  ]\n",
      " [ -4.7561116   -3.9864483   -2.9522536    1.4746249    6.8070645\n",
      "   12.290882    -4.068025    -5.5158005    9.85043     -9.45669\n",
      "    1.3306091    0.3426557  -16.158613     0.6375474    1.9106042\n",
      "    6.1013894   -3.2362611   -0.6490335   -1.7393144    4.497756\n",
      "   -5.4759555 ]\n",
      " [ -5.4290023   -4.6965337   -3.7425008    1.8436546    8.291338\n",
      "   13.63414     -4.4678087   -6.293615     9.797083    -9.782683\n",
      "    1.2485833   -0.5650554  -17.447315     0.48293293   1.3904653\n",
      "    6.446796    -3.7635155   -0.8589703   -1.6095923    5.5170293\n",
      "   -6.0841985 ]\n",
      " [ -4.9441657   -4.205796    -2.8180685    1.1860445    7.25026\n",
      "   12.954722    -4.075931    -5.670285     9.712331    -9.347901\n",
      "    1.5342547   -0.47100803 -16.835642     0.33798873   1.5858103\n",
      "    6.0136538   -3.6210837   -0.5335237   -1.4188817    5.083891\n",
      "   -5.636824  ]\n",
      " [ -5.973008    -5.8853636   -3.3607213    1.1781518    8.517587\n",
      "   14.684753    -4.712878    -6.9901834   11.98558    -11.110379\n",
      "    2.2183957    0.399956   -20.195107     0.6496776    1.9145232\n",
      "    6.940764    -4.582864    -0.50782853  -0.7406359    6.625441\n",
      "   -6.136605  ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad as torch_grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "# Hyperparameters\n",
    "###############################################################################\n",
    "latent_dim = 64       # Dimension of the noise vector.\n",
    "n_classes = 2         # Number of unique labels (for binary classification, 2).\n",
    "feature_dim = 21      # Number of features (n in mxn dataset)\n",
    "batch_size = 64\n",
    "n_epochs = 300\n",
    "lr = 0.0001\n",
    "lambda_gp = 10        # Gradient penalty coefficient.\n",
    "n_critic = 1         # Number of discriminator updates per generator update.\n",
    "\n",
    "###############################################################################\n",
    "# Conditional Generator\n",
    "###############################################################################\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_classes, output_dim):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + n_classes, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, output_dim)  # Output is n-dimensional feature vector\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        label_embedding = self.label_emb(labels)  # Convert labels to embeddings\n",
    "        x = torch.cat((noise, label_embedding), dim=1)  # Concatenate noise + label\n",
    "        return self.model(x)\n",
    "\n",
    "###############################################################################\n",
    "# Conditional Discriminator (Critic)\n",
    "###############################################################################\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, n_classes, input_dim):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(n_classes, 1)  # Embed label into single value\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, 256),  # Input is feature_dim + label embedding\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)  # Output is a single \"realness\" score\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.label_emb(labels).view(x.size(0), 1)  # Embed and reshape\n",
    "        x = torch.cat((x, label_embedding), dim=1)  # Concatenate feature vector + label\n",
    "        return self.model(x)\n",
    "\n",
    "###############################################################################\n",
    "# Gradient Penalty Function\n",
    "###############################################################################\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, labels):\n",
    "    alpha_shape = [real_samples.size(0)] + [1] * (real_samples.dim() - 1)\n",
    "    alpha = torch.rand(alpha_shape, device=real_samples.device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates, labels)\n",
    "    fake_out = torch.ones(real_samples.size(0), 1, device=real_samples.device, requires_grad=False)\n",
    "    gradients = torch_grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake_out,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "###############################################################################\n",
    "# Initialize Models and Optimizers\n",
    "###############################################################################\n",
    "generator = ConditionalGenerator(latent_dim, n_classes, feature_dim).to(device)\n",
    "discriminator = ConditionalDiscriminator(n_classes, feature_dim).to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "###############################################################################\n",
    "# Create a Fake Vector Dataset (Replace this with real data)\n",
    "###############################################################################\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simulated dataset: 1000 samples, 21 features\n",
    "X_train = np.random.randn(1000, feature_dim).astype(np.float32)  # Feature vectors\n",
    "y_train = np.random.randint(0, n_classes, size=(1000,)).astype(np.int64)  # Labels (0 or 1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(D_train_np, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "print(D_train.shape,y_train.shape)\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "###############################################################################\n",
    "# Training Loop\n",
    "####################################    ###########################################\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_samples, real_labels) in enumerate(dataloader):\n",
    "        batch_size_cur = real_samples.size(0)\n",
    "        real_samples = real_samples.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Sample noise and random labels for generator\n",
    "        z = torch.randn(batch_size_cur, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, n_classes, (batch_size_cur,), device=device)\n",
    "        fake_samples = generator(z, gen_labels)\n",
    "\n",
    "        # Discriminator outputs for real and fake samples\n",
    "        real_validity = discriminator(real_samples, real_labels)\n",
    "        fake_validity = discriminator(fake_samples.detach(), gen_labels)\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gp = compute_gradient_penalty(discriminator, real_samples.data, fake_samples.data, real_labels)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gp\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator every n_critic steps.\n",
    "        # ---------------------\n",
    "        for i in range(2):\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size_cur, latent_dim, device=device)\n",
    "            gen_labels = torch.randint(0, n_classes, (batch_size_cur,), device=device)\n",
    "            fake_samples = generator(z, gen_labels)\n",
    "            g_loss = -torch.mean(discriminator(fake_samples, gen_labels))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] | D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "###############################################################################\n",
    "# Function to Generate Feature Vectors Based on a Given Condition\n",
    "###############################################################################\n",
    "def generate_vectors(condition, num_samples=10):\n",
    "    \"\"\"\n",
    "    Generate synthetic feature vectors using the trained generator for a given condition.\n",
    "    \n",
    "    Parameters:\n",
    "        condition (int or Tensor): The condition (e.g., label 0 or 1).\n",
    "        num_samples (int): Number of vectors to generate.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Generated feature vectors.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(condition, int):\n",
    "            labels = torch.full((num_samples,), condition, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            labels = condition.to(device)\n",
    "        z = torch.randn(num_samples, latent_dim, device=device)\n",
    "        generated_vectors = generator(z, labels)\n",
    "    generator.eval()\n",
    "    return generated_vectors.cpu().numpy()\n",
    "\n",
    "###############################################################################\n",
    "# Example Usage: Generate 10 Vectors for Label 1\n",
    "###############################################################################\n",
    "gen_vectors = generate_vectors(1, num_samples=10)\n",
    "print(\"Generated Vectors for Label 1:\\n\", gen_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = generate_vectors(1,1)\n",
    "x= to_tensor(x)\n",
    "print(loaded_detector_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
