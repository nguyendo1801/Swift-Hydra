{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:26:27.425556Z",
     "start_time": "2025-01-12T16:26:27.399326Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Load & Utility\n",
    "# =========================\n",
    "def load_adbench_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Giả sử file .npz có 'X' và 'y'\n",
    "    X: (N, d), y: (N,)\n",
    "    \"\"\"\n",
    "    data = np.load(dataset_path)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def evaluate_with_classification_report_and_auc(model, test_loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch).squeeze()  # (B,)\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_labels.append(y_batch.cpu())\n",
    "\n",
    "    preds = torch.cat(all_preds).numpy()  # (N_test,)\n",
    "    labels = torch.cat(all_labels).numpy()  # (N_test,)\n",
    "\n",
    "    # Chuyển sang nhãn nhị phân\n",
    "    binary_preds = (preds > threshold).astype(int)\n",
    "\n",
    "    report = classification_report(labels, binary_preds, target_names=['Class 0', 'Class 1'])\n",
    "    print(report)\n",
    "\n",
    "    if len(set(labels)) > 1:\n",
    "        aucroc = roc_auc_score(labels, preds)\n",
    "        print(f\"AUC-ROC: {aucroc:.4f}\")\n",
    "    else:\n",
    "        aucroc = None\n",
    "        print(\"AUC-ROC: Undefined (only one class present in labels)\")\n",
    "    return report, aucroc\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Beta-CVAE\n",
    "# =========================\n",
    "class BetaCVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Beta-CVAE cho bài toán nhị phân:\n",
    "      - Encoder nhận (x, y)\n",
    "      - Decoder nhận (z, y)\n",
    "      - Thêm hệ số beta > 1 cho KL-divergence để khuyến khích latent space 'trải rộng'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=64, beta=4.0):\n",
    "        super(BetaCVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta  # hệ số \"phóng đại\" KL\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim + 1, hidden_dim)  # concat y -> +1\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim + 1, hidden_dim)  # concat y -> +1\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=1)  # (B, input_dim+1)\n",
    "        h = F.relu(self.fc1(xy))\n",
    "        mean = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        zy = torch.cat([z, y], dim=1)  # (B, latent_dim+1)\n",
    "        h = F.relu(self.fc3(zy))\n",
    "        x_recon = self.fc4(h)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_recon = self.decode(z, y)\n",
    "        return x_recon, mean, logvar\n",
    "\n",
    "\n",
    "def beta_cvae_loss_fn(x, x_recon, mean, logvar, beta=4.0):\n",
    "    \"\"\"\n",
    "    Reconstruction (MSE) + beta * KL-div\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss\n",
    "\n",
    "\n",
    "def train_beta_cvae(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).unsqueeze(1)  # (B,1)\n",
    "\n",
    "        x_recon, mean, logvar = model(x_batch, y_batch)\n",
    "        loss = beta_cvae_loss_fn(x_batch, x_recon, mean, logvar, beta=model.beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Transformer Detector\n",
    "# =========================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L, :].to(x.device)\n",
    "\n",
    "\n",
    "class TransformerDetector(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerDetector, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                   nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "\n",
    "def train_detector(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch)  # (B,)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T16:26:31.459730Z",
     "start_time": "2025-01-12T16:26:28.468805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# =========================\n",
    "# 4. MAIN DEMO\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ví dụ đường dẫn\n",
    "    dataset_path = r\"D:\\Study\\Code\\SwiftHydra\\Classical\\12_fault.npz\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 4.1: Load Data\n",
    "    X_all, y_all = load_adbench_data(dataset_path)\n",
    "    input_dim = X_all.shape[1]\n",
    "\n",
    "    # Chia train/test\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X_all.numpy(), y_all.numpy(), test_size=0.2, random_state=42, stratify=y_all\n",
    "    )\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader cho Beta-CVAE\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # 4.2: Khởi tạo Beta-CVAE\n",
    "    # Tăng beta (vd=4.0) để nhấn mạnh KL -> latent space \"trải rộng\" hơn\n",
    "    beta_cvae = BetaCVAE(input_dim=input_dim, hidden_dim=128, latent_dim=32, beta=1.0).to(device)\n",
    "    optimizer_cvae = Adam(beta_cvae.parameters(), lr=1e-3)\n",
    "\n",
    "# Train Beta-CVAE\n",
    "num_epochs_cvae = 450\n",
    "for epoch in range(num_epochs_cvae):\n",
    "    loss_cvae = train_beta_cvae(beta_cvae, train_loader, optimizer_cvae, device)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"[Beta-CVAE] Epoch {epoch + 1}/{num_epochs_cvae}, loss={loss_cvae:.2f}\")\n",
    "\n",
    "# 4.3: Sinh dữ liệu - maximize diversity\n",
    "\"\"\"\n",
    "Thay vì z ~ Normal(0,1), ta thử z ~ Uniform([-2,2]) => \n",
    "tăng coverage, khuyến khích dữ liệu sinh ra đa dạng hơn\n",
    "\"\"\"\n",
    "beta_cvae.eval()\n",
    "\n",
    "minority_mask = (y_train == 1)\n",
    "X_minority = X_train[minority_mask]\n",
    "majority_mask = (y_train == 0)\n",
    "X_majority = X_train[majority_mask]\n",
    "\n",
    "num_generate = len(X_majority) - len(X_minority)  # Hoặc tuỳ ý # số lượng sample sinh ra\n",
    "with torch.no_grad():\n",
    "    # Tạo z uniform trong [-2,2]\n",
    "    z_uniform = (torch.rand(num_generate, beta_cvae.latent_dim) * 4.0) - 2.0\n",
    "    z_uniform = z_uniform.to(device)\n",
    "\n",
    "    # Gán y=1 => oversample minority\n",
    "    # hoặc y=0.7 => “lai”\n",
    "    # Ở đây ta ví dụ oversample minority\n",
    "    y_synthetic_c = torch.full((num_generate, 1), 0.9, device=device)\n",
    "\n",
    "    # Decode\n",
    "    X_synthetic = beta_cvae.decode(z_uniform, y_synthetic_c)\n",
    "    X_synthetic = X_synthetic.cpu()\n",
    "\n",
    "# Tạo label = 1 cho samples synthetic\n",
    "y_synthetic_labels = torch.ones(num_generate)\n",
    "\n",
    "# Ghép lại\n",
    "X_train_final = torch.cat([X_train, X_synthetic], dim=0)\n",
    "y_train_final = torch.cat([y_train, y_synthetic_labels], dim=0)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use('default')  # Đảm bảo sử dụng style mặc định\n",
    "\n",
    "# Giả sử bạn đã có:\n",
    "# - X_train, y_train: Dữ liệu train và nhãn\n",
    "# - X_test, y_test: Dữ liệu test và nhãn\n",
    "# - X_synthetic: Dữ liệu synthetic\n",
    "\n",
    "# 1) Gộp dữ liệu\n",
    "X_plot = torch.cat([X_train, X_test, X_synthetic], dim=0).numpy()\n",
    "\n",
    "# Tạo nhãn:\n",
    "# - Phần đầu: y_train (0 hoặc 1)\n",
    "# - Tiếp theo: y_test (0 hoặc 1)\n",
    "# - Cuối cùng: synthetic (2)\n",
    "N_train = len(X_train)\n",
    "N_test = len(X_test)\n",
    "N_synthetic = len(X_synthetic)\n",
    "y_plot = np.concatenate([\n",
    "    y_train.numpy(),  # Nhãn train\n",
    "    y_test.numpy(),  # Nhãn test\n",
    "    np.full((N_synthetic,), 2)  # Nhãn synthetic\n",
    "], axis=0)\n",
    "\n",
    "# 2) Chuẩn hoá dữ liệu (nếu cần)\n",
    "scaler = StandardScaler()\n",
    "X_plot_scaled = scaler.fit_transform(X_plot)\n",
    "\n",
    "# 3) Tính T-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X_plot_scaled)\n",
    "# X_embedded.shape = (N_train + N_test + N_synthetic, 2)\n",
    "\n",
    "# 4) Vẽ\n",
    "plt.figure(figsize=(10, 8), facecolor='white')  # Đặt nền trắng\n",
    "\n",
    "# Train Class 0 -> đỏ\n",
    "idx0_train = (y_plot[:N_train] == 0)\n",
    "plt.scatter(X_embedded[:N_train][idx0_train, 0], X_embedded[:N_train][idx0_train, 1],\n",
    "            c='darkred', alpha=0.6, label='Class 0 (Train)')\n",
    "\n",
    "# Train Class 1 -> xanh dương\n",
    "idx1_train = (y_plot[:N_train] == 1)\n",
    "plt.scatter(X_embedded[:N_train][idx1_train, 0], X_embedded[:N_train][idx1_train, 1],\n",
    "            c='darkblue', alpha=0.6, label='Class 1 (Train)')\n",
    "\n",
    "# Test Class 0 -> cam\n",
    "idx0_test = (y_plot[N_train:N_train + N_test] == 0)\n",
    "plt.scatter(X_embedded[N_train:N_train + N_test][idx0_test, 0],\n",
    "            X_embedded[N_train:N_train + N_test][idx0_test, 1],\n",
    "            c='orange', alpha=0.6, label='Class 0 (Test)')\n",
    "\n",
    "# Test Class 1 -> xanh nhạt\n",
    "idx1_test = (y_plot[N_train:N_train + N_test] == 1)\n",
    "plt.scatter(X_embedded[N_train:N_train + N_test][idx1_test, 0],\n",
    "            X_embedded[N_train:N_train + N_test][idx1_test, 1],\n",
    "            c='skyblue', alpha=0.6, label='Class 1 (Test)')\n",
    "\n",
    "# Synthetic Data -> xanh lá\n",
    "idx_syn = (y_plot[N_train + N_test:] == 2)\n",
    "plt.scatter(X_embedded[N_train + N_test:][idx_syn, 0],\n",
    "            X_embedded[N_train + N_test:][idx_syn, 1],\n",
    "            c='green', alpha=0.6, label='Synthetic')\n",
    "\n",
    "plt.title(\"T-SNE Visualization: Train, Test, and Synthetic Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "16fc06fa75b5ff0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Beta-CVAE] Epoch 2/450, loss=1313.93\n",
      "[Beta-CVAE] Epoch 4/450, loss=1108.44\n",
      "[Beta-CVAE] Epoch 6/450, loss=1014.20\n",
      "[Beta-CVAE] Epoch 8/450, loss=968.30\n",
      "[Beta-CVAE] Epoch 10/450, loss=936.73\n",
      "[Beta-CVAE] Epoch 12/450, loss=928.86\n",
      "[Beta-CVAE] Epoch 14/450, loss=916.00\n",
      "[Beta-CVAE] Epoch 16/450, loss=892.53\n",
      "[Beta-CVAE] Epoch 18/450, loss=866.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 34\u001B[0m\n\u001B[0;32m     32\u001B[0m num_epochs_cvae \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m450\u001B[39m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs_cvae):\n\u001B[1;32m---> 34\u001B[0m     loss_cvae \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_beta_cvae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta_cvae\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_cvae\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Beta-CVAE] Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs_cvae\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_cvae\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[51], line 127\u001B[0m, in \u001B[0;36mtrain_beta_cvae\u001B[1;34m(model, data_loader, optimizer, device)\u001B[0m\n\u001B[0;32m    125\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    126\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m--> 127\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_loader)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4.4: Train mô hình TransformerDetector\n",
    "train_dataset_final = TensorDataset(X_train_final, y_train_final)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "print(\"Sau khi oversampling bằng VAE:\")\n",
    "unique, counts = np.unique(y_train_final.numpy(), return_counts=True)\n",
    "print(\"Phân phối lớp trong tập train:\", dict(zip(unique, counts)))\n",
    "model = TransformerDetector(input_size=input_dim).to(device)\n",
    "optimizer_tf = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs_tf = 100\n",
    "for epoch in range(num_epochs_tf):\n",
    "    train_loss = train_detector(model, train_loader_final, optimizer_tf, criterion, device)\n",
    "    print(f\"[Transformer] Epoch {epoch + 1}/{num_epochs_tf}, Loss={train_loss:.4f}\")\n",
    "\n",
    "    # Đánh giá\n",
    "    print(\"Test set evaluation:\")\n",
    "    evaluate_with_classification_report_and_auc(model, test_loader, device, threshold=0.5)\n",
    "    print(\"-\" * 40)"
   ],
   "id": "9c3713d88a51547a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Đảm bảo thư mục lưu trữ tồn tại\n",
    "save_dir = \"./saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Lưu Beta-CVAE\n",
    "vae_path = os.path.join(save_dir, \"beta_cvae.pth\")\n",
    "torch.save(beta_cvae.state_dict(), vae_path)\n",
    "print(f\"Beta-CVAE model saved to: {vae_path}\")\n",
    "\n",
    "# Lưu TransformerDetector\n",
    "detector_path = os.path.join(save_dir, \"transformer_detector.pth\")\n",
    "torch.save(model.state_dict(), detector_path)\n",
    "print(f\"TransformerDetector model saved to: {detector_path}\")\n"
   ],
   "id": "cb6014e5878d9652",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T16:26:36.313911Z",
     "start_time": "2025-01-12T16:26:36.216671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Khởi tạo mô hình cùng cấu hình ban đầu\n",
    "loaded_beta_cvae = BetaCVAE(input_dim=input_dim, hidden_dim=128, latent_dim=32, beta=1.0).to(device)\n",
    "loaded_detector_model = TransformerDetector(input_size=input_dim).to(device)\n",
    "\n",
    "# Load trạng thái mô hình đã lưu\n",
    "loaded_beta_cvae.load_state_dict(torch.load(vae_path))\n",
    "loaded_detector_model.load_state_dict(torch.load(detector_path))\n",
    "\n",
    "# Đặt mô hình về chế độ eval (nếu chỉ sử dụng inference)\n",
    "loaded_beta_cvae.eval()\n",
    "loaded_detector_model.eval()\n",
    "\n",
    "print(\"Models loaded successfully.\")\n"
   ],
   "id": "5582e1d33978ab06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Legion 7\\AppData\\Local\\Temp\\ipykernel_29568\\1091737754.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_beta_cvae.load_state_dict(torch.load(vae_path))\n",
      "C:\\Users\\Legion 7\\AppData\\Local\\Temp\\ipykernel_29568\\1091737754.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_detector_model.load_state_dict(torch.load(detector_path))\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T16:26:37.494063Z",
     "start_time": "2025-01-12T16:26:37.485501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_episodes = 100\n",
    "\n",
    "num_gen_data = 20"
   ],
   "id": "2a56cd45d4210583",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:37:21.677454Z",
     "start_time": "2025-01-12T19:25:40.181865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# =========================\n",
    "# 1. Hàm tiện ích\n",
    "# =========================\n",
    "def log_to_file(file_path, message):\n",
    "    \"\"\"\n",
    "    Ghi message vào file log cụ thể.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(message + \"\\n\")\n",
    "\n",
    "# =========================\n",
    "# 2. Hàm train và đánh giá\n",
    "# =========================\n",
    "def train_beta_cvae(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).unsqueeze(1)\n",
    "\n",
    "        x_recon, mean, logvar = model(x_batch, y_batch)\n",
    "        recon_loss = nn.MSELoss(reduction=\"sum\")(x_recon, x_batch)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "        loss = recon_loss + model.beta * kl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_detector(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_detector(model, test_loader, device, threshold=0.5, log_file=None):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs > threshold).float()\n",
    "            all_labels.extend(y_batch.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    if log_file:\n",
    "        log_to_file(log_file, report)\n",
    "    print(report)\n",
    "\n",
    "# =========================\n",
    "# 3. Sinh adversarial sample\n",
    "# =========================\n",
    "def generate_one_adversarial_sample(\n",
    "    beta_cvae, \n",
    "    detector, \n",
    "    x_orig, \n",
    "    device,\n",
    "    previously_generated=None, \n",
    "    alpha=1.0, \n",
    "    lambda_div=0.1,\n",
    "    lr=0.001, \n",
    "    steps=50,\n",
    "    log_file=None\n",
    "):\n",
    "    beta_cvae.eval()\n",
    "    detector.eval()\n",
    "    if previously_generated is None:\n",
    "        previously_generated = []\n",
    "\n",
    "    x_orig = x_orig.to(device).unsqueeze(0)\n",
    "    y_class1 = torch.ones((1, 1), device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean, logvar = beta_cvae.encode(x_orig, y_class1)\n",
    "        z = beta_cvae.reparameterize(mean, logvar).detach().clone().requires_grad_(True)\n",
    "\n",
    "    optimizer_z = torch.optim.Adam([z], lr=lr)\n",
    "    for step in range(steps):\n",
    "        optimizer_z.zero_grad()\n",
    "        x_synthetic = beta_cvae.decode(z, y_class1)\n",
    "        prob_class1 = detector(x_synthetic)\n",
    "        if previously_generated :  # chỉ tính nếu không rỗng\n",
    "            # 1) Gộp lại thành tensor (N, d)\n",
    "            x_old_cat = torch.stack(previously_generated, dim=0).to(device)  # (N, d)\n",
    "        \n",
    "            # 2) Tính norm theo batch\n",
    "            # Cách 1: Sử dụng broadcast (x_synthetic - x_old_cat)\n",
    "            # dist shape (N,)\n",
    "            dist = torch.norm(x_synthetic - x_old_cat, p=2, dim=1)\n",
    "            \n",
    "            # 3) Tính sum của exp(-alpha * dist)\n",
    "            diversity_term = torch.exp(-alpha * dist).sum()\n",
    "        \n",
    "        else:\n",
    "            diversity_term = 0.0\n",
    "\n",
    "        loss = prob_class1.mean() + lambda_div * diversity_term\n",
    "        loss.backward()\n",
    "        optimizer_z.step()\n",
    "\n",
    "        if log_file:\n",
    "            log_to_file(log_file, f\"Step {step + 1}/{steps}:\")\n",
    "            log_to_file(log_file, f\"  Prob_class1: {prob_class1.item():.4f}\")\n",
    "            log_to_file(log_file, f\"  Diversity term: {diversity_term:.4f}\")\n",
    "            log_to_file(log_file, f\"  Total loss: {loss.item():.4f}\")\n",
    "        # print(f\"Step {step + 1}/{steps}:\")\n",
    "        # print(f\"  Prob_class1: {prob_class1.item():.4f}\")\n",
    "        # print(f\"  Diversity term: {diversity_term:.4f}\")\n",
    "        # print(f\"  Total loss: {loss.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_adv = beta_cvae.decode(z, y_class1).detach().cpu().squeeze(0)\n",
    "    # previously_generated.append(x_adv)\n",
    "    return x_adv\n",
    "\n",
    "# =========================\n",
    "# 4. MAIN DEMO\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # File log\n",
    "    log_dir = \"./logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    beta_cvae_log = os.path.join(log_dir, \"beta_cvae.log\")\n",
    "    detector_log = os.path.join(log_dir, \"detector.log\")\n",
    "    adversarial_log = os.path.join(log_dir, \"adversarial_samples.log\")\n",
    "\n",
    "    # Dữ liệu\n",
    "    dataset_path = r\"D:\\Study\\Code\\SwiftHydra\\Classical\\12_fault.npz\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_all, y_all = load_adbench_data(dataset_path)\n",
    "    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "        X_all.numpy(), y_all.numpy(), test_size=0.2, random_state=42, stratify=y_all\n",
    "    )\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "    \n",
    "        # Lấy mask cho class 1\n",
    "    class1_mask = (y_train == 1)  # Boolean tensor: True nếu y_train[i] == 1\n",
    "    \n",
    "    # Lọc các mẫu thuộc class 1 từ X_train\n",
    "    # Lọc các mẫu thuộc class 1 từ X_train\n",
    "    X_train_no_grow_tensor = X_train[class1_mask]  # Tensor chứa tất cả các mẫu class 1\n",
    "    \n",
    "    # Chuyển mỗi hàng thành một tensor và lưu vào list\n",
    "    X_train_no_grow = [row for row in X_train_no_grow_tensor]\n",
    "\n",
    "    \n",
    "    # print(X_train_no_grow)\n",
    "\n",
    "    # Khởi tạo mô hình\n",
    "    input_dim = X_train.shape[1]\n",
    "    beta_cvae = BetaCVAE(input_dim=input_dim, hidden_dim=128, latent_dim=32, beta=1.0).to(device)\n",
    "    new_detector = TransformerDetector(input_size=input_dim).to(device)\n",
    "    optimizer_cvae = Adam(beta_cvae.parameters(), lr=1e-4)\n",
    "    optimizer_detector = Adam(new_detector.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Train Beta-CVAE\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    num_epochs_cvae = 50\n",
    "    for epoch in range(num_epochs_cvae):\n",
    "        loss_cvae = train_beta_cvae(loaded_beta_cvae, train_loader, optimizer_cvae, device)\n",
    "        log_to_file(beta_cvae_log, f\"Epoch {epoch + 1}/{num_epochs_cvae}, Loss: {loss_cvae:.4f}\")\n",
    "\n",
    "    # Train Detector với Adversarial Samples\n",
    "    num_episodes = 100\n",
    "    num_gen_data = 10\n",
    "    batch_size = 64\n",
    "    previously_generated = []\n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"===== EPISODE {ep + 1}/{num_episodes} =====\")\n",
    "        # Train Detector\n",
    "        detector_epochs = 10\n",
    "        train_dataset_detector = TensorDataset(X_train, y_train)\n",
    "        train_loader_detector = DataLoader(train_dataset_detector, batch_size=batch_size, shuffle=True)\n",
    "        for det_epoch in range(detector_epochs):\n",
    "            detector_loss = train_detector(new_detector, train_loader_detector, optimizer_detector, criterion, device)\n",
    "            log_to_file(detector_log, f\"Epoch {det_epoch + 1}/{detector_epochs}, Loss: {detector_loss:.4f}\")\n",
    "        \n",
    "        # print(f\"===== Evaluate in Training set =====\")\n",
    "        # evaluate_detector(new_detector, DataLoader(TensorDataset(X_train, y_train)), device, log_file=detector_log)\n",
    "        \n",
    "        # Giả sử y_train có dạng 0/1\n",
    "        unique_labels, counts = torch.unique(y_train, return_counts=True)\n",
    "        for lbl, cnt in zip(unique_labels, counts):\n",
    "            print(f\"Class {lbl.item()}: {cnt.item()} samples\")\n",
    "        print(f\"===== Evaluate in Testing set =====\")\n",
    "        evaluate_detector(new_detector, DataLoader(TensorDataset(X_test, y_test)), device, log_file=detector_log)\n",
    "\n",
    "        # Generate Adversarial Samples\n",
    "        idx_class1 = (y_train == 1).nonzero(as_tuple=True)[0]\n",
    "        new_samples, new_labels = [], []\n",
    "        for _ in range(num_gen_data):\n",
    "            random_idx = random.choice(idx_class1)\n",
    "            x_orig = X_train[random_idx]\n",
    "            x_adv = generate_one_adversarial_sample(\n",
    "                beta_cvae=beta_cvae,\n",
    "                detector=loaded_detector_model,\n",
    "                x_orig=x_orig,\n",
    "                device=device,\n",
    "                previously_generated=X_train_no_grow,\n",
    "                alpha=1.0,\n",
    "                lambda_div=0.1,\n",
    "                lr=0.01,\n",
    "                steps=50,\n",
    "                log_file=adversarial_log,\n",
    "            )\n",
    "            new_samples.append(x_adv.unsqueeze(0))\n",
    "            new_labels.append(torch.tensor([1]))\n",
    "        new_samples = torch.cat(new_samples, dim=0)\n",
    "        new_labels = torch.cat(new_labels, dim=0)\n",
    "        X_train = torch.cat([X_train, new_samples], dim=0)\n",
    "        y_train = torch.cat([y_train, new_labels], dim=0)"
   ],
   "id": "b63c900a61b4209d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPISODE 1/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 538 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7705    0.9252    0.8408       254\n",
      "         1.0     0.7738    0.4815    0.5936       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7722    0.7033    0.7172       389\n",
      "weighted avg     0.7716    0.7712    0.7550       389\n",
      "\n",
      "===== EPISODE 2/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 548 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7835    0.8976    0.8367       254\n",
      "         1.0     0.7347    0.5333    0.6180       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7591    0.7155    0.7274       389\n",
      "weighted avg     0.7666    0.7712    0.7608       389\n",
      "\n",
      "===== EPISODE 3/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 558 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8168    0.8780    0.8463       254\n",
      "         1.0     0.7328    0.6296    0.6773       135\n",
      "\n",
      "    accuracy                         0.7918       389\n",
      "   macro avg     0.7748    0.7538    0.7618       389\n",
      "weighted avg     0.7877    0.7918    0.7876       389\n",
      "\n",
      "===== EPISODE 4/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 568 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8165    0.8583    0.8369       254\n",
      "         1.0     0.7049    0.6370    0.6693       135\n",
      "\n",
      "    accuracy                         0.7815       389\n",
      "   macro avg     0.7607    0.7477    0.7531       389\n",
      "weighted avg     0.7778    0.7815    0.7787       389\n",
      "\n",
      "===== EPISODE 5/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 578 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7842    0.9016    0.8388       254\n",
      "         1.0     0.7423    0.5333    0.6207       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7633    0.7175    0.7298       389\n",
      "weighted avg     0.7697    0.7738    0.7631       389\n",
      "\n",
      "===== EPISODE 6/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 588 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8073    0.8740    0.8393       254\n",
      "         1.0     0.7193    0.6074    0.6586       135\n",
      "\n",
      "    accuracy                         0.7815       389\n",
      "   macro avg     0.7633    0.7407    0.7490       389\n",
      "weighted avg     0.7767    0.7815    0.7766       389\n",
      "\n",
      "===== EPISODE 7/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 598 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8217    0.8346    0.8281       254\n",
      "         1.0     0.6794    0.6593    0.6692       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7505    0.7470    0.7486       389\n",
      "weighted avg     0.7723    0.7738    0.7730       389\n",
      "\n",
      "===== EPISODE 8/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 608 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8029    0.8819    0.8405       254\n",
      "         1.0     0.7273    0.5926    0.6531       135\n",
      "\n",
      "    accuracy                         0.7815       389\n",
      "   macro avg     0.7651    0.7372    0.7468       389\n",
      "weighted avg     0.7766    0.7815    0.7755       389\n",
      "\n",
      "===== EPISODE 9/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 618 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8134    0.8583    0.8352       254\n",
      "         1.0     0.7025    0.6296    0.6641       135\n",
      "\n",
      "    accuracy                         0.7789       389\n",
      "   macro avg     0.7580    0.7439    0.7497       389\n",
      "weighted avg     0.7749    0.7789    0.7758       389\n",
      "\n",
      "===== EPISODE 10/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 628 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8052    0.8465    0.8253       254\n",
      "         1.0     0.6803    0.6148    0.6459       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7428    0.7306    0.7356       389\n",
      "weighted avg     0.7619    0.7661    0.7631       389\n",
      "\n",
      "===== EPISODE 11/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 638 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8092    0.8346    0.8217       254\n",
      "         1.0     0.6693    0.6296    0.6489       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7392    0.7321    0.7353       389\n",
      "weighted avg     0.7606    0.7635    0.7617       389\n",
      "\n",
      "===== EPISODE 12/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 648 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8113    0.8465    0.8285       254\n",
      "         1.0     0.6855    0.6296    0.6564       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7484    0.7380    0.7424       389\n",
      "weighted avg     0.7676    0.7712    0.7688       389\n",
      "\n",
      "===== EPISODE 13/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 658 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8014    0.8740    0.8362       254\n",
      "         1.0     0.7143    0.5926    0.6478       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7579    0.7333    0.7420       389\n",
      "weighted avg     0.7712    0.7763    0.7708       389\n",
      "\n",
      "===== EPISODE 14/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 668 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8161    0.8386    0.8272       254\n",
      "         1.0     0.6797    0.6444    0.6616       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7479    0.7415    0.7444       389\n",
      "weighted avg     0.7688    0.7712    0.7697       389\n",
      "\n",
      "===== EPISODE 15/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 678 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8030    0.8504    0.8260       254\n",
      "         1.0     0.6833    0.6074    0.6431       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7432    0.7289    0.7346       389\n",
      "weighted avg     0.7615    0.7661    0.7625       389\n",
      "\n",
      "===== EPISODE 16/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 688 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8150    0.8150    0.8150       254\n",
      "         1.0     0.6519    0.6519    0.6519       135\n",
      "\n",
      "    accuracy                         0.7584       389\n",
      "   macro avg     0.7334    0.7334    0.7334       389\n",
      "weighted avg     0.7584    0.7584    0.7584       389\n",
      "\n",
      "===== EPISODE 17/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 698 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8095    0.8701    0.8387       254\n",
      "         1.0     0.7155    0.6148    0.6614       135\n",
      "\n",
      "    accuracy                         0.7815       389\n",
      "   macro avg     0.7625    0.7424    0.7500       389\n",
      "weighted avg     0.7769    0.7815    0.7772       389\n",
      "\n",
      "===== EPISODE 18/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 708 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8000    0.8346    0.8170       254\n",
      "         1.0     0.6613    0.6074    0.6332       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7306    0.7210    0.7251       389\n",
      "weighted avg     0.7519    0.7558    0.7532       389\n",
      "\n",
      "===== EPISODE 19/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 718 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8062    0.8189    0.8125       254\n",
      "         1.0     0.6489    0.6296    0.6391       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7275    0.7243    0.7258       389\n",
      "weighted avg     0.7516    0.7532    0.7523       389\n",
      "\n",
      "===== EPISODE 20/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 728 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7978    0.8386    0.8177       254\n",
      "         1.0     0.6639    0.6000    0.6304       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7308    0.7193    0.7240       389\n",
      "weighted avg     0.7513    0.7558    0.7527       389\n",
      "\n",
      "===== EPISODE 21/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 738 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8219    0.7992    0.8104       254\n",
      "         1.0     0.6408    0.6741    0.6570       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7314    0.7366    0.7337       389\n",
      "weighted avg     0.7590    0.7558    0.7572       389\n",
      "\n",
      "===== EPISODE 22/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 748 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8130    0.8386    0.8256       254\n",
      "         1.0     0.6772    0.6370    0.6565       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7451    0.7378    0.7410       389\n",
      "weighted avg     0.7658    0.7686    0.7669       389\n",
      "\n",
      "===== EPISODE 23/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 758 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8092    0.8346    0.8217       254\n",
      "         1.0     0.6693    0.6296    0.6489       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7392    0.7321    0.7353       389\n",
      "weighted avg     0.7606    0.7635    0.7617       389\n",
      "\n",
      "===== EPISODE 24/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 768 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8199    0.8425    0.8311       254\n",
      "         1.0     0.6875    0.6519    0.6692       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7537    0.7472    0.7501       389\n",
      "weighted avg     0.7740    0.7763    0.7749       389\n",
      "\n",
      "===== EPISODE 25/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 778 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7971    0.8661    0.8302       254\n",
      "         1.0     0.6991    0.5852    0.6371       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7481    0.7257    0.7336       389\n",
      "weighted avg     0.7631    0.7686    0.7632       389\n",
      "\n",
      "===== EPISODE 26/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 788 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8022    0.8622    0.8311       254\n",
      "         1.0     0.6983    0.6000    0.6454       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7502    0.7311    0.7383       389\n",
      "weighted avg     0.7661    0.7712    0.7667       389\n",
      "\n",
      "===== EPISODE 27/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 798 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8113    0.8465    0.8285       254\n",
      "         1.0     0.6855    0.6296    0.6564       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7484    0.7380    0.7424       389\n",
      "weighted avg     0.7676    0.7712    0.7688       389\n",
      "\n",
      "===== EPISODE 28/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 808 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7927    0.8583    0.8242       254\n",
      "         1.0     0.6842    0.5778    0.6265       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7385    0.7180    0.7254       389\n",
      "weighted avg     0.7551    0.7609    0.7556       389\n",
      "\n",
      "===== EPISODE 29/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 818 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8193    0.8031    0.8111       254\n",
      "         1.0     0.6429    0.6667    0.6545       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7311    0.7349    0.7328       389\n",
      "weighted avg     0.7581    0.7558    0.7568       389\n",
      "\n",
      "===== EPISODE 30/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 828 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7978    0.8386    0.8177       254\n",
      "         1.0     0.6639    0.6000    0.6304       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7308    0.7193    0.7240       389\n",
      "weighted avg     0.7513    0.7558    0.7527       389\n",
      "\n",
      "===== EPISODE 31/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 838 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7927    0.8583    0.8242       254\n",
      "         1.0     0.6842    0.5778    0.6265       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7385    0.7180    0.7254       389\n",
      "weighted avg     0.7551    0.7609    0.7556       389\n",
      "\n",
      "===== EPISODE 32/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 848 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7978    0.8386    0.8177       254\n",
      "         1.0     0.6639    0.6000    0.6304       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7308    0.7193    0.7240       389\n",
      "weighted avg     0.7513    0.7558    0.7527       389\n",
      "\n",
      "===== EPISODE 33/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 858 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7970    0.8504    0.8229       254\n",
      "         1.0     0.6780    0.5926    0.6324       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7375    0.7215    0.7276       389\n",
      "weighted avg     0.7557    0.7609    0.7568       389\n",
      "\n",
      "===== EPISODE 34/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 868 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8106    0.8425    0.8263       254\n",
      "         1.0     0.6800    0.6296    0.6538       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7453    0.7361    0.7401       389\n",
      "weighted avg     0.7653    0.7686    0.7664       389\n",
      "\n",
      "===== EPISODE 35/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 878 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8097    0.8543    0.8314       254\n",
      "         1.0     0.6942    0.6222    0.6562       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7520    0.7383    0.7438       389\n",
      "weighted avg     0.7696    0.7738    0.7706       389\n",
      "\n",
      "===== EPISODE 36/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 888 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8061    0.8346    0.8201       254\n",
      "         1.0     0.6667    0.6222    0.6437       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7364    0.7284    0.7319       389\n",
      "weighted avg     0.7577    0.7609    0.7589       389\n",
      "\n",
      "===== EPISODE 37/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 898 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8132    0.8228    0.8180       254\n",
      "         1.0     0.6591    0.6444    0.6517       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7362    0.7336    0.7348       389\n",
      "weighted avg     0.7597    0.7609    0.7603       389\n",
      "\n",
      "===== EPISODE 38/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 908 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8074    0.8583    0.8321       254\n",
      "         1.0     0.6975    0.6148    0.6535       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7524    0.7365    0.7428       389\n",
      "weighted avg     0.7693    0.7738    0.7701       389\n",
      "\n",
      "===== EPISODE 39/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 918 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8061    0.8346    0.8201       254\n",
      "         1.0     0.6667    0.6222    0.6437       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7364    0.7284    0.7319       389\n",
      "weighted avg     0.7577    0.7609    0.7589       389\n",
      "\n",
      "===== EPISODE 40/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 928 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8120    0.8504    0.8308       254\n",
      "         1.0     0.6911    0.6296    0.6589       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7515    0.7400    0.7448       389\n",
      "weighted avg     0.7700    0.7738    0.7711       389\n",
      "\n",
      "===== EPISODE 41/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 938 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8140    0.8268    0.8203       254\n",
      "         1.0     0.6641    0.6444    0.6541       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7390    0.7356    0.7372       389\n",
      "weighted avg     0.7620    0.7635    0.7626       389\n",
      "\n",
      "===== EPISODE 42/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 948 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8140    0.8268    0.8203       254\n",
      "         1.0     0.6641    0.6444    0.6541       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7390    0.7356    0.7372       389\n",
      "weighted avg     0.7620    0.7635    0.7626       389\n",
      "\n",
      "===== EPISODE 43/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 958 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8043    0.8740    0.8377       254\n",
      "         1.0     0.7168    0.6000    0.6532       135\n",
      "\n",
      "    accuracy                         0.7789       389\n",
      "   macro avg     0.7606    0.7370    0.7455       389\n",
      "weighted avg     0.7740    0.7789    0.7737       389\n",
      "\n",
      "===== EPISODE 44/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 968 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8030    0.8504    0.8260       254\n",
      "         1.0     0.6833    0.6074    0.6431       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7432    0.7289    0.7346       389\n",
      "weighted avg     0.7615    0.7661    0.7625       389\n",
      "\n",
      "===== EPISODE 45/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 978 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8175    0.8465    0.8317       254\n",
      "         1.0     0.6905    0.6444    0.6667       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7540    0.7455    0.7492       389\n",
      "weighted avg     0.7734    0.7763    0.7744       389\n",
      "\n",
      "===== EPISODE 46/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 988 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8099    0.8386    0.8240       254\n",
      "         1.0     0.6746    0.6296    0.6513       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7422    0.7341    0.7377       389\n",
      "weighted avg     0.7629    0.7661    0.7641       389\n",
      "\n",
      "===== EPISODE 47/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 998 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8123    0.8346    0.8233       254\n",
      "         1.0     0.6719    0.6370    0.6540       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7421    0.7358    0.7386       389\n",
      "weighted avg     0.7635    0.7661    0.7645       389\n",
      "\n",
      "===== EPISODE 48/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1008 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7993    0.8937    0.8439       254\n",
      "         1.0     0.7429    0.5778    0.6500       135\n",
      "\n",
      "    accuracy                         0.7841       389\n",
      "   macro avg     0.7711    0.7357    0.7469       389\n",
      "weighted avg     0.7797    0.7841    0.7766       389\n",
      "\n",
      "===== EPISODE 49/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1018 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8217    0.8346    0.8281       254\n",
      "         1.0     0.6794    0.6593    0.6692       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7505    0.7470    0.7486       389\n",
      "weighted avg     0.7723    0.7738    0.7730       389\n",
      "\n",
      "===== EPISODE 50/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1028 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8231    0.8425    0.8327       254\n",
      "         1.0     0.6899    0.6593    0.6742       135\n",
      "\n",
      "    accuracy                         0.7789       389\n",
      "   macro avg     0.7565    0.7509    0.7535       389\n",
      "weighted avg     0.7769    0.7789    0.7777       389\n",
      "\n",
      "===== EPISODE 51/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1038 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8036    0.8701    0.8355       254\n",
      "         1.0     0.7105    0.6000    0.6506       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7571    0.7350    0.7431       389\n",
      "weighted avg     0.7713    0.7763    0.7714       389\n",
      "\n",
      "===== EPISODE 52/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1048 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7879    0.8189    0.8031       254\n",
      "         1.0     0.6320    0.5852    0.6077       135\n",
      "\n",
      "    accuracy                         0.7378       389\n",
      "   macro avg     0.7099    0.7020    0.7054       389\n",
      "weighted avg     0.7338    0.7378    0.7353       389\n",
      "\n",
      "===== EPISODE 53/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1058 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8125    0.8701    0.8403       254\n",
      "         1.0     0.7179    0.6222    0.6667       135\n",
      "\n",
      "    accuracy                         0.7841       389\n",
      "   macro avg     0.7652    0.7462    0.7535       389\n",
      "weighted avg     0.7797    0.7841    0.7800       389\n",
      "\n",
      "===== EPISODE 54/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1068 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8097    0.8543    0.8314       254\n",
      "         1.0     0.6942    0.6222    0.6562       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7520    0.7383    0.7438       389\n",
      "weighted avg     0.7696    0.7738    0.7706       389\n",
      "\n",
      "===== EPISODE 55/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1078 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8090    0.8504    0.8292       254\n",
      "         1.0     0.6885    0.6222    0.6537       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7488    0.7363    0.7414       389\n",
      "weighted avg     0.7672    0.7712    0.7683       389\n",
      "\n",
      "===== EPISODE 56/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1088 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8214    0.8150    0.8182       254\n",
      "         1.0     0.6569    0.6667    0.6618       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7392    0.7408    0.7400       389\n",
      "weighted avg     0.7643    0.7635    0.7639       389\n",
      "\n",
      "===== EPISODE 57/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1098 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8168    0.8425    0.8295       254\n",
      "         1.0     0.6850    0.6444    0.6641       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7509    0.7435    0.7468       389\n",
      "weighted avg     0.7711    0.7738    0.7721       389\n",
      "\n",
      "===== EPISODE 58/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1108 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8080    0.8780    0.8415       254\n",
      "         1.0     0.7257    0.6074    0.6613       135\n",
      "\n",
      "    accuracy                         0.7841       389\n",
      "   macro avg     0.7668    0.7427    0.7514       389\n",
      "weighted avg     0.7794    0.7841    0.7790       389\n",
      "\n",
      "===== EPISODE 59/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1118 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8000    0.8661    0.8318       254\n",
      "         1.0     0.7018    0.5926    0.6426       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7509    0.7294    0.7372       389\n",
      "weighted avg     0.7659    0.7712    0.7661       389\n",
      "\n",
      "===== EPISODE 60/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1128 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8015    0.8583    0.8289       254\n",
      "         1.0     0.6923    0.6000    0.6429       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7469    0.7291    0.7359       389\n",
      "weighted avg     0.7636    0.7686    0.7643       389\n",
      "\n",
      "===== EPISODE 61/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1138 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8120    0.8504    0.8308       254\n",
      "         1.0     0.6911    0.6296    0.6589       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7515    0.7400    0.7448       389\n",
      "weighted avg     0.7700    0.7738    0.7711       389\n",
      "\n",
      "===== EPISODE 62/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1148 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8189    0.8189    0.8189       254\n",
      "         1.0     0.6593    0.6593    0.6593       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7391    0.7391    0.7391       389\n",
      "weighted avg     0.7635    0.7635    0.7635       389\n",
      "\n",
      "===== EPISODE 63/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1158 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8044    0.8583    0.8305       254\n",
      "         1.0     0.6949    0.6074    0.6482       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7497    0.7328    0.7393       389\n",
      "weighted avg     0.7664    0.7712    0.7672       389\n",
      "\n",
      "===== EPISODE 64/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1168 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8083    0.8465    0.8269       254\n",
      "         1.0     0.6829    0.6222    0.6512       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7456    0.7343    0.7390       389\n",
      "weighted avg     0.7648    0.7686    0.7659       389\n",
      "\n",
      "===== EPISODE 65/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1178 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8127    0.8543    0.8330       254\n",
      "         1.0     0.6967    0.6296    0.6615       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7547    0.7420    0.7472       389\n",
      "weighted avg     0.7725    0.7763    0.7735       389\n",
      "\n",
      "===== EPISODE 66/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1188 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8113    0.8465    0.8285       254\n",
      "         1.0     0.6855    0.6296    0.6564       135\n",
      "\n",
      "    accuracy                         0.7712       389\n",
      "   macro avg     0.7484    0.7380    0.7424       389\n",
      "weighted avg     0.7676    0.7712    0.7688       389\n",
      "\n",
      "===== EPISODE 67/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1198 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8275    0.8307    0.8291       254\n",
      "         1.0     0.6791    0.6741    0.6766       135\n",
      "\n",
      "    accuracy                         0.7763       389\n",
      "   macro avg     0.7533    0.7524    0.7528       389\n",
      "weighted avg     0.7760    0.7763    0.7762       389\n",
      "\n",
      "===== EPISODE 68/100 =====\n",
      "Class 0.0 => 1014 samples\n",
      "Class 1.0 => 1208 samples\n",
      "===== Evaluate in Testing set =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8120    0.8504    0.8308       254\n",
      "         1.0     0.6911    0.6296    0.6589       135\n",
      "\n",
      "    accuracy                         0.7738       389\n",
      "   macro avg     0.7515    0.7400    0.7448       389\n",
      "weighted avg     0.7700    0.7738    0.7711       389\n",
      "\n",
      "===== EPISODE 69/100 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[98], line 199\u001B[0m\n\u001B[0;32m    197\u001B[0m train_loader_detector \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset_detector, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m det_epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(detector_epochs):\n\u001B[1;32m--> 199\u001B[0m     detector_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_detector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader_detector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_detector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m     log_to_file(detector_log, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdet_epoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdetector_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdetector_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    202\u001B[0m \u001B[38;5;66;03m# print(f\"===== Evaluate in Training set =====\")\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;66;03m# evaluate_detector(new_detector, DataLoader(TensorDataset(X_train, y_train)), device, log_file=detector_log)\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \n\u001B[0;32m    205\u001B[0m \u001B[38;5;66;03m# Giả sử y_train có dạng 0/1\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[98], line 49\u001B[0m, in \u001B[0;36mtrain_detector\u001B[1;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[0;32m     47\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(y_pred, y_batch)\n\u001B[0;32m     48\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 49\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     51\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:43:27.259475Z",
     "start_time": "2025-01-12T19:40:23.488387Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7f9d8ec997ae05e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial TRAIN distribution:\n",
      "  Class 0.0: 1014 samples\n",
      "  Class 1.0: 538 samples\n",
      "Loaded pretrained Beta-CVAE and Detector successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Legion 7\\AppData\\Local\\Temp\\ipykernel_29568\\2928958269.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_beta_cvae.load_state_dict(torch.load(vae_path))\n",
      "C:\\Users\\Legion 7\\AppData\\Local\\Temp\\ipykernel_29568\\2928958269.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_detector_model.load_state_dict(torch.load(detector_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Beta-CVAE for 10 epochs.\n",
      "\n",
      "=== EPISODE 1/100 ===\n",
      "  [Detector] final train loss = 0.0283\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8293    0.8031    0.8160       254\n",
      "         1.0     0.6503    0.6889    0.6691       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7398    0.7460    0.7425       389\n",
      "weighted avg     0.7672    0.7635    0.7650       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 538\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 2/100 ===\n",
      "  [Detector] final train loss = 0.0223\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8185    0.7992    0.8088       254\n",
      "         1.0     0.6383    0.6667    0.6522       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7284    0.7329    0.7305       389\n",
      "weighted avg     0.7560    0.7532    0.7544       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 548\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 3/100 ===\n",
      "  [Detector] final train loss = 0.0307\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8185    0.7992    0.8088       254\n",
      "         1.0     0.6383    0.6667    0.6522       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7284    0.7329    0.7305       389\n",
      "weighted avg     0.7560    0.7532    0.7544       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 558\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 4/100 ===\n",
      "  [Detector] final train loss = 0.0205\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8211    0.7953    0.8080       254\n",
      "         1.0     0.6364    0.6741    0.6547       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7288    0.7347    0.7313       389\n",
      "weighted avg     0.7570    0.7532    0.7548       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 568\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 5/100 ===\n",
      "  [Detector] final train loss = 0.0266\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8280    0.8150    0.8214       254\n",
      "         1.0     0.6619    0.6815    0.6715       135\n",
      "\n",
      "    accuracy                         0.7686       389\n",
      "   macro avg     0.7449    0.7482    0.7465       389\n",
      "weighted avg     0.7703    0.7686    0.7694       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 578\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 6/100 ===\n",
      "  [Detector] final train loss = 0.0227\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8347    0.7953    0.8145       254\n",
      "         1.0     0.6463    0.7037    0.6738       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7405    0.7495    0.7441       389\n",
      "weighted avg     0.7693    0.7635    0.7657       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 588\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 7/100 ===\n",
      "  [Detector] final train loss = 0.0324\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8219    0.7992    0.8104       254\n",
      "         1.0     0.6408    0.6741    0.6570       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7314    0.7366    0.7337       389\n",
      "weighted avg     0.7590    0.7558    0.7572       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 598\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 8/100 ===\n",
      "  [Detector] final train loss = 0.0306\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8306    0.7913    0.8105       254\n",
      "         1.0     0.6395    0.6963    0.6667       135\n",
      "\n",
      "    accuracy                         0.7584       389\n",
      "   macro avg     0.7350    0.7438    0.7386       389\n",
      "weighted avg     0.7643    0.7584    0.7606       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 608\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 9/100 ===\n",
      "  [Detector] final train loss = 0.0288\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8211    0.7953    0.8080       254\n",
      "         1.0     0.6364    0.6741    0.6547       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7288    0.7347    0.7313       389\n",
      "weighted avg     0.7570    0.7532    0.7548       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 618\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 10/100 ===\n",
      "  [Detector] final train loss = 0.0215\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8167    0.8071    0.8119       254\n",
      "         1.0     0.6449    0.6593    0.6520       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7308    0.7332    0.7319       389\n",
      "weighted avg     0.7571    0.7558    0.7564       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 628\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 11/100 ===\n",
      "  [Detector] final train loss = 0.0213\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8247    0.8150    0.8198       254\n",
      "         1.0     0.6594    0.6741    0.6667       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7421    0.7445    0.7432       389\n",
      "weighted avg     0.7673    0.7661    0.7667       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 638\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 12/100 ===\n",
      "  [Detector] final train loss = 0.0229\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8223    0.7835    0.8024       254\n",
      "         1.0     0.6259    0.6815    0.6525       135\n",
      "\n",
      "    accuracy                         0.7481       389\n",
      "   macro avg     0.7241    0.7325    0.7275       389\n",
      "weighted avg     0.7541    0.7481    0.7504       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 648\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 13/100 ===\n",
      "  [Detector] final train loss = 0.0229\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8197    0.7874    0.8032       254\n",
      "         1.0     0.6276    0.6741    0.6500       135\n",
      "\n",
      "    accuracy                         0.7481       389\n",
      "   macro avg     0.7236    0.7307    0.7266       389\n",
      "weighted avg     0.7530    0.7481    0.7500       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 658\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 14/100 ===\n",
      "  [Detector] final train loss = 0.0286\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8211    0.7953    0.8080       254\n",
      "         1.0     0.6364    0.6741    0.6547       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7288    0.7347    0.7313       389\n",
      "weighted avg     0.7570    0.7532    0.7548       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 668\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 15/100 ===\n",
      "  [Detector] final train loss = 0.0294\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8252    0.7992    0.8120       254\n",
      "         1.0     0.6434    0.6815    0.6619       135\n",
      "\n",
      "    accuracy                         0.7584       389\n",
      "   macro avg     0.7343    0.7403    0.7369       389\n",
      "weighted avg     0.7621    0.7584    0.7599       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 678\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 16/100 ===\n",
      "  [Detector] final train loss = 0.0235\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8252    0.7992    0.8120       254\n",
      "         1.0     0.6434    0.6815    0.6619       135\n",
      "\n",
      "    accuracy                         0.7584       389\n",
      "   macro avg     0.7343    0.7403    0.7369       389\n",
      "weighted avg     0.7621    0.7584    0.7599       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 688\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 17/100 ===\n",
      "  [Detector] final train loss = 0.0172\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8226    0.8031    0.8127       254\n",
      "         1.0     0.6454    0.6741    0.6594       135\n",
      "\n",
      "    accuracy                         0.7584       389\n",
      "   macro avg     0.7340    0.7386    0.7361       389\n",
      "weighted avg     0.7611    0.7584    0.7595       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 698\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 18/100 ===\n",
      "  [Detector] final train loss = 0.0201\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8272    0.7913    0.8089       254\n",
      "         1.0     0.6370    0.6889    0.6619       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7321    0.7401    0.7354       389\n",
      "weighted avg     0.7612    0.7558    0.7579       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 708\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 19/100 ===\n",
      "  [Detector] final train loss = 0.0164\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8264    0.7874    0.8065       254\n",
      "         1.0     0.6327    0.6889    0.6596       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7295    0.7381    0.7330       389\n",
      "weighted avg     0.7592    0.7532    0.7555       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 718\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 20/100 ===\n",
      "  [Detector] final train loss = 0.0245\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8320    0.7992    0.8153       254\n",
      "         1.0     0.6483    0.6963    0.6714       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7401    0.7478    0.7433       389\n",
      "weighted avg     0.7682    0.7635    0.7653       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 728\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 21/100 ===\n",
      "  [Detector] final train loss = 0.0178\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8319    0.7795    0.8049       254\n",
      "         1.0     0.6291    0.7037    0.6643       135\n",
      "\n",
      "    accuracy                         0.7532       389\n",
      "   macro avg     0.7305    0.7416    0.7346       389\n",
      "weighted avg     0.7616    0.7532    0.7561       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 738\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 22/100 ===\n",
      "  [Detector] final train loss = 0.0184\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8272    0.7913    0.8089       254\n",
      "         1.0     0.6370    0.6889    0.6619       135\n",
      "\n",
      "    accuracy                         0.7558       389\n",
      "   macro avg     0.7321    0.7401    0.7354       389\n",
      "weighted avg     0.7612    0.7558    0.7579       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 748\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 23/100 ===\n",
      "  [Detector] final train loss = 0.0182\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8235    0.7717    0.7967       254\n",
      "         1.0     0.6159    0.6889    0.6503       135\n",
      "\n",
      "    accuracy                         0.7429       389\n",
      "   macro avg     0.7197    0.7303    0.7235       389\n",
      "weighted avg     0.7515    0.7429    0.7459       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 758\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 24/100 ===\n",
      "  [Detector] final train loss = 0.0245\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8189    0.7835    0.8008       254\n",
      "         1.0     0.6233    0.6741    0.6477       135\n",
      "\n",
      "    accuracy                         0.7455       389\n",
      "   macro avg     0.7211    0.7288    0.7242       389\n",
      "weighted avg     0.7510    0.7455    0.7477       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 768\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 25/100 ===\n",
      "  [Detector] final train loss = 0.0207\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8250    0.7795    0.8016       254\n",
      "         1.0     0.6242    0.6889    0.6549       135\n",
      "\n",
      "    accuracy                         0.7481       389\n",
      "   macro avg     0.7246    0.7342    0.7283       389\n",
      "weighted avg     0.7553    0.7481    0.7507       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 778\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 26/100 ===\n",
      "  [Detector] final train loss = 0.0184\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8368    0.7874    0.8114       254\n",
      "         1.0     0.6400    0.7111    0.6737       135\n",
      "\n",
      "    accuracy                         0.7609       389\n",
      "   macro avg     0.7384    0.7493    0.7425       389\n",
      "weighted avg     0.7685    0.7609    0.7636       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 788\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 27/100 ===\n",
      "  [Detector] final train loss = 0.0152\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8382    0.7953    0.8162       254\n",
      "         1.0     0.6486    0.7111    0.6784       135\n",
      "\n",
      "    accuracy                         0.7661       389\n",
      "   macro avg     0.7434    0.7532    0.7473       389\n",
      "weighted avg     0.7724    0.7661    0.7684       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 798\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 28/100 ===\n",
      "  [Detector] final train loss = 0.0184\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8320    0.7992    0.8153       254\n",
      "         1.0     0.6483    0.6963    0.6714       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7401    0.7478    0.7433       389\n",
      "weighted avg     0.7682    0.7635    0.7653       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 808\n",
      "  => Generating 10 new samples for class 1\n",
      "\n",
      "=== EPISODE 29/100 ===\n",
      "  [Detector] final train loss = 0.0164\n",
      "  Evaluate on TEST set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8347    0.7953    0.8145       254\n",
      "         1.0     0.6463    0.7037    0.6738       135\n",
      "\n",
      "    accuracy                         0.7635       389\n",
      "   macro avg     0.7405    0.7495    0.7441       389\n",
      "weighted avg     0.7693    0.7635    0.7657       389\n",
      "\n",
      "  Current distribution => class0.0: 1014, class1.0: 818\n",
      "  => Generating 10 new samples for class 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[100], line 222\u001B[0m\n\u001B[0;32m    219\u001B[0m x_orig \u001B[38;5;241m=\u001B[39m X_train[rand_idx]\n\u001B[0;32m    221\u001B[0m \u001B[38;5;66;03m# Gọi hàm generate\u001B[39;00m\n\u001B[1;32m--> 222\u001B[0m x_adv \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_one_adversarial_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta_cvae\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloaded_beta_cvae\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdetector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloaded_detector_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_orig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_orig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreviously_generated\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreviously_generated\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlambda_div\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\n\u001B[0;32m    232\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m new_samples\u001B[38;5;241m.\u001B[39mappend(x_adv\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m    235\u001B[0m new_labels\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m1.\u001B[39m]))\n",
      "Cell \u001B[1;32mIn[100], line 98\u001B[0m, in \u001B[0;36mgenerate_one_adversarial_sample\u001B[1;34m(beta_cvae, detector, x_orig, device, previously_generated, alpha, lambda_div, lr, steps)\u001B[0m\n\u001B[0;32m     95\u001B[0m         diversity_term \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     97\u001B[0m     loss \u001B[38;5;241m=\u001B[39m prob_class1\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m+\u001B[39m lambda_div \u001B[38;5;241m*\u001B[39m diversity_term\n\u001B[1;32m---> 98\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m     optimizer_z\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b9c34992fa5efcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "971e341813aff474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "58444df028eee2af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e4dbcb7a314d75e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8f378f36d47d0422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9828d7f7f8891798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e0d706044329a5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "712b53b2cb64e2a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84d59a8b559c6eb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8f1b0ad69386f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "640b47c77fe0515f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d502041a34775ef0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63f65dc4830e088c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36ea636f45f56971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "628ed34c86dbbad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9b315bf298f6fab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1189a137496408da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7188fd4cf4e535c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8f47c3e7e4d94479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3fcd123edf11c416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "889951f9bbef2566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "710f02424f787d7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29e221124c84c628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6b2d89a1b969aba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9331061c5b721413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4e0d4c50ff1c5695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff9ee89217265740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "de19c5836be5f7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "20d81c1f63b58cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "630fd1c59d1789c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29ae8cbf965687c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ef92603e94499a6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
